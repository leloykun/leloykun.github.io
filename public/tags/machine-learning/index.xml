<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Machine Learning on Franz Louis Cesista</title>
    <link>https://leloykun.github.io/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on Franz Louis Cesista</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>en</language>
    <lastBuildDate>Wed, 20 Aug 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://leloykun.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Steepest Descent on Finsler-Structured (Matrix) Manifolds</title>
      <link>https://leloykun.github.io/ponder/steepest-descent-finsler/</link>
      <pubDate>Wed, 20 Aug 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/steepest-descent-finsler/</guid>
      <description>Fast and robust model training.</description>
    </item>
    <item>
      <title>Heuristic Solutions for Steepest Descent on the Stiefel Manifold</title>
      <link>https://leloykun.github.io/ponder/steepest-descent-stiefel/</link>
      <pubDate>Fri, 18 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/steepest-descent-stiefel/</guid>
      <description>What would Muon look like if we constrained the weights to be semi-orthogonal?</description>
    </item>
    <item>
      <title>Training Transformers with Enforced Lipschitz Bounds</title>
      <link>https://leloykun.github.io/papers/lipschitz-transformers/</link>
      <pubDate>Thu, 17 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/papers/lipschitz-transformers/</guid>
      <description>Neural networks are often highly sensitive to input and weight perturbations. This sensitivity has been linked to pathologies such as vulnerability to adversarial examples, divergent training, and overfitting. To combat these problems, past research has looked at building neural networks entirely from Lipschitz components. However, these techniques have not matured to the point where researchers have trained a modern architecture such as a transformer with a Lipschitz certificate enforced beyond initialization. To explore this gap, we begin by developing and benchmarking novel, computationally-efficient tools for maintaining norm-constrained weight matrices. Applying these tools, we are able to train transformer models with Lipschitz bounds enforced throughout training. We find that optimizer dynamics matter: switching from AdamW to Muon improves standard methods -- weight decay and spectral normalization -- allowing models to reach equal performance with a lower Lipschitz bound. Inspired by Muon&amp;#39;s update having a fixed spectral norm, we co-design a weight constraint method that improves the Lipschitz vs. performance tradeoff on MLPs and 2M parameter transformers. Our 2-Lipschitz transformer on Shakespeare text reaches validation accuracy 60%. Scaling to 145M parameters, our 10-Lipschitz transformer reaches 21% accuracy on internet text. However, to match the NanoGPT baseline validation accuracy of 39.4%, our Lipschitz upper bound increases to 10^264. Nonetheless, our Lipschitz transformers train without stability measures such as layer norm, QK norm, and logit tanh softcapping.</description>
    </item>
    <item>
      <title>Sensitivity and Sharpness of n-Simplicial Attention</title>
      <link>https://leloykun.github.io/ponder/lipschitz-n-simplical-transformer/</link>
      <pubDate>Sun, 06 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/lipschitz-n-simplical-transformer/</guid>
      <description>Towards a maximal update parameterization of n-simplicial attention</description>
    </item>
    <item>
      <title>Adam with Aggressive Gradient Clipping ≈ Smoothed SignSGD/NormSGD</title>
      <link>https://leloykun.github.io/ponder/adam-aggressive-clipping/</link>
      <pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/adam-aggressive-clipping/</guid>
      <description>Why does Adam with aggressive gradient value/norm clipping have sparse updates and do well with higher learning rates? Here we show that it is essentially equivalent to a smoothed version of SignSGD/NormSGD.</description>
    </item>
    <item>
      <title>Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping via Newton-Schulz Iteration</title>
      <link>https://leloykun.github.io/ponder/spectral-clipping/</link>
      <pubDate>Mon, 23 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/spectral-clipping/</guid>
      <description>A small step towards hardware-architecture-optimizer codesign in deep learning.</description>
    </item>
    <item>
      <title>Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds</title>
      <link>https://leloykun.github.io/ponder/steepest-descent-non-riemannian/</link>
      <pubDate>Thu, 03 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/steepest-descent-non-riemannian/</guid>
      <description>Muon from first principles, what makes it different from other optimizers, and why it works so well.</description>
    </item>
    <item>
      <title>Napkin Math on Non-Euclidean Trust Region Optimization</title>
      <link>https://leloykun.github.io/ponder/napkin-math-trust-region-opt/</link>
      <pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/napkin-math-trust-region-opt/</guid>
      <description>A possible reason why Muon converges faster &amp;amp; does better at higher learning rates than Adam.</description>
    </item>
    <item>
      <title>Blocked Matrix Formulation of Linear Attention Mechanisms</title>
      <link>https://leloykun.github.io/ponder/blockmat-linear-attn/</link>
      <pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/blockmat-linear-attn/</guid>
      <description>The blocked matrix formulation of linear attention mechanisms, multi-step online gradient descent at inference time, and chunk-wise parallelism.</description>
    </item>
    <item>
      <title>Steepest Descent Under Schatten-p Norms</title>
      <link>https://leloykun.github.io/ponder/steepest-descent-schatten-p/</link>
      <pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/steepest-descent-schatten-p/</guid>
      <description>Why Muon still work despite not perfectly semi-orthogonalizing the gradients.</description>
    </item>
    <item>
      <title>Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients</title>
      <link>https://leloykun.github.io/ponder/muon-opt-coeffs/</link>
      <pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/muon-opt-coeffs/</guid>
      <description>Simply switching to Muon can already get you 2x efficiency gains. But you can squeeze out an extra 1-2% by optimizing the Newton-Schulz coefficients.</description>
    </item>
    <item>
      <title>CASPR Without Accumulation is Muon</title>
      <link>https://leloykun.github.io/ponder/caspr-wo-accum-is-muon/</link>
      <pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/caspr-wo-accum-is-muon/</guid>
      <description>The CASPR optimizer, a variant of Shampoo, reduces to Muon when we remove the accumulation on the preconditioners.</description>
    </item>
    <item>
      <title>GRPO&#39;s Main Flaw</title>
      <link>https://leloykun.github.io/ponder/grpo-flaw/</link>
      <pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/grpo-flaw/</guid>
      <description>GRPO may not be the best choice for training reasoning models. Here&amp;#39;s why.</description>
    </item>
    <item>
      <title>(Linear) Attention as Test-Time Regression</title>
      <link>https://leloykun.github.io/ponder/test-time-regression/</link>
      <pubDate>Mon, 27 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/test-time-regression/</guid>
      <description>A unifying framework for linear attention mechanisms as test-time regression and how to parallelize training and inference.</description>
    </item>
    <item>
      <title>Deep Learning Optimizers as Steepest Descent in Normed Spaces</title>
      <link>https://leloykun.github.io/ponder/steepest-descent-opt/</link>
      <pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/steepest-descent-opt/</guid>
      <description>Instead of asking, &amp;#39;Which optimizer should I use?&amp;#39; ask, &amp;#39;In which space do my features live in?&amp;#39;</description>
    </item>
    <item>
      <title>Multimodal Structured Generation</title>
      <link>https://leloykun.github.io/personal-projects/mmsg/</link>
      <pubDate>Sun, 14 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/personal-projects/mmsg/</guid>
      <description>Generate interleaved text and image content in a structured format you can directly pass to downstream APIs.</description>
    </item>
    <item>
      <title>Multimodal Structured Generation: CVPR&#39;s 2nd MMFM Challenge Technical Report</title>
      <link>https://leloykun.github.io/papers/mmsg/</link>
      <pubDate>Mon, 17 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/papers/mmsg/</guid>
      <description>Multimodal Foundation Models (MMFMs) have shown remarkable performance on various computer vision and natural language processing tasks. However, their performance on particular tasks such as document understanding is still limited. They also require more compute, time, and engineering resources to finetune and deploy compared to traditional, unimodal models. In this report, we present Multimodal Structured Generation, a general framework which constrains the output logits of frozen MMFMs to force them to reason before responding with structured outputs that downstream APIs can parse and use. We provide a detailed account of our approach, including the technical details, theoretical discussions, and final evaluation results in the 2nd Multimodal Foundation Models Challenge hosted by the Computer Vision and Pattern Recognition (CVPR) conference. Our approach achieved the second highest score in the hidden test set for Phase 2 and third highest overall. This shows the method&amp;#39;s ability to generalize to unseen tasks. And that simple engineering can beat expensive &amp;amp; complicated modelling steps as we first discussed in our paper, Retrieval Augmented Structured Generation: Business Document Information Extraction as Tool Use.</description>
    </item>
    <item>
      <title>Flash Hyperbolic Attention Minimal [WIP]</title>
      <link>https://leloykun.github.io/personal-projects/flash-hyperbolic-attention-minimal/</link>
      <pubDate>Tue, 16 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/personal-projects/flash-hyperbolic-attention-minimal/</guid>
      <description>A minimal implementation of Flash Attention 1 &amp;amp; 2 in just ~350 lines of CUDA code. This is still a work-in-progress, but the ultimate goal is to implement the various variations of Hyperbolic Attention in CUDA.</description>
    </item>
    <item>
      <title>Retrieval Augmented Structured Generation: Business Document Information Extraction As Tool Use</title>
      <link>https://leloykun.github.io/papers/rasg/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/papers/rasg/</guid>
      <description>Business Document Information Extraction (BDIE) is the problem of transforming a blob of unstructured information (raw text, scanned documents, etc.) into a structured format that downstream systems can parse and use. It has two main tasks: Key-Information Extraction (KIE) and Line Items Recognition (LIR). In this paper, we argue that BDIE is best modeled as a Tool Use problem, where the tools are these downstream systems. We then present Retrieval Augmented Structured Generation (RASG), a novel general framework for BDIE that achieves state of the art (SOTA) results on both KIE and LIR tasks on BDIE benchmarks.
The contributions of this paper are threefold: (1) We show, with ablation benchmarks, that Large Language Models (LLMs) with RASG are already competitive with or surpasses current SOTA Large Multimodal Models (LMMs) without RASG on BDIE benchmarks. (2) We propose a new metric class for Line Items Recognition, General Line Items Recognition Metric (GLIRM), that is more aligned with practical BDIE use cases compared to existing metrics, such as ANLS*, DocILE, and GriTS. (3) We provide a heuristic algorithm for backcalculating bounding boxes of predicted line items and tables without the need for vision encoders. Finally, we claim that, while LMMs might sometimes offer marginal performance benefits, LLMs &#43; RASG is oftentimes superior given real-world applications and constraints of BDIE.</description>
    </item>
    <item>
      <title>The Human Mind May Be Universal</title>
      <link>https://leloykun.github.io/ponder/human-mind-universality/</link>
      <pubDate>Sun, 10 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/human-mind-universality/</guid>
      <description>Years of experience in building artificial minds led me to believe that these AIs may end up seeming more &amp;#39;human&amp;#39; than we currently imagine them to be.</description>
    </item>
    <item>
      <title>Llama.cpp</title>
      <link>https://leloykun.github.io/personal-projects/llama.cpp/</link>
      <pubDate>Tue, 25 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/personal-projects/llama.cpp/</guid>
      <description>A C&#43;&#43; implementation of Meta&amp;#39;s Llama2 generative large-language model. I also optimized the original C implementation by Karpathy by adding parallelization on the multi-head attention layer.</description>
    </item>
    <item>
      <title>Expedock Assistant: ChatGPT Applied to Logistics Data</title>
      <link>https://leloykun.github.io/personal-projects/expedock-assistant/</link>
      <pubDate>Tue, 31 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/personal-projects/expedock-assistant/</guid>
      <description>Expedock Assistant is a chatbot that allows you to ask questions about your shipments and get answers in real time. It’s like having a personal assistant that knows everything about your business, shipments and industry.</description>
    </item>
    <item>
      <title>Expedock AutoML</title>
      <link>https://leloykun.github.io/personal-projects/expedock-automl/</link>
      <pubDate>Mon, 25 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/personal-projects/expedock-automl/</guid>
      <description>Expedock&amp;#39;s AutoML Library -- fit a model, run batch inference, and get explanations in one line of code each.</description>
    </item>
    <item>
      <title>Vaccine Search as a Computational Problem</title>
      <link>https://leloykun.github.io/ponder/vaccine-search-as-comp-prob/</link>
      <pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/vaccine-search-as-comp-prob/</guid>
      <description>A thought dump on mRNA vaccines and the future of computational biology</description>
    </item>
    <item>
      <title>Booking Demand Prediction for Grab SEA</title>
      <link>https://leloykun.github.io/personal-projects/grab-booking-demand-prediction/</link>
      <pubDate>Sun, 16 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/personal-projects/grab-booking-demand-prediction/</guid>
      <description>Booking demand prediction for Grab&amp;#39;s Southeast Asia operations. The project involves spatio-temporal forecasting, anomaly detection, and econometric modeling.</description>
    </item>
  </channel>
</rss>
