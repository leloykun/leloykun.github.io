<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Napkin Math on Non-Euclidean Trust Region Optimization | Franz Louis Cesista</title>
<meta name="keywords" content="Machine Learning, Muon">
<meta name="description" content="A possible reason why Muon converges faster &amp; does better at higher learning rates than Adam.">
<meta name="author" content="Franz Louis Cesista">
<link rel="canonical" href="https://leloykun.github.io/ponder/napkin-math-trust-region-opt/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e50e51e1b615f62676c32fb60e15e4b5e3a80ade29f3c6f9c953c528f576fb9f.css" integrity="sha256-5Q5R4bYV9iZ2wy&#43;2DhXkteOoCt4p88b5yVPFKPV2&#43;58=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://leloykun.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://leloykun.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://leloykun.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://leloykun.github.io/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://leloykun.github.io/ponder/napkin-math-trust-region-opt/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Napkin Math on Non-Euclidean Trust Region Optimization" />
<meta property="og:description" content="A possible reason why Muon converges faster &amp; does better at higher learning rates than Adam." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://leloykun.github.io/ponder/napkin-math-trust-region-opt/" /><meta property="article:section" content="ponder" />
<meta property="article:published_time" content="2025-03-24T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-03-24T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Napkin Math on Non-Euclidean Trust Region Optimization"/>
<meta name="twitter:description" content="A possible reason why Muon converges faster &amp; does better at higher learning rates than Adam."/>
<meta name="twitter:site" content="@leloykun"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Ponder",
      "item": "https://leloykun.github.io/ponder/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Napkin Math on Non-Euclidean Trust Region Optimization",
      "item": "https://leloykun.github.io/ponder/napkin-math-trust-region-opt/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Napkin Math on Non-Euclidean Trust Region Optimization",
  "name": "Napkin Math on Non-Euclidean Trust Region Optimization",
  "description": "A possible reason why Muon converges faster \u0026 does better at higher learning rates than Adam.",
  "keywords": [
    "Machine Learning", "Muon"
  ],
  "articleBody": "In a previous post, we talked about how to derive some common optimizers from a choice of norm. In this post, we’ll go over Kovalev’s recent paper on Non-Euclidean Trust Region Optimization and how we can use its main result to explain why Muon converges faster and does better at higher learning rates than Adam.\nNon-Euclidean Trust Region Optimization We consider the following optimization problem: $$\\min_{x \\in \\mathcal{X}} f(x)$$ where $f(\\cdot): \\mathcal{X} \\rightarrow \\mathbb{R}$ is a bounded from below and differentiable objective function and $\\mathcal{X}$ is finite-dimensional vector space equipped with an inner product $\\langle \\cdot, \\cdot \\rangle: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ and a norm $|| \\cdot ||: \\mathcal{X} \\rightarrow \\mathbb{R}$ which may or may not coincide with the inner product.\nNote: Kovalev’s objective actually has a regularization term, $R(\\cdot): \\mathcal{X} \\rightarrow \\mathbb{R} \\cup \\{ +\\infty \\}$ that is proper (i.e. not $\\infty$ everywhere), closed, and convex. So the original objective is $\\min_{x \\in \\mathcal{X}} [F(x) = f(x) + R(x)]$. We omit this for simplicity.\nAssumptions The paper’s results rely on the following assumptions, which I find to be quite reasonable:\nWe have access to an unbiased and bounded variance stochastic estimator $g(\\cdot; \\xi) : \\mathcal{X} \\rightarrow \\mathcal{X}$ of the gradient $\\nabla f(\\cdot)$, $\\xi \\sim D$ is a random variable sampled from a distribution $D$. I.e., $$\\mathbb{E}_{\\xi \\sim D}[g(x; \\xi)] = \\nabla f(x)\\quad\\text{and}\\quad \\mathbb{E}_{\\xi \\sim D}[||g(x; \\xi) - \\nabla f(x)||_2^2] \\leq \\sigma^2\\quad \\forall x \\in \\mathcal{X}\\tag{A1}$$ where $\\sigma \u003e 0$ is a positive variance parameter and $||\\cdot||_2 = \\sqrt{\\langle \\cdot, \\cdot \\rangle}$.\nThe gradient $\\nabla f(\\cdot)$ is Lipschitz continuous with constant $L \u003e 0$. I.e., $$||\\nabla f(x) - \\nabla f(x’)||^{\\dagger} \\leq L||x - x’||\\quad \\forall x, x’ \\in \\mathcal{X}\\tag{A2}$$ where $||\\cdot||^{\\dagger}$ is the dual norm of $||\\cdot||$.\nTo connect the norms in (A1) and (A2), we use the following inequality: $$||x||^{\\dagger} \\leq \\rho\\cdot||x||_2\\quad \\forall x \\in \\mathcal{X}\\tag{A3}$$ where $\\rho \u003e 0$ is a positive constant. Note that this $\\rho$ always exists due to the norm equivalence theorem for finite-dimensional vector spaces.\nThese are sufficient conditions for the convergence of the following trust region optimization algorithm.\nAlgorithm 1: Stochastic Non-Euclidean Trust-Region Gradient Method with Momentum Important Results The first is Lemma 3 which gives an upper bound on the distance between the momentum $m_k$ and the true gradient $\\nabla f(x_k)$, following the update rule in Algorithm 1. And the second is Theorem 2 which provides convergence guarantees for Algorithm 1.\n(Kovalev’s) Lemma 3. Let Assumptions (A1) to (A3) hold and let $m_0 = g(x_0; \\xi_0)$. Then the iterations of Algorithm 1 satisfy the following inequality for $k \\geq 1$: $$\\mathbb{E}[|| m_k - \\nabla f(x_k) ||^{\\dagger}] \\leq (1-a)^k \\rho \\sigma + \\sqrt{\\alpha}\\rho\\sigma+\\frac{L\\eta}{\\alpha}.$$\nThis is why, unlike OSGDM, we must accumulate the momentum term before we apply the dualizer. Otherwise, we can’t guarantee that our momentum term would actually be useful, nor even converge. And this lemma also works for any well-defined norm $||\\cdot||$ on $\\mathcal{X}$. Previous work by Cutkosky \u0026 Mehta (2020) has only shown this for the Frobenius norm.\n(Kovalev’s) Theorem 2. Let Assumptions (A1) to (A3) hold and let $m_0 = g(x_0; \\xi_0)$. Then the iterations of Algorithm 1 satisfy the following inequality: $$\\mathbb{E}\\left[ \\min_{k=1,\\ldots,K} || \\nabla f(x_k) ||^{\\dagger} \\right] \\leq \\frac{\\Delta_0}{\\eta K} + \\frac{2\\rho\\sigma}{\\alpha K} + 2\\sqrt{\\alpha}\\rho\\sigma + \\frac{3L\\eta}{2} + \\frac{2L\\eta}{\\alpha},$$ where $\\Delta_0 = f(x_0) - \\inf_x f(x)$. Hence, to reach precision $\\mathbb{E}\\left[ \\min_{k=1,\\ldots,K} || \\nabla f(x_k) ||^{\\dagger} \\right] \\leq \\epsilon$, it is sufficient to choose the stepsize $\\eta$ and the momentum parameter $\\alpha$ as follows: $$\\eta = \\min\\left\\{\\frac{\\epsilon}{16L}, \\frac{\\epsilon^3}{256\\rho^2\\sigma^2L} \\right\\},\\quad\\quad \\alpha=\\min\\left\\{ 1, \\frac{\\epsilon^2}{16\\rho^2\\sigma^2} \\right\\},$$ and the number of iterations $K$ as follows: $$K = \\left\\lceil \\max\\left\\{ \\frac{2048L\\Delta_0\\rho^2\\sigma^2}{\\epsilon^4}, \\frac{256\\rho^3\\sigma^3}{\\epsilon^3},\\frac{128L\\Delta_0}{\\epsilon^2}, \\frac{16\\rho\\sigma}{\\epsilon} \\right\\}\\right\\rceil.$$\nI think this is the real meat of the paper. And this works for any well-defined norm $||\\cdot||$ on $\\mathcal{X}$, not just the Frobenius norm.\nHowever, the norm embedding constant $\\rho$ and the Lipschitz constant $L$ both depend on the choice of norm. And as they increase, the lower $\\eta$ and $\\alpha$ become and the higher $K$ becomes.\nA Possible Reason Why Muon Outperforms Adam We’re entering napkin math territory here, so take everything with a grain of salt.\nHere we’ll show that Adam’s $\\rho$ and $L$ are higher than Muon’s and consequently its $\\eta$ and $\\alpha$ are lower and $K$ is higher. And this may explain why Muon converges faster and does better at higher learning rates than Adam.\nPreliminaries Let’s focus on linear layers. I.e., $\\mathcal{X} = \\mathbb{R}^{m \\times n}$. And following Bernstein \u0026 Newhouse (2024), we can interpret Adam and Muon as steepest descent under the operator norms $||A||_{\\text{max}}$ and $||A||_{2\\to 2}$, respectively.\nOptimizer Norm ($||\\cdot||$) Dual Norm ($||\\cdot||^{\\dagger}$) Adam $\\quad||A||_{\\text{max}} = ||vec(A)||_\\infty = \\max_{i,j}|A_{i,j}|\\quad$ $||A||_{1} = \\sum_{i,j}|A_{i,j}|$ Muon $\\quad||A||_{2\\to 2} = ||\\sigma(A)||_\\infty = \\max_i \\sigma_i(A)\\quad$ $||A||_{\\text{nuc}} = ||\\sigma(A)||_1 = \\sum_i \\sigma_i(A)$ where $A \\in \\mathbb{R}^{m \\times n}$, $\\sigma(A) = (\\sigma(A)_1, \\sigma(A)_2, \\ldots, \\sigma(A)_r)$ are the singular values of $A$, and $r = \\text{rank}(A) \\leq \\min(m, n)$.\nWe will also heavily rely on the following inequality in our proofs below:\nThe monotonicity inequality for $p$-norms. For any vector $x \\in \\mathbb{R}^N$ and for $0 \u003c p’ \\leq q’ \\leq \\infty$, the $p$-norms satisfy $$||x||_{q’} \\leq ||x||_{p’} \\leq N^{1/{p’} - 1/{q’}}||x||_{q’},\\tag{1}$$\nwhich is a direct consequence of the Hölder’s inequality.\nAdam has a higher norm embedding constant $\\rho$ than Muon For Adam:\n$$ \\begin{align*} ||A||_{\\text{max}}^{\\dagger} \u0026= ||A||_1 = ||vec(A)||_1\\\\ \u0026\\leq (mn)^{1/1 - 1/2}||vec(A)||_2 \u0026\u0026 \\text{(from $\\bm{(1)}$, with $p’=1$ and $q’=2$)}\\\\ \u0026\\leq \\sqrt{mn}||vec(A)||_2\\\\ ||A||_{\\text{max}}^{\\dagger} \u0026\\leq \\sqrt{mn}||A||_F \\tag{2} \\end{align*} $$ Thus, Adam’s $\\rho$ is $\\sqrt{mn}$.\nLikewise, for Muon:\n$$ \\begin{align*} ||A||_{2\\to 2}^{\\dagger} \u0026= ||A||_{nuc} = ||\\sigma(A)||_1\\\\ \u0026\\leq (\\min\\{m,n\\})^{1/1 - 1/2}||\\sigma(A)||_{2} \u0026\u0026 \\text{(from $\\bm{(1)}$, with $p’=1$ and $q’=2$)}\\\\ \u0026\\leq \\sqrt{\\min\\{m,n\\}}||\\sigma(A)||_{2}\\\\ ||A||_{2\\to 2}^{\\dagger} \u0026\\leq \\sqrt{\\min\\{m,n\\}}||A||_F \\tag{3} \\end{align*} $$ Thus, Muon’s $\\rho$ is $\\sqrt{\\min\\{m,n\\}}$ which is always less than Adam’s $\\sqrt{mn}$.\nAdam has a higher Lipschitz constant $L$ than Muon We make another assumption:\nSuppose that we have a constant $\\hat{L} \u003e 0$ such that for all $X, X’ \\in \\mathbb{R}^{m\\times n}$ we have $$||\\nabla f(X) - \\nabla f(X’)||_F \\leq \\hat{L} || X - X’ ||_F.\\tag{A4}$$\nFor Adam:\n$$ \\begin{align*} ||vec(A)||_2 \u0026\\leq (mn)^{1/2 - 0}||vec(A)||_\\infty \u0026\u0026 \\text{(from $\\bm{(1)}$, with $p’=2$ and $q’=\\infty$)}\\\\ ||A||_F \u0026\\leq \\sqrt{mn}||A||_{\\text{max}}\\tag{4} \\end{align*} $$ Thus for all $X, X’ \\in \\mathbb{R}^{m\\times n}$, $$ \\begin{align*} ||\\nabla f(X) - \\nabla f(X’)||_\\text{max}^{\\dagger} \u0026\\leq \\sqrt{mn}||\\nabla f(X) - \\nabla f(X’)||_F \u0026\u0026 \\text{(from $\\bm{(2)}$)}\\\\ \u0026\\leq \\hat{L}\\sqrt{mn}||X - X’||_F \u0026\u0026 \\text{(from $\\bm{(A4)}$)}\\\\ ||\\nabla f(X) - \\nabla f(X’)||_\\text{max}^{\\dagger} \u0026\\leq \\hat{L}mn||X - X’||_\\text{max} \u0026\u0026 \\text{(from $\\bm{(4)}$)}\\tag{5}\\\\ \\end{align*} $$ Thus, Adam’s $L$ is $\\hat{L}mn$.\nLikewise, for Muon:\n$$ \\begin{align*} ||\\sigma(X)||_2 \u0026\\leq (\\min\\{m,n\\})^{1/2 - 0}||\\sigma(X)||_\\infty \u0026\u0026 \\text{(from $\\bm{(1)}$, with $p’=2$ and $q’=\\infty$)}\\\\ ||X||_F \u0026\\leq \\sqrt{\\min\\{m,n\\}}||X||_{2\\to 2}\\tag{6} \\end{align*} $$ Thus for all $X, X’ \\in \\mathbb{R}^{m\\times n}$, $$ $$ \\begin{align*} ||\\nabla f(X) - \\nabla f(X’)||_{2\\to 2}^{\\dagger} \u0026\\leq \\sqrt{\\min\\{m,n\\}}||\\nabla f(X) - \\nabla f(X’)||_F \u0026\u0026 \\text{(from $\\bm{(3)}$)}\\\\ \u0026\\leq \\hat{L}\\sqrt{\\min\\{m,n\\}}||X - X’||_F \u0026\u0026 \\text{(from $\\bm{(A4)}$)}\\\\ ||\\nabla f(X) - \\nabla f(X’)||_{2\\to 2}^{\\dagger} \u0026\\leq \\hat{L}\\min\\{m,n\\}||X - X’||_{2\\to 2} \u0026\u0026 \\text{(from $\\bm{(6)}$)}\\tag{7}\\\\ \\end{align*} $$ $$ Thus, Muon’s $L$ is $\\hat{L}\\min\\{m,n\\}$ which is always less than Adam’s $\\hat{L}mn$.\nConclusion Let’s go back to Kovalev’s main result: $$\\eta = \\min\\left\\{\\frac{\\epsilon}{16L}, \\frac{\\epsilon^3}{256\\rho^2\\sigma^2L} \\right\\},\\quad\\quad \\alpha=\\min\\left\\{ 1, \\frac{\\epsilon^2}{16\\rho^2\\sigma^2} \\right\\},$$ $$K = \\left\\lceil \\max\\left\\{ \\frac{2048L\\Delta_0\\rho^2\\sigma^2}{\\epsilon^4}, \\frac{256\\rho^3\\sigma^3}{\\epsilon^3},\\frac{128L\\Delta_0}{\\epsilon^2}, \\frac{16\\rho\\sigma}{\\epsilon} \\right\\}\\right\\rceil$$\nMuon has a lower norm embedding constant $\\rho$ and Lipschitz constant $L$ than Adam. Thus, Muon’s learning rate $\\eta$ and momentum $\\alpha$ should be higher. And in practice, we do observe that Muon loves higher learning rates. Also notice that a lower $\\rho$ and $L$ leads to a lower $K$. This means that, theoretically, Muon should converge faster than Adam–which is exactly what we observe in practice.\nCaveats The bounds here are very loose. And for SGD, which is steepest descent under the Frobenius norm, we have $\\rho = 1$ and $L = \\hat{L}$ which implies that it’s better than both Adam and Muon, which is not true in practice. Thus, again, take everything here with a grain of salt.\nHow to Cite @misc{cesista2025tro, author = {Franz Louis Cesista}, title = {Napkin Math on Non-Euclidean Trust Region Optimization}, year = {2025}, url = {http://leloykun.github.io/ponder/napkin-math-trust-region-opt/}, } References Dmitry Kovalev (2025). Understanding Gradient Orthogonalization for Deep Learning via Non-Euclidean Trust-Region Optimization. Available at: https://arxiv.org/abs/2503.12645 Jeremy Bernstein and Laker Newhouse. “Old optimizer, new norm: An anthology.” arXiv preprint arXiv:2409.20325 (2024). Keller Jordan, Jeremy Bernstein, Brendan Rappazzo, @fernbear.bsky.social, Boza Vlado, Jiacheng You, Franz Cesista, Braden Koszarsky, and @Grad62304977. modded-nanogpt: Speedrunning the NanoGPT baseline. 2024. Available at: https://github.com/KellerJordan/modded-nanogpt. Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein (2024). Muon: An optimizer for hidden layers in neural networks. Available at: https://kellerjordan.github.io/posts/muon/. Moonshot AI Team (2025). Muon is Scalable for LLM Training. URL https://arxiv.org/abs/2502.16982 Cutkosky, A., \u0026 Mehta, H. (2020). Momentum improves normalized SGD. In Proceedings of the 37th International Conference on Machine Learning (PMLR Vol. 119, pp. 2260-2268). http://proceedings.mlr.press/v119/cutkosky20b.html ",
  "wordCount" : "1446",
  "inLanguage": "en",
  "datePublished": "2025-03-24T00:00:00Z",
  "dateModified": "2025-03-24T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Franz Louis Cesista"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://leloykun.github.io/ponder/napkin-math-trust-region-opt/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Franz Louis Cesista",
    "logo": {
      "@type": "ImageObject",
      "url": "https://leloykun.github.io/favicon.ico"
    }
  }
}
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-BWCGBRX8G1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BWCGBRX8G1');
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"
  integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"
  integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js"
  integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
        { left: "\\begin{equation}", right: "\\end{equation}", display: true },
        { left: "\\begin{equation*}", right: "\\end{equation*}", display: true },
        { left: "\\begin{align}", right: "\\end{align}", display: true },
        { left: "\\begin{align*}", right: "\\end{align*}", display: true },
        { left: "\\begin{alignat}", right: "\\end{alignat}", display: true },
        { left: "\\begin{gather}", right: "\\end{gather}", display: true },
        { left: "\\begin{CD}", right: "\\end{CD}", display: true },
      ],
      throwOnError: false,
      trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
      macros: {
        "\\eqref": "\\href{###1}{(\\text{#1})}",
        "\\ref": "\\href{###1}{\\text{#1}}",
        "\\label": "\\htmlId{#1}{}"
      }
    });
  });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://leloykun.github.io/" accesskey="h" title="Franz Louis Cesista">
                <img src="https://leloykun.github.io/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Franz Louis Cesista</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://leloykun.github.io/cv.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/personal-projects/" title="Personal Projects">
                    <span>Personal Projects</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/ponder/" title="Ponder (Posts)">
                    <span>Ponder (Posts)</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Napkin Math on Non-Euclidean Trust Region Optimization
    </h1>
    <div class="post-meta"><span title='2025-03-24 00:00:00 +0000 UTC'>March 24, 2025</span>&nbsp;&middot;&nbsp;Franz Louis Cesista

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#non-euclidean-trust-region-optimization">Non-Euclidean Trust Region Optimization</a>
      <ul>
        <li><a href="#assumptions">Assumptions</a></li>
        <li><a href="#algorithm-1-stochastic-non-euclidean-trust-region-gradient-method-with-momentum">Algorithm 1: Stochastic Non-Euclidean Trust-Region Gradient Method with Momentum</a></li>
        <li><a href="#important-results">Important Results</a></li>
      </ul>
    </li>
    <li><a href="#a-possible-reason-why-muon-outperforms-adam">A Possible Reason Why Muon Outperforms Adam</a>
      <ul>
        <li><a href="#preliminaries">Preliminaries</a></li>
        <li><a href="#adam-has-a-higher-norm-embedding-constant-rho-than-muon">Adam has a higher norm embedding constant $\rho$ than Muon</a></li>
        <li><a href="#adam-has-a-higher-lipschitz-constant-l-than-muon">Adam has a higher Lipschitz constant $L$ than Muon</a></li>
        <li><a href="#conclusion">Conclusion</a></li>
        <li><a href="#caveats">Caveats</a></li>
      </ul>
    </li>
    <li><a href="#how-to-cite">How to Cite</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><p>In a <a href="../steepest-descent-schatten-p/">previous post</a>, we talked about how to derive some common optimizers from a choice of norm. In this post, we&rsquo;ll go over <a href="https://arxiv.org/abs/2503.12645" target="_blank">Kovalev&rsquo;s recent paper on Non-Euclidean Trust Region Optimization</a> and how we can use its main result to explain why Muon converges faster and does better at higher learning rates than Adam.</p>
<h2 id="non-euclidean-trust-region-optimization">Non-Euclidean Trust Region Optimization<a hidden class="anchor" aria-hidden="true" href="#non-euclidean-trust-region-optimization">#</a></h2>
<p>We consider the following optimization problem:
$$\min_{x \in \mathcal{X}} f(x)$$
where $f(\cdot): \mathcal{X} \rightarrow \mathbb{R}$ is a bounded from below and differentiable objective function and $\mathcal{X}$ is finite-dimensional vector space equipped with an inner product $\langle \cdot, \cdot \rangle: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ and a norm $|| \cdot ||: \mathcal{X} \rightarrow \mathbb{R}$ which may or may not coincide with the inner product.</p>
<blockquote>
<p>Note: Kovalev&rsquo;s objective actually has a regularization term, $R(\cdot): \mathcal{X} \rightarrow \mathbb{R} \cup \{ +\infty \}$ that is proper (i.e. not $\infty$ everywhere), closed, and convex. So the original objective is $\min_{x \in \mathcal{X}} [F(x) = f(x) + R(x)]$. We omit this for simplicity.</p></blockquote>
<h3 id="assumptions">Assumptions<a hidden class="anchor" aria-hidden="true" href="#assumptions">#</a></h3>
<p>The paper&rsquo;s results rely on the following assumptions, which I find to be quite reasonable:</p>
<ol>
<li>
<p>We have access to an unbiased and bounded variance stochastic estimator $g(\cdot; \xi) : \mathcal{X} \rightarrow \mathcal{X}$ of the gradient $\nabla f(\cdot)$, $\xi \sim D$ is a random variable sampled from a distribution $D$. I.e., $$\mathbb{E}_{\xi \sim D}[g(x; \xi)] = \nabla f(x)\quad\text{and}\quad \mathbb{E}_{\xi \sim D}[||g(x; \xi) - \nabla f(x)||_2^2] \leq \sigma^2\quad \forall x \in \mathcal{X}\tag{A1}$$ where $\sigma &gt; 0$ is a positive variance parameter and $||\cdot||_2 = \sqrt{\langle \cdot, \cdot \rangle}$.</p>
</li>
<li>
<p>The gradient $\nabla f(\cdot)$ is Lipschitz continuous with constant $L &gt; 0$. I.e., $$||\nabla f(x) - \nabla f(x&rsquo;)||^{\dagger} \leq L||x - x&rsquo;||\quad \forall x, x&rsquo; \in \mathcal{X}\tag{A2}$$ where $||\cdot||^{\dagger}$ is the dual norm of $||\cdot||$.</p>
</li>
<li>
<p>To connect the norms in <strong>(A1)</strong> and <strong>(A2)</strong>, we use the following inequality: $$||x||^{\dagger} \leq \rho\cdot||x||_2\quad \forall x \in \mathcal{X}\tag{A3}$$ where $\rho &gt; 0$ is a positive constant. Note that this $\rho$ always exists due to the norm equivalence theorem for finite-dimensional vector spaces.</p>
</li>
</ol>
<p>These are sufficient conditions for the convergence of the following trust region optimization algorithm.</p>
<h3 id="algorithm-1-stochastic-non-euclidean-trust-region-gradient-method-with-momentum">Algorithm 1: Stochastic Non-Euclidean Trust-Region Gradient Method with Momentum<a hidden class="anchor" aria-hidden="true" href="#algorithm-1-stochastic-non-euclidean-trust-region-gradient-method-with-momentum">#</a></h3>
<p><img loading="lazy" src="/ponder/napkin-math-trust-region-opt/tro-algo-1.png#center"></p>
<p><img loading="lazy" src="/ponder/napkin-math-trust-region-opt/tro-algo-2.png#center"></p>
<h3 id="important-results">Important Results<a hidden class="anchor" aria-hidden="true" href="#important-results">#</a></h3>
<p>The first is Lemma 3 which gives an upper bound on the distance between the momentum $m_k$ and the true gradient $\nabla f(x_k)$, following the update rule in Algorithm 1. And the second is Theorem 2 which provides convergence guarantees for Algorithm 1.</p>
<blockquote>
<p><strong>(Kovalev&rsquo;s) Lemma 3.</strong> Let Assumptions <strong>(A1)</strong> to <strong>(A3)</strong> hold and let $m_0 = g(x_0; \xi_0)$. Then the iterations of Algorithm 1 satisfy the following inequality for $k \geq 1$:
$$\mathbb{E}[|| m_k - \nabla f(x_k) ||^{\dagger}] \leq (1-a)^k \rho \sigma + \sqrt{\alpha}\rho\sigma+\frac{L\eta}{\alpha}.$$</p></blockquote>
<p>This is why, unlike OSGDM, we must accumulate the momentum term <em>before</em> we apply the dualizer. Otherwise, we can&rsquo;t guarantee that our momentum term would actually be useful, nor even converge. And this lemma also works for any well-defined norm $||\cdot||$ on $\mathcal{X}$. Previous work by Cutkosky &amp; Mehta (2020) has only shown this for the Frobenius norm.</p>
<blockquote>
<p><strong>(Kovalev&rsquo;s) Theorem 2.</strong> Let Assumptions <strong>(A1)</strong> to <strong>(A3)</strong> hold and let $m_0 = g(x_0; \xi_0)$. Then the iterations of Algorithm 1 satisfy the following inequality:
$$\mathbb{E}\left[ \min_{k=1,\ldots,K} || \nabla f(x_k) ||^{\dagger} \right] \leq \frac{\Delta_0}{\eta K} + \frac{2\rho\sigma}{\alpha K} + 2\sqrt{\alpha}\rho\sigma + \frac{3L\eta}{2} + \frac{2L\eta}{\alpha},$$ where $\Delta_0 = f(x_0) - \inf_x f(x)$. Hence, to reach precision $\mathbb{E}\left[ \min_{k=1,\ldots,K} || \nabla f(x_k) ||^{\dagger} \right] \leq \epsilon$, it is sufficient to choose the stepsize $\eta$ and the momentum parameter $\alpha$ as follows:
$$\eta = \min\left\{\frac{\epsilon}{16L}, \frac{\epsilon^3}{256\rho^2\sigma^2L} \right\},\quad\quad \alpha=\min\left\{ 1, \frac{\epsilon^2}{16\rho^2\sigma^2} \right\},$$ and the number of iterations $K$ as follows:
$$K = \left\lceil \max\left\{ \frac{2048L\Delta_0\rho^2\sigma^2}{\epsilon^4}, \frac{256\rho^3\sigma^3}{\epsilon^3},\frac{128L\Delta_0}{\epsilon^2}, \frac{16\rho\sigma}{\epsilon} \right\}\right\rceil.$$</p></blockquote>
<p>I think this is the real meat of the paper. And this works for any well-defined norm $||\cdot||$ on $\mathcal{X}$, not just the Frobenius norm.</p>
<p>However, the norm embedding constant $\rho$ and the Lipschitz constant $L$ both depend on the choice of norm. And as they increase, the lower $\eta$ and $\alpha$ become and the higher $K$ becomes.</p>
<h2 id="a-possible-reason-why-muon-outperforms-adam">A Possible Reason Why Muon Outperforms Adam<a hidden class="anchor" aria-hidden="true" href="#a-possible-reason-why-muon-outperforms-adam">#</a></h2>
<p>We&rsquo;re entering napkin math territory here, so take everything with a grain of salt.</p>
<p>Here we&rsquo;ll show that Adam&rsquo;s $\rho$ and $L$ are higher than Muon&rsquo;s and consequently its $\eta$ and $\alpha$ are lower and $K$ is higher. And this may explain why Muon converges faster and does better at higher learning rates than Adam.</p>
<h3 id="preliminaries">Preliminaries<a hidden class="anchor" aria-hidden="true" href="#preliminaries">#</a></h3>
<p>Let&rsquo;s focus on linear layers. I.e., $\mathcal{X} = \mathbb{R}^{m \times n}$. And following Bernstein &amp; Newhouse (2024), we can interpret Adam and Muon as steepest descent under the operator norms $||A||_{\text{max}}$ and $||A||_{2\to 2}$, respectively.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Optimizer</th>
          <th style="text-align: center">Norm ($||\cdot||$)</th>
          <th style="text-align: center">Dual Norm ($||\cdot||^{\dagger}$)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">Adam</td>
          <td style="text-align: center">$\quad||A||_{\text{max}} = ||vec(A)||_\infty = \max_{i,j}|A_{i,j}|\quad$</td>
          <td style="text-align: center">$||A||_{1} = \sum_{i,j}|A_{i,j}|$</td>
      </tr>
      <tr>
          <td style="text-align: center">Muon</td>
          <td style="text-align: center">$\quad||A||_{2\to 2} = ||\sigma(A)||_\infty = \max_i \sigma_i(A)\quad$</td>
          <td style="text-align: center">$||A||_{\text{nuc}} = ||\sigma(A)||_1 = \sum_i \sigma_i(A)$</td>
      </tr>
  </tbody>
</table>
<p>where $A \in \mathbb{R}^{m \times n}$, $\sigma(A) = (\sigma(A)_1, \sigma(A)_2, \ldots, \sigma(A)_r)$ are the singular values of $A$, and $r = \text{rank}(A) \leq \min(m, n)$.</p>
<p>We will also heavily rely on the following inequality in our proofs below:</p>
<blockquote>
<p><strong>The monotonicity inequality for $p$-norms.</strong> For any vector $x \in \mathbb{R}^N$ and for $0 &lt; p&rsquo; \leq q&rsquo; \leq \infty$, the $p$-norms satisfy $$||x||_{q&rsquo;} \leq ||x||_{p&rsquo;} \leq N^{1/{p&rsquo;} - 1/{q&rsquo;}}||x||_{q&rsquo;},\tag{1}$$</p></blockquote>
<p>which is a direct consequence of the Hölder&rsquo;s inequality.</p>
<h3 id="adam-has-a-higher-norm-embedding-constant-rho-than-muon">Adam has a higher norm embedding constant $\rho$ than Muon<a hidden class="anchor" aria-hidden="true" href="#adam-has-a-higher-norm-embedding-constant-rho-than-muon">#</a></h3>
<p><strong>For Adam:</strong></p>
<blockquote>
<p>$$
\begin{align*}
||A||_{\text{max}}^{\dagger}
&amp;= ||A||_1 = ||vec(A)||_1\\
&amp;\leq (mn)^{1/1 - 1/2}||vec(A)||_2 &amp;&amp; \text{(from $\bm{(1)}$, with $p&rsquo;=1$ and $q&rsquo;=2$)}\\
&amp;\leq \sqrt{mn}||vec(A)||_2\\
||A||_{\text{max}}^{\dagger} &amp;\leq \sqrt{mn}||A||_F \tag{2}
\end{align*}
$$
Thus, Adam&rsquo;s $\rho$ is $\sqrt{mn}$.</p></blockquote>
<p>Likewise, <strong>for Muon:</strong></p>
<blockquote>
<p>$$
\begin{align*}
||A||_{2\to 2}^{\dagger}
&amp;= ||A||_{nuc} = ||\sigma(A)||_1\\
&amp;\leq (\min\{m,n\})^{1/1 - 1/2}||\sigma(A)||_{2} &amp;&amp; \text{(from $\bm{(1)}$, with $p&rsquo;=1$ and $q&rsquo;=2$)}\\
&amp;\leq \sqrt{\min\{m,n\}}||\sigma(A)||_{2}\\
||A||_{2\to 2}^{\dagger} &amp;\leq \sqrt{\min\{m,n\}}||A||_F \tag{3}
\end{align*}
$$
Thus, Muon&rsquo;s $\rho$ is $\sqrt{\min\{m,n\}}$ which is always less than Adam&rsquo;s $\sqrt{mn}$.</p></blockquote>
<h3 id="adam-has-a-higher-lipschitz-constant-l-than-muon">Adam has a higher Lipschitz constant $L$ than Muon<a hidden class="anchor" aria-hidden="true" href="#adam-has-a-higher-lipschitz-constant-l-than-muon">#</a></h3>
<p>We make another assumption:</p>
<blockquote>
<p>Suppose that we have a constant $\hat{L} &gt; 0$ such that for all $X, X&rsquo; \in \mathbb{R}^{m\times n}$ we have $$||\nabla f(X) - \nabla f(X&rsquo;)||_F \leq \hat{L} || X - X&rsquo; ||_F.\tag{A4}$$</p></blockquote>
<p><strong>For Adam:</strong></p>
<blockquote>
<p>$$
\begin{align*}
||vec(A)||_2 &amp;\leq (mn)^{1/2 - 0}||vec(A)||_\infty &amp;&amp; \text{(from $\bm{(1)}$, with $p&rsquo;=2$ and $q&rsquo;=\infty$)}\\
||A||_F       &amp;\leq \sqrt{mn}||A||_{\text{max}}\tag{4}
\end{align*}
$$
Thus for all $X, X&rsquo; \in \mathbb{R}^{m\times n}$,
$$
\begin{align*}
||\nabla f(X) - \nabla f(X&rsquo;)||_\text{max}^{\dagger}
&amp;\leq \sqrt{mn}||\nabla f(X) - \nabla f(X&rsquo;)||_F &amp;&amp; \text{(from $\bm{(2)}$)}\\
&amp;\leq \hat{L}\sqrt{mn}||X - X&rsquo;||_F &amp;&amp; \text{(from $\bm{(A4)}$)}\\
||\nabla f(X) - \nabla f(X&rsquo;)||_\text{max}^{\dagger}
&amp;\leq \hat{L}mn||X - X&rsquo;||_\text{max} &amp;&amp; \text{(from $\bm{(4)}$)}\tag{5}\\
\end{align*}
$$
Thus, Adam&rsquo;s $L$ is $\hat{L}mn$.</p></blockquote>
<p>Likewise, <strong>for Muon:</strong></p>
<blockquote>
<p>$$
\begin{align*}
||\sigma(X)||_2 &amp;\leq (\min\{m,n\})^{1/2 - 0}||\sigma(X)||_\infty &amp;&amp; \text{(from $\bm{(1)}$, with $p&rsquo;=2$ and $q&rsquo;=\infty$)}\\
||X||_F       &amp;\leq \sqrt{\min\{m,n\}}||X||_{2\to 2}\tag{6}
\end{align*}
$$
Thus for all $X, X&rsquo; \in \mathbb{R}^{m\times n}$,
$$
$$
\begin{align*}
||\nabla f(X) - \nabla f(X&rsquo;)||_{2\to 2}^{\dagger}
&amp;\leq \sqrt{\min\{m,n\}}||\nabla f(X) - \nabla f(X&rsquo;)||_F &amp;&amp; \text{(from $\bm{(3)}$)}\\
&amp;\leq \hat{L}\sqrt{\min\{m,n\}}||X - X&rsquo;||_F &amp;&amp; \text{(from $\bm{(A4)}$)}\\
||\nabla f(X) - \nabla f(X&rsquo;)||_{2\to 2}^{\dagger}
&amp;\leq \hat{L}\min\{m,n\}||X - X&rsquo;||_{2\to 2} &amp;&amp; \text{(from $\bm{(6)}$)}\tag{7}\\
\end{align*}
$$
$$
Thus, Muon&rsquo;s $L$ is $\hat{L}\min\{m,n\}$ which is always less than Adam&rsquo;s $\hat{L}mn$.</p></blockquote>
<h3 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h3>
<p>Let&rsquo;s go back to Kovalev&rsquo;s main result:
$$\eta = \min\left\{\frac{\epsilon}{16L}, \frac{\epsilon^3}{256\rho^2\sigma^2L} \right\},\quad\quad \alpha=\min\left\{ 1, \frac{\epsilon^2}{16\rho^2\sigma^2} \right\},$$ $$K = \left\lceil \max\left\{ \frac{2048L\Delta_0\rho^2\sigma^2}{\epsilon^4}, \frac{256\rho^3\sigma^3}{\epsilon^3},\frac{128L\Delta_0}{\epsilon^2}, \frac{16\rho\sigma}{\epsilon} \right\}\right\rceil$$</p>
<p>Muon has a lower norm embedding constant $\rho$ and Lipschitz constant $L$ than Adam. Thus, Muon&rsquo;s learning rate $\eta$ and momentum $\alpha$ should be higher. And in practice, we do observe that Muon loves higher learning rates. Also notice that a lower $\rho$ and $L$ leads to a lower $K$. This means that, theoretically, Muon should converge faster than Adam&ndash;which is exactly what we observe in practice.</p>
<h3 id="caveats">Caveats<a hidden class="anchor" aria-hidden="true" href="#caveats">#</a></h3>
<p>The bounds here are <em>very</em> loose. And for SGD, which is steepest descent under the Frobenius norm, we have $\rho = 1$ and $L = \hat{L}$ which implies that it&rsquo;s better than both Adam and Muon, which is not true in practice. Thus, again, take everything  here with a grain of salt.</p>
<h2 id="how-to-cite">How to Cite<a hidden class="anchor" aria-hidden="true" href="#how-to-cite">#</a></h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bibtex" data-lang="bibtex"><span style="display:flex;"><span><span style="color:#0a0;text-decoration:underline">@misc</span>{cesista2025tro,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">author</span> = <span style="color:#a50">{Franz Louis Cesista}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">title</span> = <span style="color:#a50">{Napkin Math on Non-Euclidean Trust Region Optimization}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">year</span> = <span style="color:#a50">{2025}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">url</span> = <span style="color:#a50">{http://leloykun.github.io/ponder/napkin-math-trust-region-opt/}</span>,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ol>
<li>Dmitry Kovalev (2025). Understanding Gradient Orthogonalization for Deep Learning via Non-Euclidean Trust-Region Optimization. Available at: <a href="https://arxiv.org/abs/2503.12645" target="_blank">https://arxiv.org/abs/2503.12645</a></li>
<li>Jeremy Bernstein and Laker Newhouse. “Old optimizer, new norm: An anthology.” arXiv preprint arXiv:2409.20325 (2024).</li>
<li>Keller Jordan, Jeremy Bernstein, Brendan Rappazzo, @fernbear.bsky.social, Boza Vlado, Jiacheng You, Franz Cesista, Braden Koszarsky, and @Grad62304977. modded-nanogpt: Speedrunning the NanoGPT baseline. 2024. Available at: <a href="https://github.com/KellerJordan/modded-nanogpt" target="_blank">https://github.com/KellerJordan/modded-nanogpt</a>.</li>
<li>Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein (2024). Muon: An optimizer for hidden layers in neural networks. Available at: <a href="https://kellerjordan.github.io/posts/muon/" target="_blank">https://kellerjordan.github.io/posts/muon/</a>.</li>
<li>Moonshot AI Team (2025). Muon is Scalable for LLM Training. URL <a href="https://arxiv.org/abs/2502.16982" target="_blank">https://arxiv.org/abs/2502.16982</a></li>
<li>Cutkosky, A., &amp; Mehta, H. (2020). Momentum improves normalized SGD. In Proceedings of the 37th International Conference on Machine Learning (PMLR Vol. 119, pp. 2260-2268). <a href="http://proceedings.mlr.press/v119/cutkosky20b.html" target="_blank">http://proceedings.mlr.press/v119/cutkosky20b.html</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://leloykun.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://leloykun.github.io/tags/muon/">Muon</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Napkin Math on Non-Euclidean Trust Region Optimization on x"
            href="https://x.com/intent/tweet/?text=Napkin%20Math%20on%20Non-Euclidean%20Trust%20Region%20Optimization&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fnapkin-math-trust-region-opt%2f&amp;hashtags=MachineLearning%2cMuon">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Napkin Math on Non-Euclidean Trust Region Optimization on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fnapkin-math-trust-region-opt%2f&amp;title=Napkin%20Math%20on%20Non-Euclidean%20Trust%20Region%20Optimization&amp;summary=Napkin%20Math%20on%20Non-Euclidean%20Trust%20Region%20Optimization&amp;source=https%3a%2f%2fleloykun.github.io%2fponder%2fnapkin-math-trust-region-opt%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Napkin Math on Non-Euclidean Trust Region Optimization on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fleloykun.github.io%2fponder%2fnapkin-math-trust-region-opt%2f&title=Napkin%20Math%20on%20Non-Euclidean%20Trust%20Region%20Optimization">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Napkin Math on Non-Euclidean Trust Region Optimization on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fleloykun.github.io%2fponder%2fnapkin-math-trust-region-opt%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Napkin Math on Non-Euclidean Trust Region Optimization on whatsapp"
            href="https://api.whatsapp.com/send?text=Napkin%20Math%20on%20Non-Euclidean%20Trust%20Region%20Optimization%20-%20https%3a%2f%2fleloykun.github.io%2fponder%2fnapkin-math-trust-region-opt%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Napkin Math on Non-Euclidean Trust Region Optimization on telegram"
            href="https://telegram.me/share/url?text=Napkin%20Math%20on%20Non-Euclidean%20Trust%20Region%20Optimization&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fnapkin-math-trust-region-opt%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Napkin Math on Non-Euclidean Trust Region Optimization on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Napkin%20Math%20on%20Non-Euclidean%20Trust%20Region%20Optimization&u=https%3a%2f%2fleloykun.github.io%2fponder%2fnapkin-math-trust-region-opt%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://leloykun.github.io/">Franz Louis Cesista</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
