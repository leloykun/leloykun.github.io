<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Deep Learning Optimizers as Steepest Descent in Normed Spaces | Franz Louis Cesista</title>
<meta name="keywords" content="Machine Learning">
<meta name="description" content="Instead of asking, &#39;Which optimizer should I use?&#39; ask, &#39;In which space do my features live in?&#39;">
<meta name="author" content="Franz Louis Cesista">
<link rel="canonical" href="https://leloykun.github.io/ponder/steepest-descent-opt/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f1e4501a2ac2bf9fff5dc0c77f152affb825b371cb176acfcf9201015d59b4d4.css" integrity="sha256-8eRQGirCv5//XcDHfxUq/7gls3HLF2rPz5IBAV1ZtNQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://leloykun.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://leloykun.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://leloykun.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://leloykun.github.io/apple-touch-icon.png">

<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://leloykun.github.io/ponder/steepest-descent-opt/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Deep Learning Optimizers as Steepest Descent in Normed Spaces" />
<meta property="og:description" content="Instead of asking, &#39;Which optimizer should I use?&#39; ask, &#39;In which space do my features live in?&#39;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://leloykun.github.io/ponder/steepest-descent-opt/" />
<meta property="og:image" content="https://leloykun.github.io/deep-learning-optimizers-intro.png" /><meta property="article:section" content="ponder" />
<meta property="article:published_time" content="2024-10-20T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-10-20T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://leloykun.github.io/deep-learning-optimizers-intro.png" />
<meta name="twitter:title" content="Deep Learning Optimizers as Steepest Descent in Normed Spaces"/>
<meta name="twitter:description" content="Instead of asking, &#39;Which optimizer should I use?&#39; ask, &#39;In which space do my features live in?&#39;"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Ponder",
      "item": "https://leloykun.github.io/ponder/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Deep Learning Optimizers as Steepest Descent in Normed Spaces",
      "item": "https://leloykun.github.io/ponder/steepest-descent-opt/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Deep Learning Optimizers as Steepest Descent in Normed Spaces",
  "name": "Deep Learning Optimizers as Steepest Descent in Normed Spaces",
  "description": "Instead of asking, 'Which optimizer should I use?' ask, 'In which space do my features live in?'",
  "keywords": [
    "Machine Learning"
  ],
  "articleBody": " Note: This was originally posted as a Twitter thread. I’ve reformatted it here for better readability.\nWhy do steepest descent in non-Euclidean spaces? Why does adaptive preconditioning work so well in practice? And, Why normalize everything ala nGPT? Ideally, when training a neural network, we want to bound the features, the weights, and their respective updates so that:\n[lower] the model actually “learns” stuff; and [upper] model training is stable These bounds then depend on the norms, but which norms?\nThe fun part is that the norms of the input and output features already induce the norm of the weights between them. We can also let the feature and feature updates have the same norm (likewise for the weights). And so, we only have to choose the norms for the features!\nNow, our datasets are usually Euclidean or locally Euclidean (see Manifold Hypothesis)\nWhat’s the norm induced by Euclidean input and output vector spaces? The Spectral Norm!\nSo even if we don’t want to do anything fancy, we’d still have to do steepest descent in non-Euclidean space because:\nThe induced norm for the weights (w/ Euclidean features) is non-Euclidean; and We’re optimizing the weights, not the features. The model inputs and outputs being Euclidean sounds reasonable, but why do the “hidden” features have to be Euclidean too?\nIf we vary the norms of these features, we also vary the induced norms of the weights and vice versa.\nAdaptive preconditioning then “searches” for the proper norms.\nThis also answers @mattecapu ’s Q here\nShampoo \u0026 SOAP starts from Euclidean features and Spectral weights, then tunes the norms over time. SOAP does this tuning with momentum so it’s theoretically faster.\nreally cool to also optimize the p in the norm. do you have a conceptual idea of what that’s tuning? I guess intuitively as p-\u003eoo each dimension is getting ‘further away’ from each other..\nhttps://x.com/mattecapu/status/1847218617567301804 A more cynical answer, from a mathematician to another, is that almost nobody in this field is actually doing proper linear algebra. Adaptive preconditioning allows us to start from really crappy configurations/parametrizations and get away scoff free.\nBut a more pro-ML answer would be that humans suck at predicting which inductive biases would work best when cooked into the models. E.g. why should the “hidden” features be in Euclidean space? Why not let the model learn the proper space(s) to work with?\nFinally, why is it a good idea to normalize everything everywhere?\nCuz it lets us have sane bounds \u0026 same norms on the features which means we can use the same optimizer for all the layers with minimal tuning!\nhttps://arxiv.org/abs/2410.01131 How to Cite @misc{cesista2024firstordernormedopt, author = {Franz Louis Cesista}, title = {Deep Learning Optimizers as Steepest Descent in Normed Spaces}, year = {2024}, url = {http://leloykun.github.io/ponder/steepest-descent-opt/}, } References Loshchilov, I., Hsieh, C., Sun, S., Ginsburg, B. (2024). nGPT: Normalized Transformer with Representation Learning on the Hypersphere. URL https://arxiv.org/abs/2410.01131 Yang, G., Simon, J., Bernstein, J. (2024). A Spectral Condition for Feature Learning. URL https://arxiv.org/abs/2310.17813 Bernstein, J., Newhouse, L. (2024). Old Optimizer, New Norm: An Anthology. URL https://arxiv.org/abs/2409.20325 ",
  "wordCount" : "511",
  "inLanguage": "en",
  "image":"https://leloykun.github.io/deep-learning-optimizers-intro.png","datePublished": "2024-10-20T00:00:00Z",
  "dateModified": "2024-10-20T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Franz Louis Cesista"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://leloykun.github.io/ponder/steepest-descent-opt/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Franz Louis Cesista",
    "logo": {
      "@type": "ImageObject",
      "url": "https://leloykun.github.io/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://leloykun.github.io/" accesskey="h" title="Franz Louis Cesista">
             
                <img src="https://leloykun.github.io/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Franz Louis Cesista</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://leloykun.github.io/cv.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/personal-projects/" title="Personal Projects">
                    <span>Personal Projects</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/ponder/" title="Ponder (Blog)">
                    <span>Ponder (Blog)</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Deep Learning Optimizers as Steepest Descent in Normed Spaces
    </h1>
    <div class="post-meta"><span title='2024-10-20 00:00:00 +0000 UTC'>October 20, 2024</span>&nbsp;&middot;&nbsp;Franz Louis Cesista&nbsp;&middot;&nbsp;<a href="https://x.com/leloykun/status/1847919153589735705" rel="noopener noreferrer" target="_blank">Crossposted from X (formerly Twitter)</a>

</div>
  </header> 
  <div class="post-content"><div align="center">
    <img src="deep-learning-optimizers-intro.png"/>
</div>
<blockquote>
<p>Note: This was originally posted as a Twitter thread. I&rsquo;ve reformatted it here for better readability.</p>
</blockquote>
<ol>
<li>Why do steepest descent in non-Euclidean spaces?</li>
<li>Why does adaptive preconditioning work so well in practice? And,</li>
<li>Why normalize everything ala nGPT?</li>
</ol>
<hr>
<p>Ideally, when training a neural network, we want to bound the features, the weights, and their respective updates so that:</p>
<ol>
<li>[lower] the model actually &ldquo;learns&rdquo; stuff; and</li>
<li>[upper] model training is stable</li>
</ol>
<p>These bounds then depend on the norms, but which norms?</p>
<div align="center">
    <img src="dl-opt-bounds.png" style="width:70%; height:70%" />
</div>
<hr>
<p>The fun part is that the norms of the input and output features already induce the norm of the weights between them. We can also let the feature and feature updates have the same norm (likewise for the weights). And so, we only have to choose the norms for the features!</p>
<div align="center">
    <img src="dl-opt-induced-norms.png" style="width:70%; height:70%" />
</div>
<hr>
<p>Now, our datasets are usually Euclidean or locally Euclidean (see Manifold Hypothesis)</p>
<p>What&rsquo;s the norm induced by Euclidean input and output vector spaces? The Spectral Norm!</p>
<div align="center">
    <img src="dl-opt-norms.png" style="width:80%; height:80%" />
</div>
<hr>
<p>So even if we don&rsquo;t want to do anything fancy, we&rsquo;d still have to do steepest descent in non-Euclidean space because:</p>
<ol>
<li>The induced norm for the weights (w/ Euclidean features) is non-Euclidean; and</li>
<li>We&rsquo;re optimizing the weights, not the features.</li>
</ol>
<hr>
<p>The model inputs and outputs being Euclidean sounds reasonable, but why do the &ldquo;hidden&rdquo; features have to be Euclidean too?</p>
<p>If we vary the norms of these features, we also vary the induced norms of the weights and vice versa.</p>
<p>Adaptive preconditioning then &ldquo;searches&rdquo; for the proper norms.</p>
<div align="center">
    <img src="dl-opt-adaptive.png" style="width:90%; height:90%" />
</div>
<hr>
<p>This also answers <a href="https://x.com/mattecapu" target="_blank">@mattecapu</a>
&rsquo;s Q here</p>
<p>Shampoo &amp; SOAP starts from Euclidean features and Spectral weights, then tunes the norms over time. SOAP does this tuning with momentum so it&rsquo;s theoretically faster.</p>
<blockquote>
<p>really cool to also optimize the p in the norm. do you have a conceptual idea of what that&rsquo;s tuning? I guess intuitively as p-&gt;oo each dimension is getting &lsquo;further away&rsquo; from each other..</p>
</blockquote>
<p><a href="https://x.com/mattecapu/status/1847218617567301804" target="_blank">https://x.com/mattecapu/status/1847218617567301804</a>
</p>
<p>A more cynical answer, from a mathematician to another, is that almost nobody in this field is actually doing proper linear algebra. Adaptive preconditioning allows us to start from really crappy configurations/parametrizations and get away scoff free.</p>
<p>But a more pro-ML answer would be that humans suck at predicting which inductive biases would work best when cooked into the models. E.g. why should the &ldquo;hidden&rdquo; features be in Euclidean space? Why not let the model learn the proper space(s) to work with?</p>
<hr>
<p>Finally, why is it a good idea to normalize everything everywhere?</p>
<p>Cuz it lets us have sane bounds &amp; same norms on the features which means we can use the same optimizer for all the layers with minimal tuning!</p>
<p><a href="https://arxiv.org/abs/2410.01131" target="_blank">https://arxiv.org/abs/2410.01131</a>
</p>
<h2 id="how-to-cite">How to Cite</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bibtex" data-lang="bibtex"><span style="display:flex;"><span><span style="color:#0a0;text-decoration:underline">@misc</span>{cesista2024firstordernormedopt,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">author</span> = <span style="color:#a50">{Franz Louis Cesista}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">title</span> = <span style="color:#a50">{Deep Learning Optimizers as Steepest Descent in Normed Spaces}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">year</span> = <span style="color:#a50">{2024}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">url</span> = <span style="color:#a50">{http://leloykun.github.io/ponder/steepest-descent-opt/}</span>,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="references">References</h2>
<ul>
<li>Loshchilov, I., Hsieh, C., Sun, S., Ginsburg, B. (2024). nGPT: Normalized Transformer with Representation Learning on the Hypersphere. URL <a href="https://arxiv.org/abs/2410.01131" target="_blank">https://arxiv.org/abs/2410.01131</a>
</li>
<li>Yang, G., Simon, J., Bernstein, J. (2024). A Spectral Condition for Feature Learning. URL <a href="https://arxiv.org/abs/2310.17813" target="_blank">https://arxiv.org/abs/2310.17813</a>
</li>
<li>Bernstein, J., Newhouse, L. (2024). Old Optimizer, New Norm: An Anthology. URL <a href="https://arxiv.org/abs/2409.20325" target="_blank">https://arxiv.org/abs/2409.20325</a>
</li>
</ul>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://leloykun.github.io/tags/machine-learning/">Machine Learning</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    &copy; 2025 Franz Louis Cesista
    <span>
    &middot;  Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
