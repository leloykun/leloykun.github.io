<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Ponder on Franz Louis Cesista</title>
    <link>https://leloykun.github.io/ponder/</link>
    <description>Recent content in Ponder on Franz Louis Cesista</description>
    <generator>Hugo -- 0.125.4</generator>
    <language>en</language>
    <lastBuildDate>Wed, 20 Aug 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://leloykun.github.io/ponder/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Steepest Descent on Finsler-Structured (Matrix) Manifolds</title>
      <link>https://leloykun.github.io/ponder/steepest-descent-finsler/</link>
      <pubDate>Wed, 20 Aug 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/steepest-descent-finsler/</guid>
      <description>Fast and robust model training.</description>
    </item>
    <item>
      <title>Heuristic Solutions for Steepest Descent on the Stiefel Manifold</title>
      <link>https://leloykun.github.io/ponder/steepest-descent-stiefel/</link>
      <pubDate>Fri, 18 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/steepest-descent-stiefel/</guid>
      <description>What would Muon look like if we constrained the weights to be semi-orthogonal?</description>
    </item>
    <item>
      <title>Sensitivity and Sharpness of n-Simplicial Attention</title>
      <link>https://leloykun.github.io/ponder/lipschitz-n-simplical-transformer/</link>
      <pubDate>Sun, 06 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/lipschitz-n-simplical-transformer/</guid>
      <description>Towards a maximal update parameterization of n-simplicial attention</description>
    </item>
    <item>
      <title>Adam with Aggressive Gradient Clipping â‰ˆ Smoothed SignSGD/NormSGD</title>
      <link>https://leloykun.github.io/ponder/adam-aggressive-clipping/</link>
      <pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/adam-aggressive-clipping/</guid>
      <description>Why does Adam with aggressive gradient value/norm clipping have sparse updates and do well with higher learning rates? Here we show that it is essentially equivalent to a smoothed version of SignSGD/NormSGD.</description>
    </item>
    <item>
      <title>Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping via Newton-Schulz Iteration</title>
      <link>https://leloykun.github.io/ponder/spectral-clipping/</link>
      <pubDate>Mon, 23 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/spectral-clipping/</guid>
      <description>A small step towards hardware-architecture-optimizer codesign in deep learning.</description>
    </item>
    <item>
      <title>Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds</title>
      <link>https://leloykun.github.io/ponder/steepest-descent-non-riemannian/</link>
      <pubDate>Thu, 03 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/steepest-descent-non-riemannian/</guid>
      <description>Muon from first principles, what makes it different from other optimizers, and why it works so well.</description>
    </item>
    <item>
      <title>Napkin Math on Non-Euclidean Trust Region Optimization</title>
      <link>https://leloykun.github.io/ponder/napkin-math-trust-region-opt/</link>
      <pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/napkin-math-trust-region-opt/</guid>
      <description>A possible reason why Muon converges faster &amp;amp; does better at higher learning rates than Adam.</description>
    </item>
    <item>
      <title>Blocked Matrix Formulation of Linear Attention Mechanisms</title>
      <link>https://leloykun.github.io/ponder/blockmat-linear-attn/</link>
      <pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/blockmat-linear-attn/</guid>
      <description>The blocked matrix formulation of linear attention mechanisms, multi-step online gradient descent at inference time, and chunk-wise parallelism.</description>
    </item>
    <item>
      <title>Steepest Descent Under Schatten-p Norms</title>
      <link>https://leloykun.github.io/ponder/steepest-descent-schatten-p/</link>
      <pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/steepest-descent-schatten-p/</guid>
      <description>Why Muon still work despite not perfectly semi-orthogonalizing the gradients.</description>
    </item>
    <item>
      <title>Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients</title>
      <link>https://leloykun.github.io/ponder/muon-opt-coeffs/</link>
      <pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/muon-opt-coeffs/</guid>
      <description>Simply switching to Muon can already get you 2x efficiency gains. But you can squeeze out an extra 1-2% by optimizing the Newton-Schulz coefficients.</description>
    </item>
    <item>
      <title>CASPR Without Accumulation is Muon</title>
      <link>https://leloykun.github.io/ponder/caspr-wo-accum-is-muon/</link>
      <pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/caspr-wo-accum-is-muon/</guid>
      <description>The CASPR optimizer, a variant of Shampoo, reduces to Muon when we remove the accumulation on the preconditioners.</description>
    </item>
    <item>
      <title>GRPO&#39;s Main Flaw</title>
      <link>https://leloykun.github.io/ponder/grpo-flaw/</link>
      <pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/grpo-flaw/</guid>
      <description>GRPO may not be the best choice for training reasoning models. Here&amp;#39;s why.</description>
    </item>
    <item>
      <title>(Linear) Attention as Test-Time Regression</title>
      <link>https://leloykun.github.io/ponder/test-time-regression/</link>
      <pubDate>Mon, 27 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/test-time-regression/</guid>
      <description>A unifying framework for linear attention mechanisms as test-time regression and how to parallelize training and inference.</description>
    </item>
    <item>
      <title>Deep Learning Optimizers as Steepest Descent in Normed Spaces</title>
      <link>https://leloykun.github.io/ponder/steepest-descent-opt/</link>
      <pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/steepest-descent-opt/</guid>
      <description>Instead of asking, &amp;#39;Which optimizer should I use?&amp;#39; ask, &amp;#39;In which space do my features live in?&amp;#39;</description>
    </item>
    <item>
      <title>ChatGPT May Have Developed Seasonal Depression</title>
      <link>https://leloykun.github.io/ponder/chatgpt-seasonal-depression/</link>
      <pubDate>Sat, 16 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/chatgpt-seasonal-depression/</guid>
      <description>Could ChatGPT&amp;#39;s shorter responses be an indication of something more bizarre going on?</description>
    </item>
    <item>
      <title>The Human Mind May Be Universal</title>
      <link>https://leloykun.github.io/ponder/human-mind-universality/</link>
      <pubDate>Sun, 10 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/human-mind-universality/</guid>
      <description>Years of experience in building artificial minds led me to believe that these AIs may end up seeming more &amp;#39;human&amp;#39; than we currently imagine them to be.</description>
    </item>
    <item>
      <title>Vaccine Search as a Computational Problem</title>
      <link>https://leloykun.github.io/ponder/vaccine-search-as-comp-prob/</link>
      <pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://leloykun.github.io/ponder/vaccine-search-as-comp-prob/</guid>
      <description>A thought dump on mRNA vaccines and the future of computational biology</description>
    </item>
  </channel>
</rss>
