<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>The Blocked Matrix Formulation of Linear Attention Mechanisms | Franz Louis Cesista</title>
<meta name="keywords" content="Machine Learning, Linear Attention, Test-Time Regression">
<meta name="description" content="The blocked matrix formulation of linear attention mechanisms, multi-step online gradient descent at inference time, and chunk-wise parallelism.">
<meta name="author" content="Franz Louis Cesista">
<link rel="canonical" href="http://localhost:1313/ponder/test-time-regression-2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.7695d65198b4eb0454d08822ddd0b9bc75437b4a2a995eef4f73b0ae209c4e92.css" integrity="sha256-dpXWUZi06wRU0Igi3dC5vHVDe0oqmV7vT3OwriCcTpI=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/ponder/test-time-regression-2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="The Blocked Matrix Formulation of Linear Attention Mechanisms" />
<meta property="og:description" content="The blocked matrix formulation of linear attention mechanisms, multi-step online gradient descent at inference time, and chunk-wise parallelism." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/ponder/test-time-regression-2/" /><meta property="article:section" content="ponder" />
<meta property="article:published_time" content="2025-03-14T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-03-14T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="The Blocked Matrix Formulation of Linear Attention Mechanisms"/>
<meta name="twitter:description" content="The blocked matrix formulation of linear attention mechanisms, multi-step online gradient descent at inference time, and chunk-wise parallelism."/>
<meta name="twitter:site" content="@leloykun"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Ponder",
      "item": "http://localhost:1313/ponder/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The Blocked Matrix Formulation of Linear Attention Mechanisms",
      "item": "http://localhost:1313/ponder/test-time-regression-2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The Blocked Matrix Formulation of Linear Attention Mechanisms",
  "name": "The Blocked Matrix Formulation of Linear Attention Mechanisms",
  "description": "The blocked matrix formulation of linear attention mechanisms, multi-step online gradient descent at inference time, and chunk-wise parallelism.",
  "keywords": [
    "Machine Learning", "Linear Attention", "Test-Time Regression"
  ],
  "articleBody": "In the previous post, we derived several linear attention mechanisms from scratch by formulating them as online test-time regression problems. Here, we’ll discuss a more intuitive way to represent the update rules of these linear attention mechanisms using a blocked matrix formulation. Then, we’ll discuss how to use it to (1) derive the update rules for linear attention mechanisms that take multiple gradient descent steps per token and (2) derive the update rules for chunk-wise parallelism of already-existing linear attention mechanisms.\nBlocked Matrix Formulation The update rules of linear attention mechanisms have the following structure:\n$$S_i = S_{i-1}A_i + B_i$$\nwhere $S_{i-1}$ is the (old) state at time step $i-1$, $S_i$ is the (new) state at time step $i$, and $A_i$ and $B_i$ are update matrices. In most cases, $A_i$ “removes” some information from the state while $B_i$ “adds” new information to the state. But if $A_i$ can have negative eigenvalues, we can think of it as “inverting” some information instead of merely “removing” it.\nHere are a couple of examples:\nLinear Attention Mechanism Update Rule $A_i$ $B_i$ Vanilla Linear Attention $S_i = S_{i-1} + \\bm{v}_i \\bm{k}_i^T$ $I$ $\\bm{v}_i \\bm{k}_i^T$ Mamba 2 $S_i = S_{i-1}\\text{diag}\\left(\\alpha_i I\\right) + \\bm{v}_i \\bm{k}_i^T$ $\\text{diag}\\left(\\alpha_i I\\right)$ $\\bm{v}_i \\bm{k}_i^T$ DeltaNet $S_i = S_{i-1}(I - \\beta_i \\bm{k}_i \\bm{k}_i^T) + \\bm{v}_i \\bm{k}_i^T$ $I - \\beta_i \\bm{k}_i \\bm{k}_i^T$ $\\beta_i \\bm{v}_i \\bm{k}_i^T$ Gated DeltaNet $S_i = S_{i-1}\\alpha_i(I - \\beta_i \\bm{k}_i \\bm{k}_i^T) + \\beta_i \\bm{v}_i \\bm{k}_i^T$ $\\alpha_i(I - \\beta_i \\bm{k}_i \\bm{k}_i^T)$ $\\beta_i \\bm{v}_i \\bm{k}_i^T$ where $v_i$ and $k_i$ are the value and key at time step $i$, respectively.\nNow, notice that the update rule above can be rewritten as:\n$$ S_{i} = \\begin{bmatrix} S_{i-1} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_i \\\\ B_i \\end{bmatrix} $$ or, equivalently, $$ \\begin{bmatrix} S_{i} \u0026 I \\end{bmatrix} = \\begin{bmatrix} S_{i-1} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_i \u0026 0 \\\\ B_i \u0026 I \\end{bmatrix} $$\nNow, at training time, we need an efficient way to compute $S_i$ for all $i$. We can do this by unrolling the update rule for $N$ steps:\n$$ \\begin{align*} \\begin{bmatrix} S_{N} \u0026 I \\end{bmatrix} \u0026= \\begin{bmatrix} S_{0} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_1 \u0026 0 \\\\ B_1 \u0026 I \\end{bmatrix} \\begin{bmatrix} A_2 \u0026 0 \\\\ B_2 \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A_N \u0026 0 \\\\ B_N \u0026 I \\end{bmatrix}\\\\ S_N \u0026= \\begin{bmatrix} S_{0} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_1 \u0026 0 \\\\ B_1 \u0026 I \\end{bmatrix} \\begin{bmatrix} A_2 \u0026 0 \\\\ B_2 \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A_N \u0026 0 \\\\ B_N \u0026 I \\end{bmatrix} \\begin{bmatrix} I \\\\ 0 \\end{bmatrix} \\end{align*} $$\nIn practice, we usually initialize $S_0$ as the zero matrix. Thus,\n$$ \\begin{align} S_N \u0026= \\begin{bmatrix} 0 \u0026 I \\end{bmatrix} \\begin{bmatrix} A_1 \u0026 0 \\\\ B_1 \u0026 I \\end{bmatrix} \\begin{bmatrix} A_2 \u0026 0 \\\\ B_2 \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A_N \u0026 0 \\\\ B_N \u0026 I \\end{bmatrix} \\begin{bmatrix} I \\\\ 0 \\end{bmatrix}\\\\ S_N \u0026= \\begin{bmatrix} 0 \u0026 I \\end{bmatrix} \\begin{bmatrix} \\prod_{i=1}^{N} A_i \u0026 0 \\\\ \\sum_{i=1}^{N} \\left(B_i \\prod_{j=i+1}^{N} A_j\\right) \u0026 I \\end{bmatrix} \\begin{bmatrix} I \\\\ 0 \\end{bmatrix}\\\\ S_N \u0026= \\sum_{i=1}^{N} \\left(B_i \\prod_{j=i+1}^{N} A_j\\right) \\end{align} $$ where $(1) \\rightarrow (2)$ can be proven by induction.\nEquation $(1)$ makes it obvious why and how we can parallelize computation of $S_N$, for all $N$, at training time: the updates are merely (blocked) matrix multiplications; matrix multiplications are associative; and we can easily parallelize associative operations!\nOne-Step Online Gradient Descent per Token Let’s derive $S_N$ for each of the linear attention mechanisms in the table above.\nVanilla Linear Attention $$A_i = I \\quad\\quad B_i = \\bm{v}_i \\bm{k}_i^T$$ $$ \\begin{align*} S_N \u0026= \\sum_{i=1}^{N} \\left(\\bm{v}_i \\bm{k}_i^T \\prod_{j=i+1}^{N} I\\right)\\\\ S_N \u0026= \\sum_{i=1}^{N} \\bm{v}_i \\bm{k}_i^T \\end{align*} $$\nMamba 2 $$A_i = \\text{diag}\\left(\\alpha_i I\\right) \\quad\\quad B_i = \\bm{v}_i \\bm{k}_i^T$$ $$ \\begin{align*} S_N \u0026= \\sum_{i=1}^{N} \\left(\\bm{v}_i \\bm{k}_i^T \\prod_{j=i+1}^{N} \\text{diag}\\left(\\alpha_j I\\right)\\right)\\\\ S_N \u0026= \\sum_{i=1}^{N} \\left( \\prod_{j=i+1}^{N} \\alpha_j \\right) \\bm{v}_i \\bm{k}_i^T \\end{align*} $$\nDeltaNet $$A_i = I - \\beta_i \\bm{k}_i \\bm{k}_i^T \\quad\\quad B_i = \\beta_i \\bm{v}_i \\bm{k}_i^T$$ $$S_N = \\sum_{i=1}^{N} \\left(\\beta_i \\bm{v}_i \\bm{k}_i^T \\prod_{j=i+1}^{N} \\left(I - \\beta_j \\bm{k}_j \\bm{k}_j^T\\right)\\right)$$\nGated DeltaNet $$A_i = \\alpha_i(I - \\beta_i \\bm{k}_i \\bm{k}_i^T) \\quad\\quad B_i = \\beta_i \\bm{v}_i \\bm{k}_i^T$$ $$ \\begin{align*} S_N \u0026= \\sum_{i=1}^{N} \\left(\\beta_i \\bm{v}_i \\bm{k}_i^T \\prod_{j=i+1}^{N} \\alpha_j \\left(I - \\beta_j \\bm{k}_j \\bm{k}_j^T\\right)\\right)\\\\ S_N \u0026= \\sum_{i=1}^{N} \\left(\\left(\\beta_i \\prod_{j=i+1}^{N} \\alpha_j \\right) \\bm{v}_i \\bm{k}_i^T \\prod_{j=i+1}^{N} \\left(I - \\beta_j \\bm{k}_j \\bm{k}_j^T\\right)\\right) \\end{align*} $$\nEasy!\nMulti-Step Online Gradient Descent per Token Now, what if we want to take $n_h$ gradient descent steps per token?\nTo do this, we can follow the procedure outlined in the DeltaProduct paper where they:\nRecurrently generate $n_h$ intermediate tokens for each input token (including the input token itself), Calculate the key-value pairs for each intermediate token, Update the state using the key-value pairs, and Discard the outputs except for the last one to maintain the number of tokens. So, instead of updating the state only once per token, we update the state $n_h$ times per token. And to get the final state for each token $S_N$, we expand equation $(1)$ above: $$ \\begin{align} S_N = \\begin{bmatrix} 0 \u0026 I \\end{bmatrix} \\begin{bmatrix} A_{1,1} \u0026 0 \\\\ B_{1,1} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_{1,2} \u0026 0 \\\\ B_{1,2} \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A_{1,n_h} \u0026 0 \\\\ B_{1,n_h} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_{2,1} \u0026 0 \\\\ B_{2,1} \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A_{N, n_h} \u0026 0 \\\\ B_{N, n_h} \u0026 I \\end{bmatrix} \\begin{bmatrix} I \\\\ 0 \\end{bmatrix} \\end{align} $$ where $S_{i,j}$, $A_{i,j}$, and $B_{i,j}$ are the state and update matrices for the $j$-th gradient descent step for the $i$-th token.\nAnd if we reindex this as $[\\cdot]_k = [\\cdot]_{k/n_h,\\space (k-1) \\% n_h + 1}$, then from equation $(3)$ above, we get: $$S_N = \\sum_{k=1}^{Nn_h} \\left( B_k \\prod_{k’=k+1}^{Nn_h} A_{k’}\\right)$$\nAlternatively, we can also combine the updates for each token into a single update matrix first before multiplying them together:\n$$ \\begin{bmatrix} A’_{i} \u0026 0 \\\\ B’_{i} \u0026 I \\end{bmatrix} = \\prod_{j=1}^{n_h} \\begin{bmatrix} A_{i,j} \u0026 0 \\\\ B_{i,j} \u0026 I \\end{bmatrix} = \\begin{bmatrix} \\prod_{j=1}^{n_h} A_{i,j} \u0026 0 \\\\ \\sum_{j=1}^{n_h} \\left(B_{i,j} \\prod_{j’=j+1}^{n_h} A_{i,j’}\\right) \u0026 I \\end{bmatrix} $$\n$$ \\begin{align*} S_N \u0026= \\begin{bmatrix} 0 \u0026 I \\end{bmatrix} \\begin{bmatrix} A’_1 \u0026 0 \\\\ B’_1 \u0026 I \\end{bmatrix} \\begin{bmatrix} A’_2 \u0026 0 \\\\ B’_2 \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A’_N \u0026 0 \\\\ B’_N \u0026 I \\end{bmatrix} \\begin{bmatrix} I \\\\ 0 \\end{bmatrix}\\\\ S_N \u0026= \\begin{bmatrix} 0 \u0026 I \\end{bmatrix} \\begin{bmatrix} \\prod_{i=1}^N A’_i \u0026 0 \\\\ \\sum_{i=1}^N \\left( B’_i \\prod_{i’=i+1}^N A’_{i’} \\right) \u0026 I \\end{bmatrix} \\begin{bmatrix} I \\\\ 0 \\end{bmatrix}\\\\ S_N \u0026= \\begin{bmatrix} 0 \u0026 I \\end{bmatrix} \\begin{bmatrix} \\prod_{i=1}^N \\prod_{j=1}^{n_h} A_{i,j} \u0026 0 \\\\ \\sum_{i=1}^N \\sum_{j=1}^{n_h} \\left( B_{i,j} \\left(\\prod_{j’=j+1}^{n_h} A_{i,j’}\\right) \\left(\\prod_{i’=i+1}^N \\prod_{j’=1}^{n_h} A_{i’,j’} \\right)\\right) \u0026 I \\end{bmatrix} \\begin{bmatrix} I \\\\ 0 \\end{bmatrix}\\\\ S_N \u0026= \\sum_{i=1}^N \\sum_{j=1}^{n_h} \\left( B_{i,j} \\underline{\\left(\\prod_{j’=j+1}^{n_h} A_{i,j’}\\right) \\left(\\prod_{i’=i+1}^N \\prod_{j’=1}^{n_h} A_{i’,j’} \\right)}\\right) \\end{align*} $$\nwhich, again, if we reindex this as $[\\cdot]_k = [\\cdot]_{k/n_h,\\space (k-1) \\% n_h + 1}$, we get:\n$$S_N = \\sum_{k=1}^{Nn_h} \\left( B_k \\prod_{k’=k+1}^{Nn_h} A_{k’}\\right)$$\nNow, let’s derive the $S_N$ for the linear attention mechanisms in the table above, but this time, with $n_h$ gradient descent steps per token.\nVanilla Linear Attention $$A_i = I \\quad\\quad B_i = \\bm{v}_i \\bm{k}_i^T$$ $$ \\begin{align*} S_N \u0026= \\sum_{i=1}^N \\sum_{j=1}^{n_h} \\left(\\bm{v}_{i,j} \\bm{k}_{i,j}^T \\left( \\prod_{j’=j+1}^{n_h} I \\right) \\left(\\prod_{i’=i+1}^N \\prod_{j’=1}^{n_h} I \\right)\\right)\\\\ S_N \u0026= \\sum_{i=1}^N \\sum_{j=1}^{n_h} \\bm{v}_{i,j} \\bm{k}_{i,j}^T\\\\ S_N \u0026= \\sum_{k=1}^{Nn_h} \\bm{v}_k \\bm{k}_k^T \\end{align*} $$\nMamba 2 $$A_i = \\text{diag}\\left(\\alpha_i I\\right) \\quad\\quad B_i = \\bm{v}_i \\bm{k}_i^T$$ $$ \\begin{align*} S_N \u0026= \\sum_{i=1}^N \\sum_{j=1}^{n_h} \\left( \\bm{v}_{i,j} \\bm{k}_{i,j}^T \\left(\\prod_{j’=j+1}^{n_h} \\text{diag}\\left(\\alpha_{i,j’} I\\right)\\right) \\left(\\prod_{i’=i+1}^N \\prod_{j’=1}^{n_h} \\text{diag}\\left(\\alpha_{i’,j’} I\\right) \\right)\\right)\\\\ S_N \u0026= \\sum_{i=1}^N \\sum_{j=1}^{n_h} \\left(\\underline{\\left( \\prod_{j’=j+1}^{n_h} \\alpha_{i,j’}\\right) \\left(\\prod_{i’=i+1}^N \\prod_{j’=1}^{n_h} \\alpha_{i’,j’} \\right)} \\right) \\bm{v}_{i,j} \\bm{k}_{i,j}^T\\\\ S_N \u0026= \\sum_{k=1}^{Nn_h} \\left(\\prod_{k’=k+1}^{Nn_h} \\alpha_{k’}\\right) \\bm{v}_k \\bm{k}_k^T \\end{align*} $$\nDeltaNet $$A_i = I - \\beta_i \\bm{k}_i \\bm{k}_i^T \\quad\\quad B_i = \\beta_i \\bm{v}_i \\bm{k}_i^T$$ $$ \\begin{align*} S_N \u0026= \\sum_{i=1}^N \\sum_{j=1}^{n_h} \\left( \\beta_{i,j} \\bm{v}_{i,j} \\bm{k}_{i,j}^T \\underline{\\left(\\prod_{j’=j+1}^{n_h} \\left(I - \\beta_{i,j’} \\bm{k}_{i,j’} \\bm{k}_{i,j’}^T\\right)\\right) \\left(\\prod_{i’=i+1}^N \\prod_{j’=1}^{n_h} \\left(I - \\beta_{i’,j’} \\bm{k}_{i’,j’} \\bm{k}_{i’,j’}^T\\right) \\right)}\\right)\\\\ S_N \u0026= \\sum_{k=1}^{Nn_h} \\left(\\beta_k \\bm{v}_k \\bm{k}_k^T \\prod_{k’=k+1}^{Nn_h} \\left(I - \\beta_{k’} \\bm{k}_{k’} \\bm{k}_{k’}^T\\right)\\right) \\end{align*} $$\nGated DeltaNet $$A_i = \\alpha_i(I - \\beta_i \\bm{k}_i \\bm{k}_i^T) \\quad\\quad B_i = \\beta_i \\bm{v}_i \\bm{k}_i^T$$ $$ \\begin{align*} S_N \u0026= \\sum_{i=1}^N \\sum_{j=1}^{n_h} \\left( \\beta_{i,j} \\bm{v}_{i,j} \\bm{k}_{i,j}^T \\underline{\\left(\\prod_{j’=j+1}^{n_h} \\alpha_{i,j’} \\left(I - \\beta_{i,j’} \\bm{k}_{i,j’} \\bm{k}_{i,j’}^T\\right)\\right) \\left(\\prod_{i’=i+1}^N \\prod_{j’=1}^{n_h} \\alpha_{i’,j’} \\left(I - \\beta_{i’,j’} \\bm{k}_{i’,j’} \\bm{k}_{i’,j’}^T\\right) \\right)}\\right)\\\\ S_N \u0026= \\sum_{k=1}^{Nn_h} \\left(\\beta_k \\bm{v}_k \\bm{k}_k^T \\prod_{k’=k+1}^{Nn_h} \\alpha_{k’} \\left(I - \\beta_{k’} \\bm{k}_{k’} \\bm{k}_{k’}^T\\right)\\right)\\\\ S_N \u0026= \\sum_{k=1}^{Nn_h} \\left(\\left( \\beta_k \\prod_{k’=k+1}^{Nn_h} \\alpha_{k’} \\right) \\bm{v}_k \\bm{k}_k^T \\prod_{k’=k+1}^{Nn_h} \\left(I - \\beta_{k’} \\bm{k}_{k’} \\bm{k}_{k’}^T\\right)\\right) \\end{align*} $$\nChunk-Wise Parallelism ",
  "wordCount" : "1363",
  "inLanguage": "en",
  "datePublished": "2025-03-14T00:00:00Z",
  "dateModified": "2025-03-14T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Franz Louis Cesista"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/ponder/test-time-regression-2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Franz Louis Cesista",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Franz Louis Cesista">
                <img src="http://localhost:1313/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Franz Louis Cesista</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/cv.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/personal-projects/" title="Personal Projects">
                    <span>Personal Projects</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/ponder/" title="Ponder (Posts)">
                    <span>Ponder (Posts)</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      The Blocked Matrix Formulation of Linear Attention Mechanisms
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="42" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-03-14 00:00:00 +0000 UTC'>March 14, 2025</span>&nbsp;&middot;&nbsp;Franz Louis Cesista

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#blocked-matrix-formulation">Blocked Matrix Formulation</a></li>
    <li><a href="#one-step-online-gradient-descent-per-token">One-Step Online Gradient Descent per Token</a></li>
    <li><a href="#multi-step-online-gradient-descent-per-token">Multi-Step Online Gradient Descent per Token</a></li>
    <li><a href="#chunk-wise-parallelism">Chunk-Wise Parallelism</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><p>In the previous post, we derived several linear attention mechanisms from scratch by formulating them as online test-time regression problems. Here, we&rsquo;ll discuss a more intuitive way to represent the update rules of these linear attention mechanisms using a blocked matrix formulation. Then, we&rsquo;ll discuss how to use it to (1) derive the update rules for linear attention mechanisms that take multiple gradient descent steps per token and (2) derive the update rules for chunk-wise parallelism of already-existing linear attention mechanisms.</p>
<h2 id="blocked-matrix-formulation">Blocked Matrix Formulation<a hidden class="anchor" aria-hidden="true" href="#blocked-matrix-formulation">#</a></h2>
<p>The update rules of linear attention mechanisms have the following structure:</p>
<p>$$S_i = S_{i-1}A_i + B_i$$</p>
<p>where $S_{i-1}$ is the (old) state at time step $i-1$, $S_i$ is the (new) state at time step $i$, and $A_i$ and $B_i$ are update matrices. In most cases, $A_i$ &ldquo;removes&rdquo; some information from the state while $B_i$ &ldquo;adds&rdquo; new information to the state. But if $A_i$ can have negative eigenvalues, we can think of it as &ldquo;inverting&rdquo; some information instead of merely &ldquo;removing&rdquo; it.</p>
<p>Here are a couple of examples:</p>
<table>
<thead>
<tr>
<th><strong>Linear Attention Mechanism</strong></th>
<th style="text-align:right"><strong>Update Rule</strong></th>
<th style="text-align:right"><strong>$A_i$</strong></th>
<th style="text-align:right"><strong>$B_i$</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Vanilla Linear Attention</td>
<td style="text-align:right">$S_i = S_{i-1} + \bm{v}_i \bm{k}_i^T$</td>
<td style="text-align:right">$I$</td>
<td style="text-align:right">$\bm{v}_i \bm{k}_i^T$</td>
</tr>
<tr>
<td>Mamba 2</td>
<td style="text-align:right">$S_i = S_{i-1}\text{diag}\left(\alpha_i I\right) + \bm{v}_i \bm{k}_i^T$</td>
<td style="text-align:right">$\text{diag}\left(\alpha_i I\right)$</td>
<td style="text-align:right">$\bm{v}_i \bm{k}_i^T$</td>
</tr>
<tr>
<td>DeltaNet</td>
<td style="text-align:right">$S_i = S_{i-1}(I - \beta_i \bm{k}_i \bm{k}_i^T) + \bm{v}_i \bm{k}_i^T$</td>
<td style="text-align:right">$I - \beta_i \bm{k}_i \bm{k}_i^T$</td>
<td style="text-align:right">$\beta_i \bm{v}_i \bm{k}_i^T$</td>
</tr>
<tr>
<td>Gated DeltaNet</td>
<td style="text-align:right">$S_i = S_{i-1}\alpha_i(I - \beta_i \bm{k}_i \bm{k}_i^T) + \beta_i \bm{v}_i \bm{k}_i^T$</td>
<td style="text-align:right">$\alpha_i(I - \beta_i \bm{k}_i \bm{k}_i^T)$</td>
<td style="text-align:right">$\beta_i \bm{v}_i \bm{k}_i^T$</td>
</tr>
</tbody>
</table>
<p>where $v_i$ and $k_i$ are the value and key at time step $i$, respectively.</p>
<p>Now, notice that the update rule above can be rewritten as:</p>
<p>$$
S_{i} =
\begin{bmatrix}
S_{i-1} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_i \\
B_i
\end{bmatrix}
$$
or, equivalently,
$$
\begin{bmatrix}
S_{i} &amp; I
\end{bmatrix} =
\begin{bmatrix}
S_{i-1} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_i &amp; 0 \\
B_i &amp; I
\end{bmatrix}
$$</p>
<p>Now, at training time, we need an efficient way to compute $S_i$ for all $i$. We can do this by unrolling the update rule for $N$ steps:</p>
<p>$$
\begin{align*}
\begin{bmatrix}
S_{N} &amp; I
\end{bmatrix} &amp;=
\begin{bmatrix}
S_{0} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_1 &amp; 0 \\
B_1 &amp; I
\end{bmatrix}
\begin{bmatrix}
A_2 &amp; 0 \\
B_2 &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A_N &amp; 0 \\
B_N &amp; I
\end{bmatrix}\\
S_N &amp;=
\begin{bmatrix}
S_{0} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_1 &amp; 0 \\
B_1 &amp; I
\end{bmatrix}
\begin{bmatrix}
A_2 &amp; 0 \\
B_2 &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A_N &amp; 0 \\
B_N &amp; I
\end{bmatrix}
\begin{bmatrix}
I \\
0
\end{bmatrix}
\end{align*}
$$</p>
<p>In practice, we usually initialize $S_0$ as the zero matrix. Thus,</p>
<p>$$
\begin{align}
S_N &amp;=
\begin{bmatrix}
0 &amp; I
\end{bmatrix}
\begin{bmatrix}
A_1 &amp; 0 \\
B_1 &amp; I
\end{bmatrix}
\begin{bmatrix}
A_2 &amp; 0 \\
B_2 &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A_N &amp; 0 \\
B_N &amp; I
\end{bmatrix}
\begin{bmatrix}
I \\
0
\end{bmatrix}\\
S_N &amp;=
\begin{bmatrix}
0 &amp; I
\end{bmatrix}
\begin{bmatrix}
\prod_{i=1}^{N} A_i &amp; 0 \\
\sum_{i=1}^{N} \left(B_i \prod_{j=i+1}^{N} A_j\right) &amp; I
\end{bmatrix}
\begin{bmatrix}
I \\
0
\end{bmatrix}\\
S_N &amp;= \sum_{i=1}^{N} \left(B_i \prod_{j=i+1}^{N} A_j\right)
\end{align}
$$
where $(1) \rightarrow (2)$ can be proven by induction.</p>
<p>Equation $(1)$ makes it obvious <em>why</em> and <em>how</em> we can parallelize computation of $S_N$, for all $N$, at training time: the updates are merely (blocked) matrix multiplications; matrix multiplications are associative; and we can easily parallelize associative operations!</p>
<h2 id="one-step-online-gradient-descent-per-token">One-Step Online Gradient Descent per Token<a hidden class="anchor" aria-hidden="true" href="#one-step-online-gradient-descent-per-token">#</a></h2>
<p>Let&rsquo;s derive $S_N$ for each of the linear attention mechanisms in the table above.</p>
<h3 id="vanilla-linear-attention">Vanilla Linear Attention<a hidden class="anchor" aria-hidden="true" href="#vanilla-linear-attention">#</a></h3>
<p>$$A_i = I \quad\quad B_i = \bm{v}_i \bm{k}_i^T$$
$$
\begin{align*}
S_N &amp;= \sum_{i=1}^{N} \left(\bm{v}_i \bm{k}_i^T \prod_{j=i+1}^{N} I\right)\\
S_N &amp;= \sum_{i=1}^{N} \bm{v}_i \bm{k}_i^T
\end{align*}
$$</p>
<h3 id="mamba-2">Mamba 2<a hidden class="anchor" aria-hidden="true" href="#mamba-2">#</a></h3>
<p>$$A_i = \text{diag}\left(\alpha_i I\right) \quad\quad B_i = \bm{v}_i \bm{k}_i^T$$
$$
\begin{align*}
S_N &amp;= \sum_{i=1}^{N} \left(\bm{v}_i \bm{k}_i^T \prod_{j=i+1}^{N} \text{diag}\left(\alpha_j I\right)\right)\\
S_N &amp;= \sum_{i=1}^{N} \left( \prod_{j=i+1}^{N} \alpha_j \right) \bm{v}_i \bm{k}_i^T
\end{align*}
$$</p>
<h3 id="deltanet">DeltaNet<a hidden class="anchor" aria-hidden="true" href="#deltanet">#</a></h3>
<p>$$A_i = I - \beta_i \bm{k}_i \bm{k}_i^T \quad\quad B_i = \beta_i \bm{v}_i \bm{k}_i^T$$
$$S_N = \sum_{i=1}^{N} \left(\beta_i \bm{v}_i \bm{k}_i^T \prod_{j=i+1}^{N} \left(I - \beta_j \bm{k}_j \bm{k}_j^T\right)\right)$$</p>
<h3 id="gated-deltanet">Gated DeltaNet<a hidden class="anchor" aria-hidden="true" href="#gated-deltanet">#</a></h3>
<p>$$A_i = \alpha_i(I - \beta_i \bm{k}_i \bm{k}_i^T) \quad\quad B_i = \beta_i \bm{v}_i \bm{k}_i^T$$
$$
\begin{align*}
S_N &amp;= \sum_{i=1}^{N} \left(\beta_i \bm{v}_i \bm{k}_i^T \prod_{j=i+1}^{N} \alpha_j \left(I - \beta_j \bm{k}_j \bm{k}_j^T\right)\right)\\
S_N &amp;= \sum_{i=1}^{N} \left(\left(\beta_i \prod_{j=i+1}^{N} \alpha_j \right) \bm{v}_i \bm{k}_i^T \prod_{j=i+1}^{N} \left(I - \beta_j \bm{k}_j \bm{k}_j^T\right)\right)
\end{align*}
$$</p>
<p>Easy!</p>
<hr>
<h2 id="multi-step-online-gradient-descent-per-token">Multi-Step Online Gradient Descent per Token<a hidden class="anchor" aria-hidden="true" href="#multi-step-online-gradient-descent-per-token">#</a></h2>
<p>Now, what if we want to take $n_h$ gradient descent steps per token?</p>
<p>To do this, we can follow the procedure outlined in the DeltaProduct paper where they:</p>
<ol>
<li>Recurrently generate $n_h$ intermediate tokens for each input token (including the input token itself),</li>
<li>Calculate the key-value pairs for each intermediate token,</li>
<li>Update the state using the key-value pairs, and</li>
<li>Discard the outputs except for the last one to maintain the number of tokens.</li>
</ol>
<p>So, instead of updating the state only once per token, we update the state $n_h$ times per token. And to get the final state for each token $S_N$, we expand equation $(1)$ above:
$$
\begin{align}
S_N =
\begin{bmatrix}
0 &amp; I
\end{bmatrix}
\begin{bmatrix}
A_{1,1} &amp; 0 \\
B_{1,1} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_{1,2} &amp; 0 \\
B_{1,2} &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A_{1,n_h} &amp; 0 \\
B_{1,n_h} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_{2,1} &amp; 0 \\
B_{2,1} &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A_{N, n_h} &amp; 0 \\
B_{N, n_h} &amp; I
\end{bmatrix}
\begin{bmatrix}
I \\
0
\end{bmatrix}
\end{align}
$$
where $S_{i,j}$, $A_{i,j}$, and $B_{i,j}$ are the state and update matrices for the $j$-th gradient descent step for the $i$-th token.</p>
<p>And if we reindex this as $[\cdot]_k = [\cdot]_{k/n_h,\space (k-1) \% n_h + 1}$, then from equation $(3)$ above, we get:
$$S_N = \sum_{k=1}^{Nn_h} \left( B_k \prod_{k&rsquo;=k+1}^{Nn_h} A_{k&rsquo;}\right)$$</p>
<p>Alternatively, we can also combine the updates for each token into a single update matrix first before multiplying them together:</p>
<p>$$
\begin{bmatrix}
A&rsquo;_{i} &amp; 0 \\
B&rsquo;_{i} &amp; I
\end{bmatrix}
= \prod_{j=1}^{n_h}
\begin{bmatrix}
A_{i,j} &amp; 0 \\
B_{i,j} &amp; I
\end{bmatrix}
= \begin{bmatrix}
\prod_{j=1}^{n_h} A_{i,j} &amp; 0 \\
\sum_{j=1}^{n_h} \left(B_{i,j} \prod_{j&rsquo;=j+1}^{n_h} A_{i,j&rsquo;}\right) &amp; I
\end{bmatrix}
$$</p>
<p>$$
\begin{align*}
S_N &amp;=
\begin{bmatrix}
0 &amp; I
\end{bmatrix}
\begin{bmatrix}
A&rsquo;_1 &amp; 0 \\
B&rsquo;_1 &amp; I
\end{bmatrix}
\begin{bmatrix}
A&rsquo;_2 &amp; 0 \\
B&rsquo;_2 &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A&rsquo;_N &amp; 0 \\
B&rsquo;_N &amp; I
\end{bmatrix}
\begin{bmatrix}
I \\
0
\end{bmatrix}\\
S_N &amp;=
\begin{bmatrix}
0 &amp; I
\end{bmatrix}
\begin{bmatrix}
\prod_{i=1}^N A&rsquo;_i &amp; 0 \\
\sum_{i=1}^N \left( B&rsquo;_i \prod_{i&rsquo;=i+1}^N A&rsquo;_{i&rsquo;} \right) &amp; I
\end{bmatrix}
\begin{bmatrix}
I \\
0
\end{bmatrix}\\
S_N &amp;=
\begin{bmatrix}
0 &amp; I
\end{bmatrix}
\begin{bmatrix}
\prod_{i=1}^N \prod_{j=1}^{n_h} A_{i,j} &amp; 0 \\
\sum_{i=1}^N \sum_{j=1}^{n_h} \left( B_{i,j} \left(\prod_{j&rsquo;=j+1}^{n_h} A_{i,j&rsquo;}\right) \left(\prod_{i&rsquo;=i+1}^N \prod_{j&rsquo;=1}^{n_h} A_{i&rsquo;,j&rsquo;} \right)\right) &amp; I
\end{bmatrix}
\begin{bmatrix}
I \\
0
\end{bmatrix}\\
S_N &amp;= \sum_{i=1}^N \sum_{j=1}^{n_h} \left( B_{i,j} \underline{\left(\prod_{j&rsquo;=j+1}^{n_h} A_{i,j&rsquo;}\right) \left(\prod_{i&rsquo;=i+1}^N \prod_{j&rsquo;=1}^{n_h} A_{i&rsquo;,j&rsquo;} \right)}\right)
\end{align*}
$$</p>
<p>which, again, if we reindex this as $[\cdot]_k = [\cdot]_{k/n_h,\space (k-1) \% n_h + 1}$, we get:</p>
<p>$$S_N = \sum_{k=1}^{Nn_h} \left( B_k \prod_{k&rsquo;=k+1}^{Nn_h} A_{k&rsquo;}\right)$$</p>
<hr>
<p>Now, let&rsquo;s derive the $S_N$ for the linear attention mechanisms in the table above, but this time, with $n_h$ gradient descent steps per token.</p>
<h3 id="vanilla-linear-attention-1">Vanilla Linear Attention<a hidden class="anchor" aria-hidden="true" href="#vanilla-linear-attention-1">#</a></h3>
<p>$$A_i = I \quad\quad B_i = \bm{v}_i \bm{k}_i^T$$
$$
\begin{align*}
S_N &amp;= \sum_{i=1}^N \sum_{j=1}^{n_h} \left(\bm{v}_{i,j} \bm{k}_{i,j}^T \left( \prod_{j&rsquo;=j+1}^{n_h} I \right) \left(\prod_{i&rsquo;=i+1}^N \prod_{j&rsquo;=1}^{n_h} I \right)\right)\\
S_N &amp;= \sum_{i=1}^N \sum_{j=1}^{n_h} \bm{v}_{i,j} \bm{k}_{i,j}^T\\
S_N &amp;= \sum_{k=1}^{Nn_h} \bm{v}_k \bm{k}_k^T
\end{align*}
$$</p>
<h3 id="mamba-2-1">Mamba 2<a hidden class="anchor" aria-hidden="true" href="#mamba-2-1">#</a></h3>
<p>$$A_i = \text{diag}\left(\alpha_i I\right) \quad\quad B_i = \bm{v}_i \bm{k}_i^T$$
$$
\begin{align*}
S_N &amp;= \sum_{i=1}^N \sum_{j=1}^{n_h} \left( \bm{v}_{i,j} \bm{k}_{i,j}^T \left(\prod_{j&rsquo;=j+1}^{n_h} \text{diag}\left(\alpha_{i,j&rsquo;} I\right)\right) \left(\prod_{i&rsquo;=i+1}^N \prod_{j&rsquo;=1}^{n_h} \text{diag}\left(\alpha_{i&rsquo;,j&rsquo;} I\right) \right)\right)\\
S_N &amp;= \sum_{i=1}^N \sum_{j=1}^{n_h} \left(\underline{\left( \prod_{j&rsquo;=j+1}^{n_h} \alpha_{i,j&rsquo;}\right) \left(\prod_{i&rsquo;=i+1}^N \prod_{j&rsquo;=1}^{n_h} \alpha_{i&rsquo;,j&rsquo;} \right)} \right) \bm{v}_{i,j} \bm{k}_{i,j}^T\\
S_N &amp;= \sum_{k=1}^{Nn_h} \left(\prod_{k&rsquo;=k+1}^{Nn_h} \alpha_{k&rsquo;}\right) \bm{v}_k \bm{k}_k^T
\end{align*}
$$</p>
<h3 id="deltanet-1">DeltaNet<a hidden class="anchor" aria-hidden="true" href="#deltanet-1">#</a></h3>
<p>$$A_i = I - \beta_i \bm{k}_i \bm{k}_i^T \quad\quad B_i = \beta_i \bm{v}_i \bm{k}_i^T$$
$$
\begin{align*}
S_N &amp;= \sum_{i=1}^N \sum_{j=1}^{n_h} \left( \beta_{i,j} \bm{v}_{i,j} \bm{k}_{i,j}^T \underline{\left(\prod_{j&rsquo;=j+1}^{n_h} \left(I - \beta_{i,j&rsquo;} \bm{k}_{i,j&rsquo;} \bm{k}_{i,j&rsquo;}^T\right)\right) \left(\prod_{i&rsquo;=i+1}^N \prod_{j&rsquo;=1}^{n_h} \left(I - \beta_{i&rsquo;,j&rsquo;} \bm{k}_{i&rsquo;,j&rsquo;} \bm{k}_{i&rsquo;,j&rsquo;}^T\right) \right)}\right)\\
S_N &amp;= \sum_{k=1}^{Nn_h} \left(\beta_k \bm{v}_k \bm{k}_k^T \prod_{k&rsquo;=k+1}^{Nn_h} \left(I - \beta_{k&rsquo;} \bm{k}_{k&rsquo;} \bm{k}_{k&rsquo;}^T\right)\right)
\end{align*}
$$</p>
<h3 id="gated-deltanet-1">Gated DeltaNet<a hidden class="anchor" aria-hidden="true" href="#gated-deltanet-1">#</a></h3>
<p>$$A_i = \alpha_i(I - \beta_i \bm{k}_i \bm{k}_i^T) \quad\quad B_i = \beta_i \bm{v}_i \bm{k}_i^T$$
$$
\begin{align*}
S_N &amp;= \sum_{i=1}^N \sum_{j=1}^{n_h} \left( \beta_{i,j} \bm{v}_{i,j} \bm{k}_{i,j}^T \underline{\left(\prod_{j&rsquo;=j+1}^{n_h} \alpha_{i,j&rsquo;} \left(I - \beta_{i,j&rsquo;} \bm{k}_{i,j&rsquo;} \bm{k}_{i,j&rsquo;}^T\right)\right) \left(\prod_{i&rsquo;=i+1}^N \prod_{j&rsquo;=1}^{n_h} \alpha_{i&rsquo;,j&rsquo;} \left(I - \beta_{i&rsquo;,j&rsquo;} \bm{k}_{i&rsquo;,j&rsquo;} \bm{k}_{i&rsquo;,j&rsquo;}^T\right) \right)}\right)\\
S_N &amp;= \sum_{k=1}^{Nn_h} \left(\beta_k \bm{v}_k \bm{k}_k^T \prod_{k&rsquo;=k+1}^{Nn_h} \alpha_{k&rsquo;} \left(I - \beta_{k&rsquo;} \bm{k}_{k&rsquo;} \bm{k}_{k&rsquo;}^T\right)\right)\\
S_N &amp;= \sum_{k=1}^{Nn_h} \left(\left( \beta_k \prod_{k&rsquo;=k+1}^{Nn_h} \alpha_{k&rsquo;} \right) \bm{v}_k \bm{k}_k^T \prod_{k&rsquo;=k+1}^{Nn_h} \left(I - \beta_{k&rsquo;} \bm{k}_{k&rsquo;} \bm{k}_{k&rsquo;}^T\right)\right)
\end{align*}
$$</p>
<hr>
<h2 id="chunk-wise-parallelism">Chunk-Wise Parallelism<a hidden class="anchor" aria-hidden="true" href="#chunk-wise-parallelism">#</a></h2>
<p><img loading="lazy" src="chunkwise-parallelism.png#center" alt=""  />
</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="http://localhost:1313/tags/linear-attention/">Linear Attention</a></li>
      <li><a href="http://localhost:1313/tags/test-time-regression/">Test-Time Regression</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The Blocked Matrix Formulation of Linear Attention Mechanisms on x"
            href="https://x.com/intent/tweet/?text=The%20Blocked%20Matrix%20Formulation%20of%20Linear%20Attention%20Mechanisms&amp;url=http%3a%2f%2flocalhost%3a1313%2fponder%2ftest-time-regression-2%2f&amp;hashtags=MachineLearning%2cLinearAttention%2cTest-TimeRegression">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The Blocked Matrix Formulation of Linear Attention Mechanisms on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fponder%2ftest-time-regression-2%2f&amp;title=The%20Blocked%20Matrix%20Formulation%20of%20Linear%20Attention%20Mechanisms&amp;summary=The%20Blocked%20Matrix%20Formulation%20of%20Linear%20Attention%20Mechanisms&amp;source=http%3a%2f%2flocalhost%3a1313%2fponder%2ftest-time-regression-2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The Blocked Matrix Formulation of Linear Attention Mechanisms on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fponder%2ftest-time-regression-2%2f&title=The%20Blocked%20Matrix%20Formulation%20of%20Linear%20Attention%20Mechanisms">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The Blocked Matrix Formulation of Linear Attention Mechanisms on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fponder%2ftest-time-regression-2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The Blocked Matrix Formulation of Linear Attention Mechanisms on whatsapp"
            href="https://api.whatsapp.com/send?text=The%20Blocked%20Matrix%20Formulation%20of%20Linear%20Attention%20Mechanisms%20-%20http%3a%2f%2flocalhost%3a1313%2fponder%2ftest-time-regression-2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The Blocked Matrix Formulation of Linear Attention Mechanisms on telegram"
            href="https://telegram.me/share/url?text=The%20Blocked%20Matrix%20Formulation%20of%20Linear%20Attention%20Mechanisms&amp;url=http%3a%2f%2flocalhost%3a1313%2fponder%2ftest-time-regression-2%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The Blocked Matrix Formulation of Linear Attention Mechanisms on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=The%20Blocked%20Matrix%20Formulation%20of%20Linear%20Attention%20Mechanisms&u=http%3a%2f%2flocalhost%3a1313%2fponder%2ftest-time-regression-2%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="http://localhost:1313/">Franz Louis Cesista</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
