<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>(Linear) Attention as Test-Time Regression | Franz Louis Cesista</title>
<meta name="keywords" content="Machine Learning, Linear Attention, Test-Time Regression">
<meta name="description" content="A unifying framework for linear attention mechanisms as test-time regression and how to parallelize training and inference.">
<meta name="author" content="Franz Louis Cesista">
<link rel="canonical" href="https://leloykun.github.io/ponder/test-time-regression/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d75186359589da78e189364979a1944c630638f9f001edf480c76c751674ee86.css" integrity="sha256-11GGNZWJ2njhiTZJeaGUTGMGOPnwAe30gMdsdRZ07oY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://leloykun.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://leloykun.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://leloykun.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://leloykun.github.io/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://leloykun.github.io/ponder/test-time-regression/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="(Linear) Attention as Test-Time Regression" />
<meta property="og:description" content="A unifying framework for linear attention mechanisms as test-time regression and how to parallelize training and inference." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://leloykun.github.io/ponder/test-time-regression/" />
<meta property="og:image" content="https://leloykun.github.io/ponder/test-time-regression/cover.jpg" /><meta property="article:section" content="ponder" />
<meta property="article:published_time" content="2025-01-27T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-01-27T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://leloykun.github.io/ponder/test-time-regression/cover.jpg" />
<meta name="twitter:title" content="(Linear) Attention as Test-Time Regression"/>
<meta name="twitter:description" content="A unifying framework for linear attention mechanisms as test-time regression and how to parallelize training and inference."/>
<meta name="twitter:site" content="@leloykun"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Ponder",
      "item": "https://leloykun.github.io/ponder/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "(Linear) Attention as Test-Time Regression",
      "item": "https://leloykun.github.io/ponder/test-time-regression/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "(Linear) Attention as Test-Time Regression",
  "name": "(Linear) Attention as Test-Time Regression",
  "description": "A unifying framework for linear attention mechanisms as test-time regression and how to parallelize training and inference.",
  "keywords": [
    "Machine Learning", "Linear Attention", "Test-Time Regression"
  ],
  "articleBody": " Note: This was originally posted as a Twitter thread. I’ve reformatted it here for better readability.\nBy now, you’ve probably already heard of linear attention, in-context learning, test-time scaling, and etc…\nHere, I’ll discuss:\nThe unifying framework that ties them all together; How to derive different linear attention variants from scratch; and How to parallelize training linear attention models Learning how to learn in-context First, what do we want from an ideal language model?\nIn-context learning: At inference time, we want the model to learn from information it has ingested so far and use that information to make more accurate predictions moving forward.\nComputational expressivity: We want it to have a complex-enough internal machinery so it can actually solve hard problems we encounter in real-life. And\nEfficiency.\nOf course, there are other stuff we’d want. But these are the most important ones. And here, we’ll focus on the first one.\nAssociative Recall To teach the models to learn in context, let’s steal an idea from nature: Associative Recall\nMom’s cooking triggers memories of my childhood Live wires remind me of the time I got electrocuted That’s associative recall.\nA “cue” goes into the brain and a “response” comes out. And the brain learns this “cue”-“response” mapping automatically through experience.\nWe want our models to learn how to do this too. But in practice, we call the “cue” the “key” and the “response” the “value” (following the Attention is All You Need$^{[1]}$ paper).\nFrom here, we have a (major) architectural design decision to make: Either we let the model’s “state” grow with the context length, or… we fix it at a certain size.\nThe former allows us to keep as much information as we can as we chug through the context. This, in turn, helps with the model’s expressivity as there is little to no information loss.\nThe latter, on the other hand, is much more efficient at the cost of expressivity. There’s an upper limit on how much information we can store in this state. And there’s also the question of how we’re gonna teach the model to learn which information are important enough to store and which to discard.\nFor the rest of the thread, I’ll focus on linear attention… I’ll make another thread for the former case (stay tuned!).\nHeirarchical optimization process with (linear) attention mechanisms Now, another design decision we need to make is how to map the input context into key-value pairs.\nInterestingly, this results in a two-layer optimization process:\nThe “outer model” optimizes the mapping from the input context into key-value pairs.\nWhile the “inner model” treats the outer model as a black box and simply optimizes its state to better predict the values from the keys.\nAnd with more modern optimizers, such as Shampoo/PSGD, you can actually think of this as a three-layer optimization process because:\nThe optimizer is also trying to learn the geometry of the loss landscape by adjusting the gradient preconditioners. Deriving linear attention Mechanisms from first principles If the “inner model” is optimizing something, then what is it optimizing? Again, we need to make another design decision here on which loss function to use. But which one is the most appropriate?\nTo simulate associative recall, we want the model to learn a mapping from the keys to the values. In other words, we want to learn a function:\n$$M : \\text{key} \\rightarrow \\text{value}$$\nThus, we need to define a distance metric between the model’s prediction and the actual value:\n$$\\text{loss}_M(\\text{key}, \\text{value}) = \\text{distance}(M(\\text{key}), \\text{value})$$\nQuestion is, how do we define this “distance”?\nIn practice, we’ve pretty much settled on two of the most basic distance metrics:\nThe negative dot product. Minimizing this is equivalent to maximizing the dot product or the “alignment” between $Mk$ and $v$. The (squared) Euclidean distance. Minimizing this is equivalent to doing (linear) regression between the keys and the values. From here, we can add the tricks we’ve learned so far from designing optimizers one-by-one to arrive at different variants of linear attention.\nIf we pick the negative dot product loss and do online gradient descent, we’ll get Vanilla Linear Attention. If we add a data-independent weight decay, we’ll get Lightning Attention 2 that’s used in the MiniMax-O1 paper. If we make the weight decay data dependent instead, we’ll get Mamba 2 that was all the rave last year. Now, if we pick the Euclidean loss instead, we’ll get the Vanilla DeltaNet. If we add a data-dependent weight decay, we’ll get Gated DeltaNet. Then we fork from here: If we use non-linear key -\u003e value transforms, we’ll get TTT. But if we add a momentum term instead, we’ll get the newly released Titans. How to design flash kernels for linear attention mechanisms Let’s go back to my claim earlier that linear attention mechanisms are more efficient. This is clearly true at inference time because, unlike (Vaswani) Softmax Attention, we don’t need to loop through all the previous key-value pairs to make a prediction–we only need to update the state with the new key-value pair and use that updated state to make a prediction.\nBut what about at training time? Can we parallelize training of linear attention mechanisms?\nRemember: the primary reason pretty much everyone dropped RNNs in favor of (Vaswani) Softmax Attention is that the latter is very easy to parallelize. Thus, we can scale it up better. And scale is, often times, all you need.\nA sufficient condition for parallelizing training As a rule of thumb, if you can recast your update rule as an associative operation over sequences, then you can parallelize it! This is, of course, only a sufficient, but not necessary, condition. There are non-associative operations that can be parallelized too with Chunk-wise parallelism.\nNote that there are faster ways to implement DeltaNet’s update rule (e.g. WY representations, etc.). We’ll discuss that next time!\nComputational forms of paralleled training In practice, how do we actually calculate the running “sums” efficiently? Remember those leetcode job interview data structure questions you hate? Well… this is when they become relevant…\nThe most naive way is to simply run a loop through the key-value pairs. This is the recurrent form, and this is what we should be doing at inference time. But we can do much better than this.\nOn the other extreme end is the fully-parallel associative scan where we first aggregate the running sums by powers of two, then do a second pass to propagate the running sums across the sequence. If you’ve implemented a Fenwick Tree before, this is roughly how it works.\nBut in practice, we use chunk-wise parallelism where we:\nDivide the sequence into chunks. Use fully-parallel associative scan within each chunk. And Use the recurrent form to propagate running sums across chunks. That’s it for now. Next time, I plan to talk about:\nHow to derive different attention mechanisms using tensor string diagrams. Circuit complexity of different attention mechanisms. LLM reasoning. Stay tuned!\nHow to Cite @misc{cesista2025linearattn, author = {Franz Louis Cesista}, title = {({L}inear) {A}ttention as {T}est-{T}ime {R}egression}, year = {2025}, url = {https://leloykun.github.io/ponder/test-time-regression/}, } References Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … \u0026 Polosukhin, I. (2017). Attention is all you need. URL https://arxiv.org/abs/1706.03762 Gupta, V., Koren, T., \u0026 Singer, Y. (2018). Shampoo: Preconditioned Stochastic Tensor Optimization. URL https://arxiv.org/abs/1802.09568 Wang, K., Shi, J., Fox., E. (2025). Test-time regression: a unifying framework for designing sequence models with associative memory. URL https://arxiv.org/abs/2501.12352 Yang, S. (2025). What’s Next for Mamba? Towards More Expressive Recurrent Update Rules. URL https://sustcsonglin.github.io/assets/pdf/talk_250117.pdf Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 5156–5165. PMLR, 2020b. URL http://proceedings.mlr.press/v119/katharopoulos20a.html. Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In Proceedings of the 41st International Conference on MachineLearning, volume 235 of Proceedingsof Machine Learning Research, pp. 10041–10071. PMLR, 2024b. URL https://proceedings.mlr.press/v235/dao24a.html. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim (2025). Parallelizing Linear Transformers with the Delta Rule over Sequence Length. URL https://arxiv.org/abs/2406.06484 Songlin Yang, Jan Kautz, Ali Hatamizadeh (2025). Gated Delta Networks: Improving Mamba2 with Delta Rule. URL https://arxiv.org/abs/2412.06464 Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 9099–9117. PMLR, 2022b. URL https://proceedings.mlr.press/v162/hua22a.html. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. ArXiv preprint, abs/2307.08621, 2023. URL https://arxiv.org/abs/2307.08621. ",
  "wordCount" : "1473",
  "inLanguage": "en",
  "image":"https://leloykun.github.io/ponder/test-time-regression/cover.jpg","datePublished": "2025-01-27T00:00:00Z",
  "dateModified": "2025-01-27T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Franz Louis Cesista"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://leloykun.github.io/ponder/test-time-regression/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Franz Louis Cesista",
    "logo": {
      "@type": "ImageObject",
      "url": "https://leloykun.github.io/favicon.ico"
    }
  }
}
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-BWCGBRX8G1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BWCGBRX8G1');
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"
  integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"
  integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js"
  integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
        { left: "\\begin{equation}", right: "\\end{equation}", display: true },
        { left: "\\begin{equation*}", right: "\\end{equation*}", display: true },
        { left: "\\begin{align}", right: "\\end{align}", display: true },
        { left: "\\begin{align*}", right: "\\end{align*}", display: true },
        { left: "\\begin{alignat}", right: "\\end{alignat}", display: true },
        { left: "\\begin{gather}", right: "\\end{gather}", display: true },
        { left: "\\begin{CD}", right: "\\end{CD}", display: true },
      ],
      throwOnError: false,
      trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
      macros: {
        "\\eqref": "\\href{###1}{(\\text{#1})}",
        "\\ref": "\\href{###1}{\\text{#1}}",
        "\\label": "\\htmlId{#1}{}"
      }
    });
  });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://leloykun.github.io/" accesskey="h" title="Franz Louis Cesista">
                <img src="https://leloykun.github.io/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Franz Louis Cesista</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://leloykun.github.io/cv.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/personal-projects/" title="Personal Projects">
                    <span>Personal Projects</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/ponder/" title="Ponder (Posts)">
                    <span>Ponder (Posts)</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      (Linear) Attention as Test-Time Regression
    </h1>
    <div class="post-meta"><span title='2025-01-27 00:00:00 +0000 UTC'>January 27, 2025</span>&nbsp;&middot;&nbsp;Franz Louis Cesista&nbsp;&middot;&nbsp;<a href="https://x.com/leloykun/status/1883634169902952655" rel="noopener noreferrer" target="_blank">Crossposted from X (formerly Twitter)</a>

</div>
  </header> 
<figure class="entry-cover">
            <img loading="eager"
                srcset='https://leloykun.github.io/ponder/test-time-regression/cover_hu_578158ffab31b2ae.jpg 360w,https://leloykun.github.io/ponder/test-time-regression/cover_hu_74a4aaf981023949.jpg 480w,https://leloykun.github.io/ponder/test-time-regression/cover_hu_b0ae4ade20b1b1cd.jpg 720w,https://leloykun.github.io/ponder/test-time-regression/cover_hu_5dbecbf7d2318feb.jpg 1080w,https://leloykun.github.io/ponder/test-time-regression/cover_hu_54783ac7c7de8506.jpg 1500w,https://leloykun.github.io/ponder/test-time-regression/cover.jpg 5423w'
                src="https://leloykun.github.io/ponder/test-time-regression/cover.jpg"
                sizes="(min-width: 768px) 720px, 100vw"
                width="5423" height="2951"
                alt="Cover">
        
</figure><div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#learning-how-to-learn-in-context">Learning how to learn in-context</a>
      <ul>
        <li><a href="#associative-recall">Associative Recall</a></li>
        <li><a href="#heirarchical-optimization-process-with-linear-attention-mechanisms">Heirarchical optimization process with (linear) attention mechanisms</a></li>
      </ul>
    </li>
    <li><a href="#deriving-linear-attention-mechanisms-from-first-principles">Deriving linear attention Mechanisms from first principles</a></li>
    <li><a href="#how-to-design-flash-kernels-for-linear-attention-mechanisms">How to design flash kernels for linear attention mechanisms</a>
      <ul>
        <li><a href="#a-sufficient-condition-for-parallelizing-training">A sufficient condition for parallelizing training</a></li>
        <li><a href="#computational-forms-of-paralleled-training">Computational forms of paralleled training</a></li>
      </ul>
    </li>
    <li><a href="#how-to-cite">How to Cite</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>Note: This was originally posted as a Twitter thread. I&rsquo;ve reformatted it here for better readability.</p></blockquote>
<p>By now, you&rsquo;ve probably already heard of linear attention, in-context learning, test-time scaling, and etc&hellip;</p>
<p>Here, I&rsquo;ll discuss:</p>
<ol>
<li>The unifying framework that ties them all together;</li>
<li>How to derive different linear attention variants from scratch; and</li>
<li>How to parallelize training linear attention models</li>
</ol>
<h2 id="learning-how-to-learn-in-context">Learning how to learn in-context<a hidden class="anchor" aria-hidden="true" href="#learning-how-to-learn-in-context">#</a></h2>
<p>First, what do we want from an <em>ideal</em> language model?</p>
<ol>
<li>
<p><strong>In-context learning</strong>: <em>At inference time</em>, we want the model to learn from information it has ingested so far and use that information to make more accurate predictions moving forward.</p>
</li>
<li>
<p><strong>Computational expressivity</strong>: We want it to have a complex-enough internal machinery so it can actually solve hard problems we encounter in real-life. And</p>
</li>
<li>
<p><strong>Efficiency</strong>.</p>
</li>
</ol>
<p>Of course, there are other stuff we&rsquo;d want. But these are the most important ones. And here, we&rsquo;ll focus on the first one.</p>
<h3 id="associative-recall">Associative Recall<a hidden class="anchor" aria-hidden="true" href="#associative-recall">#</a></h3>
<p>To teach the models to learn in context, let&rsquo;s steal an idea from nature: <strong>Associative Recall</strong></p>
<ul>
<li>Mom&rsquo;s cooking triggers memories of my childhood</li>
<li>Live wires remind me of the time I got electrocuted</li>
</ul>
<p>That&rsquo;s associative recall.</p>
<div align="center">
    <img src="associative-recall.png" style="width:80%; height:80%" />
</div>
<p>A &ldquo;cue&rdquo; goes into the brain and a &ldquo;response&rdquo; comes out. And the brain learns this &ldquo;cue&rdquo;-&ldquo;response&rdquo; mapping automatically through experience.</p>
<p>We want our models to learn how to do this too. But in practice, we call the &ldquo;cue&rdquo; the &ldquo;key&rdquo; and the &ldquo;response&rdquo; the &ldquo;value&rdquo; (following the Attention is All You Need$^{[1]}$ paper).</p>
<hr>
<p>From here, we have a (major) architectural design decision to make: Either we let the model&rsquo;s &ldquo;state&rdquo; grow with the context length, or&hellip; we fix it at a certain size.</p>
<p>The former allows us to keep as much information as we can as we chug through the context. This, in turn, helps with the model&rsquo;s expressivity as there is little to no information loss.</p>
<p>The latter, on the other hand, is <em>much</em> more efficient at the cost of expressivity. There&rsquo;s an upper limit on how much information we can store in this state. And there&rsquo;s also the question of <em>how</em> we&rsquo;re gonna teach the model to learn which information are important enough to store and which to discard.</p>
<div align="center">
    <img src="vaswani-vs-linear-attention.png" style="width:90%; height:90%;" />
</div>
<p>For the rest of the thread, I&rsquo;ll focus on linear attention&hellip; I&rsquo;ll make another thread for the former case (stay tuned!).</p>
<h3 id="heirarchical-optimization-process-with-linear-attention-mechanisms">Heirarchical optimization process with (linear) attention mechanisms<a hidden class="anchor" aria-hidden="true" href="#heirarchical-optimization-process-with-linear-attention-mechanisms">#</a></h3>
<p>Now, another design decision we need to make is how to map the input context into key-value pairs.</p>
<p>Interestingly, this results in a two-layer optimization process:</p>
<ol>
<li>
<p>The &ldquo;outer model&rdquo; optimizes the mapping from the input context into key-value pairs.</p>
</li>
<li>
<p>While the &ldquo;inner model&rdquo; treats the outer model as a black box and simply optimizes its state to better predict the values from the keys.</p>
</li>
</ol>
<p><img loading="lazy" src="/ponder/test-time-regression/inner-opt.png#center"></p>
<p>And with more modern optimizers, such as Shampoo/PSGD, you can actually think of this as a three-layer optimization process because:</p>
<ol start="3">
<li>The optimizer is also trying to learn the geometry of the loss landscape by adjusting the gradient preconditioners.</li>
</ol>
<h2 id="deriving-linear-attention-mechanisms-from-first-principles">Deriving linear attention Mechanisms from first principles<a hidden class="anchor" aria-hidden="true" href="#deriving-linear-attention-mechanisms-from-first-principles">#</a></h2>
<p>If the &ldquo;inner model&rdquo; is optimizing something, then what is it optimizing? Again, we need to make another design decision here on which loss function to use. But which one is the most appropriate?</p>
<p>To simulate associative recall, we want the model to learn a mapping from the keys to the values. In other words, we want to learn a function:</p>
<p>$$M : \text{key} \rightarrow \text{value}$$</p>
<p>Thus, we need to define a distance metric between the model&rsquo;s prediction and the actual value:</p>
<p>$$\text{loss}_M(\text{key}, \text{value}) = \text{distance}(M(\text{key}), \text{value})$$</p>
<p>Question is, how do we define this &ldquo;distance&rdquo;?</p>
<p><img loading="lazy" src="/ponder/test-time-regression/linear-attn-loss-functions.png#center"></p>
<p>In practice, we&rsquo;ve pretty much settled on two of the most basic distance metrics:</p>
<ol>
<li>The <strong>negative dot product</strong>. Minimizing this is equivalent to maximizing the dot product or the &ldquo;alignment&rdquo; between $Mk$ and $v$.</li>
<li>The <strong>(squared) Euclidean distance</strong>. Minimizing this is equivalent to doing (linear) regression between the keys and the values.</li>
</ol>
<hr>
<p><img alt="Deriving Linear Attention Mechanisms from the Loss Functions they Optimize" loading="lazy" src="/ponder/test-time-regression/linear-attention-derivations.png#center"></p>
<p>From here, we can add the tricks we&rsquo;ve learned so far from designing optimizers one-by-one to arrive at different variants of linear attention.</p>
<ul>
<li>If we pick the negative dot product loss and do online gradient descent, we&rsquo;ll get Vanilla Linear Attention.
<ul>
<li>If we add a data-independent weight decay, we&rsquo;ll get Lightning Attention 2 that&rsquo;s used in the MiniMax-O1 paper.</li>
<li>If we make the weight decay data dependent instead, we&rsquo;ll get Mamba 2 that was all the rave last year.</li>
</ul>
</li>
<li>Now, if we pick the Euclidean loss instead, we&rsquo;ll get the Vanilla DeltaNet.
<ul>
<li>If we add a data-dependent weight decay, we&rsquo;ll get Gated DeltaNet.</li>
<li>Then we fork from here:
<ul>
<li>If we use non-linear key -&gt; value transforms, we&rsquo;ll get TTT.</li>
<li>But if we add a momentum term instead, we&rsquo;ll get the newly released Titans.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="how-to-design-flash-kernels-for-linear-attention-mechanisms">How to design flash kernels for linear attention mechanisms<a hidden class="anchor" aria-hidden="true" href="#how-to-design-flash-kernels-for-linear-attention-mechanisms">#</a></h2>
<p>Let&rsquo;s go back to my claim earlier that linear attention mechanisms are more efficient. This is clearly true at inference time because, unlike (Vaswani) Softmax Attention, we don&rsquo;t need to loop through all the previous key-value pairs to make a prediction&ndash;we only need to update the state with the new key-value pair and use that updated state to make a prediction.</p>
<p>But what about at training time? Can we parallelize training of linear attention mechanisms?</p>
<p>Remember: the primary reason pretty much everyone dropped RNNs in favor of (Vaswani) Softmax Attention is that the latter is very easy to parallelize. Thus, we can scale it up better. And scale is, often times, all you need.</p>
<h3 id="a-sufficient-condition-for-parallelizing-training">A sufficient condition for parallelizing training<a hidden class="anchor" aria-hidden="true" href="#a-sufficient-condition-for-parallelizing-training">#</a></h3>
<p>As a rule of thumb, if you can recast your update rule as an associative operation over sequences, then you can parallelize it! This is, of course, only a sufficient, but not necessary, condition. There are non-associative operations that can be parallelized too with Chunk-wise parallelism.</p>
<div align="center">
    <img src="linear-attn-parallel-training.png" style="width:75%; height:75%;" />
</div>
<p>Note that there are faster ways to implement DeltaNet&rsquo;s update rule (e.g. WY representations, etc.). We&rsquo;ll discuss that next time!</p>
<h3 id="computational-forms-of-paralleled-training">Computational forms of paralleled training<a hidden class="anchor" aria-hidden="true" href="#computational-forms-of-paralleled-training">#</a></h3>
<p>In practice, how do we actually calculate the running &ldquo;sums&rdquo; efficiently? Remember those leetcode job interview data structure questions you hate? Well&hellip; this is when they become relevant&hellip;</p>
<div align="center">
    <img src="linear-attn-comp-forms.png" style="width:75%; height:75%;" />
</div>
<p>The most naive way is to simply run a loop through the key-value pairs. This is the <strong>recurrent form</strong>, and this is what we should be doing at inference time. But we can do much better than this.</p>
<p>On the other extreme end is the <strong>fully-parallel associative scan</strong> where we first aggregate the running sums by powers of two, then do a second pass to propagate the running sums across the sequence. If you&rsquo;ve implemented a Fenwick Tree before, this is roughly how it works.</p>
<p>But in practice, we use <strong>chunk-wise parallelism</strong> where we:</p>
<ol>
<li>Divide the sequence into chunks.</li>
<li>Use fully-parallel associative scan within each chunk. And</li>
<li>Use the recurrent form to propagate running sums across chunks.</li>
</ol>
<hr>
<p>That&rsquo;s it for now. Next time, I plan to talk about:</p>
<ol>
<li>How to derive different attention mechanisms using tensor string diagrams.</li>
<li>Circuit complexity of different attention mechanisms.</li>
<li>LLM reasoning.</li>
</ol>
<p>Stay tuned!</p>
<h2 id="how-to-cite">How to Cite<a hidden class="anchor" aria-hidden="true" href="#how-to-cite">#</a></h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bibtex" data-lang="bibtex"><span style="display:flex;"><span><span style="color:#0a0;text-decoration:underline">@misc</span>{cesista2025linearattn,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">author</span> = <span style="color:#a50">{Franz Louis Cesista}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">title</span> = <span style="color:#a50">{({L}inear) {A}ttention as {T}est-{T}ime {R}egression}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">year</span> = <span style="color:#a50">{2025}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">url</span> = <span style="color:#a50">{https://leloykun.github.io/ponder/test-time-regression/}</span>,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ol>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., &hellip; &amp; Polosukhin, I. (2017). Attention is all you need. URL <a href="https://arxiv.org/abs/1706.03762" target="_blank">https://arxiv.org/abs/1706.03762</a></li>
<li>Gupta, V., Koren, T., &amp; Singer, Y. (2018). Shampoo: Preconditioned Stochastic Tensor Optimization. URL <a href="https://arxiv.org/abs/1802.09568" target="_blank">https://arxiv.org/abs/1802.09568</a></li>
<li>Wang, K., Shi, J., Fox., E. (2025). Test-time regression: a unifying framework for designing sequence models with associative memory. URL <a href="https://arxiv.org/abs/2501.12352" target="_blank">https://arxiv.org/abs/2501.12352</a></li>
<li>Yang, S. (2025). What’s Next for Mamba? Towards More Expressive Recurrent Update Rules. URL <a href="https://sustcsonglin.github.io/assets/pdf/talk_250117.pdf" target="_blank">https://sustcsonglin.github.io/assets/pdf/talk_250117.pdf</a></li>
<li>Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 5156–5165. PMLR, 2020b. URL <a href="http://proceedings.mlr.press/v119/katharopoulos20a.html" target="_blank">http://proceedings.mlr.press/v119/katharopoulos20a.html</a>.</li>
<li>Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In Proceedings of the 41st International Conference on MachineLearning, volume 235 of Proceedingsof Machine Learning Research, pp. 10041–10071. PMLR, 2024b. URL <a href="https://proceedings.mlr.press/v235/dao24a.html" target="_blank">https://proceedings.mlr.press/v235/dao24a.html</a>.</li>
<li>Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim (2025). Parallelizing Linear Transformers with the Delta Rule over Sequence Length. URL <a href="https://arxiv.org/abs/2406.06484" target="_blank">https://arxiv.org/abs/2406.06484</a></li>
<li>Songlin Yang, Jan Kautz, Ali Hatamizadeh (2025). Gated Delta Networks: Improving Mamba2 with Delta Rule. URL <a href="https://arxiv.org/abs/2412.06464" target="_blank">https://arxiv.org/abs/2412.06464</a></li>
<li>Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 9099–9117. PMLR, 2022b. URL <a href="https://proceedings.mlr.press/v162/hua22a.html" target="_blank">https://proceedings.mlr.press/v162/hua22a.html</a>.</li>
<li>Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. ArXiv preprint, abs/2307.08621, 2023. URL <a href="https://arxiv.org/abs/2307.08621" target="_blank">https://arxiv.org/abs/2307.08621</a>.</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://leloykun.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://leloykun.github.io/tags/linear-attention/">Linear Attention</a></li>
      <li><a href="https://leloykun.github.io/tags/test-time-regression/">Test-Time Regression</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share (Linear) Attention as Test-Time Regression on x"
            href="https://x.com/intent/tweet/?text=%28Linear%29%20Attention%20as%20Test-Time%20Regression&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2ftest-time-regression%2f&amp;hashtags=MachineLearning%2cLinearAttention%2cTest-TimeRegression">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share (Linear) Attention as Test-Time Regression on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2ftest-time-regression%2f&amp;title=%28Linear%29%20Attention%20as%20Test-Time%20Regression&amp;summary=%28Linear%29%20Attention%20as%20Test-Time%20Regression&amp;source=https%3a%2f%2fleloykun.github.io%2fponder%2ftest-time-regression%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share (Linear) Attention as Test-Time Regression on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fleloykun.github.io%2fponder%2ftest-time-regression%2f&title=%28Linear%29%20Attention%20as%20Test-Time%20Regression">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share (Linear) Attention as Test-Time Regression on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fleloykun.github.io%2fponder%2ftest-time-regression%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share (Linear) Attention as Test-Time Regression on whatsapp"
            href="https://api.whatsapp.com/send?text=%28Linear%29%20Attention%20as%20Test-Time%20Regression%20-%20https%3a%2f%2fleloykun.github.io%2fponder%2ftest-time-regression%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share (Linear) Attention as Test-Time Regression on telegram"
            href="https://telegram.me/share/url?text=%28Linear%29%20Attention%20as%20Test-Time%20Regression&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2ftest-time-regression%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share (Linear) Attention as Test-Time Regression on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=%28Linear%29%20Attention%20as%20Test-Time%20Regression&u=https%3a%2f%2fleloykun.github.io%2fponder%2ftest-time-regression%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://leloykun.github.io/">Franz Louis Cesista</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
