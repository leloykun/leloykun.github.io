<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients | Franz Louis Cesista</title>
<meta name="keywords" content="Machine Learning, Muon, NanoGPT Speedrun">
<meta name="description" content="Simply switching to Muon can already get you 2x efficiency gains. But you can squeeze out an extra 1-2% by optimizing the Newton-Schulz coefficients.">
<meta name="author" content="Franz Louis Cesista">
<link rel="canonical" href="https://leloykun.github.io/ponder/muon-opt-coeffs/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e50e51e1b615f62676c32fb60e15e4b5e3a80ade29f3c6f9c953c528f576fb9f.css" integrity="sha256-5Q5R4bYV9iZ2wy&#43;2DhXkteOoCt4p88b5yVPFKPV2&#43;58=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://leloykun.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://leloykun.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://leloykun.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://leloykun.github.io/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://leloykun.github.io/ponder/muon-opt-coeffs/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients" />
<meta property="og:description" content="Simply switching to Muon can already get you 2x efficiency gains. But you can squeeze out an extra 1-2% by optimizing the Newton-Schulz coefficients." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://leloykun.github.io/ponder/muon-opt-coeffs/" />
<meta property="og:image" content="https://leloykun.github.io/ponder/muon-opt-coeffs/muon-022125-speedrun-record.png" /><meta property="article:section" content="ponder" />
<meta property="article:published_time" content="2025-02-21T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-02-21T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://leloykun.github.io/ponder/muon-opt-coeffs/muon-022125-speedrun-record.png" />
<meta name="twitter:title" content="Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients"/>
<meta name="twitter:description" content="Simply switching to Muon can already get you 2x efficiency gains. But you can squeeze out an extra 1-2% by optimizing the Newton-Schulz coefficients."/>
<meta name="twitter:site" content="@leloykun"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Ponder",
      "item": "https://leloykun.github.io/ponder/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients",
      "item": "https://leloykun.github.io/ponder/muon-opt-coeffs/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients",
  "name": "Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients",
  "description": "Simply switching to Muon can already get you 2x efficiency gains. But you can squeeze out an extra 1-2% by optimizing the Newton-Schulz coefficients.",
  "keywords": [
    "Machine Learning", "Muon", "NanoGPT Speedrun"
  ],
  "articleBody": "What’s Muon? Muon is an optimizer for 2D parameters that approximately semi-orthogonalizes the gradient first using Newton-Schulz iteration before using it to update the parameters.\nWhy semi-orthogonalize the gradients first? Because this is the update rule you get if you turn off the preconditioner accumulation of 2nd order algorithms like Shampoo or its variants like CASPR. In theory, you can do this if you assume that your batch size is large enough for you to have enough information to have a proper estimate of the hessian.\nYou can think of Muon as doing steepest descent under the spectral norm. Why the spectral norm? Because it’s the operator norm you induce on the parameters if your inputs and outputs are Euclidean/RMS-normed–which is a very reasonable assumption. And\nBecause it works well in practice ;P\nFor more details, I’d recommend reading Keller’s writeup on Muon and Jeremy’s paper on steepest descent under operator norms.\nWhy does Muon still work well despite only approximately semi-orthogonalizing the gradients? The reason we don’t need to perfectly semi-orthogonalize the gradients is that we can recast Muon as steepest descent under Schatten-p norm.\nThe Schatten-2 norm is just the Frobenius norm. Thus, steepest descent under that normed space is equivalent to the usual stochastic gradient descent. And The Schatten-$\\infty$ norm is just the Spectral norm. Thus, steepest descent under that normed space is equivalent to spectral gradient descent or… Muon with perfect semi-orthogonalization. And if we interpolate, all of the directions from the original gradient to its nearest semi-orthogonal matrix are all good descent directions!\nWhat’s the problem with the original coefficients of Muon? As you can see below, Muon’s Newton-Schulz iteration actually introduces a lot of noise to the resulting singular values–too much variance!\nAnd if my maths is correct, if you were doing steepest descent under the Schatten-32 norm instead, the spread of the resulting singular values would be about the same. So, in a sense, you can interpret Muon with the old coefficients as doing steepest descent under the Schatten-32 norm.\nHow do we optimize the coefficients? Keller found the original coefficients using gradient descent with a bias for steep curves.\nA key insight is that we don’t have to use the same coefficients for all the newton-shultz iteration steps! We can initialize a [NUM_STEPS, 3] matrix and do gradient descent to find the optimal coefficients. We can even use Muon to improve itself!\nYouJiaching, Keller, \u0026 I actually experimented a lot on this a few months ago when we only had the GPT2-small track. But we didn’t get any improvements with the trick, so we abandoned it. It’s only now that we have a GPT2-medium track that this has started to be useful.\nI also added a couple of auxiliary losses for improved stability and aesthetics. E.g.:\nThe peak of the previous iteration must be less than the right fixed point of the current iteration. Otherwise, the singular values are going to blow up.\nThe through of the current iteration must not cross the x-axis. Otherwise, the singular values are gonna switch signs.\nThe coefficients must be small and be stable even when truncating the decimals.\nFor aesthetics, I constrained the attractor basins to grow smaller at each step.\nAnd etc.\nTakeaways for Optimizer Researchers In early training, the ‘steepness’ of the curve matters more than noise reduction. This is because the stable rank of the gradients tends to be smaller in early training.\nBut noise reduction matters more for longer training runs. I.e., having a smaller variance in the resulting singular values after NS iterations results in lower loss overall.\nCode \"\"\" Tool for optimizing the coefficients of the Newton-Schulz iterators in Muon. Usage notes: - Use a high `epsilon` value to prevent the singular values from either blowing up or switching signs. - Set --enable_flatness_aux_loss to get flatter composite curves. \"\"\" import argparse from functools import partial import jax import jax.numpy as jnp import optax import sympy as sp DEFAULT_EPS = 1. / 16 DEFAULT_PRECISION = 4 gamma_ = sp.Symbol(\"gamma\", interval=(5/4, sp.S.Infinity), left_open=True, right_open=True) l_ = sp.Symbol(\"l\", interval=(0, 1), left_open=False, right_open=True) r_ = sp.Symbol(\"r\", interval=(0, 1), left_open=False, right_open=True) x_ = sp.Symbol(\"x\", real=True) fp_ = [-(1 + r_), -(1 - l_), 0, 1 - l_, 1 + r_] iterator_ = x_ + gamma_ * (x_ - fp_[0])*(x_ - fp_[1])*(x_ - fp_[2])*(x_ - fp_[3])*(x_ - fp_[4]) iterator_simplified_ = sp.collect(sp.expand(iterator_), x_) abc_iterator_jax = jax.jit(lambda x, a, b, c: a*x + b*x**3 + c*x**5) glr_iterator_jax = sp.lambdify((x_, gamma_, l_, r_), iterator_simplified_, \"jax\") a_, b_, c_ = sp.Poly(iterator_simplified_, x_).coeffs()[::-1] a_jax = sp.lambdify((gamma_, l_, r_), a_, \"jax\") b_jax = sp.lambdify((gamma_, l_, r_), b_, \"jax\") c_jax = sp.lambdify((gamma_, l_, r_), c_, \"jax\") def abc_to_glr_reparam(a: float, b: float, c: float, verbose: bool = False): iterator_fn = a*x_ + b*x_**3 + c*x_**5 iterator_roots = sp.nroots(iterator_fn - x_) if verbose: print(iterator_roots) iterator_roots_real = [root.evalf() for root in iterator_roots if root.is_real] iterator_roots = sorted(iterator_roots_real) return float(c), float(1 - iterator_roots[-2]), float(iterator_roots[-1] - 1) @partial(jax.jit, static_argnames=(\"decimals\",)) def glr_to_abc_reparam(gamma: float, l: float, r: float, decimals: int = 4): abc = jnp.stack([a_jax(gamma, l, r), b_jax(gamma, l, r), c_jax(gamma, l, r)]) return abc + jax.lax.stop_gradient(jnp.round(abc, decimals) - abc) def loss( x: jax.Array, params: jax.Array, eps: float = DEFAULT_EPS, precision: int = DEFAULT_PRECISION, enable_contraction_aux_loss: bool = True, enable_flatness_aux_loss: bool = False, ): def scan_body_fn(y: jax.Array, glr: jax.Array): gamma, l, r = glr # The peak of the previous iteration should be at most 1 + r - eps # to prevent singular values from blowing up intermediate_loss = jnp.clip(y.max() - (1 + r - eps), min=0) a, b, c = glr_to_abc_reparam(gamma, l, r, precision) new_y = abc_iterator_jax(y, a, b, c) # The iterator must not cross the x-axis # to prevent singular values from switching signs intermediate_loss += jnp.clip(eps - jnp.amin(jnp.where(y \u003e 0.5, new_y, jnp.inf)), min=0) return new_y, intermediate_loss y, intermediate_losses = jax.lax.scan(scan_body_fn, x, params) # This auxiliary loss term encourages the contraction of the # attractor basins of the iterators aesthetic_aux_loss = ( jnp.clip(params[1:,2] - params[:-1,2], min=0).sum() + jnp.clip(params[1:,1] - params[:-1,1], min=0).sum() + jnp.clip(params[1:,0] - params[:-1,0], min=0).sum() ) # This auxiliary loss term encourages the flatness of the composite curve # Taken from @YouJiacheng's code here: https://gist.github.com/YouJiacheng/393c90cbdc23b09d5688815ba382288b y_max = jnp.amax(y) y_min = jnp.amin(jnp.where(x \u003e 0.05, y, jnp.inf)) diff_ratio = (y_max - y_min) / jnp.clip(y_max, min=1e-3) loss1 = jnp.sqrt(jnp.mean((y - 1) ** 2)) loss2 = ( intermediate_losses.mean() + jnp.int32(enable_contraction_aux_loss) * aesthetic_aux_loss + jnp.int32(enable_flatness_aux_loss) * diff_ratio ) return loss1 + loss2 loss_and_grad_fn = jax.jit(jax.value_and_grad(loss, argnums=1), static_argnums=(2, 3, 4, 5)) @partial(jax.jit, static_argnums=(2, 3, 4, 5, 6)) def train( x: jax.Array, params: jax.Array, learning_rate: float = 0.001, num_steps: int = 10_000, eps: float = DEFAULT_EPS, precision: int = DEFAULT_PRECISION, enable_contraction_aux_loss: bool = True, enable_flatness_aux_loss: bool = False, ): optimizer = optax.chain( # can also use optax.contrib.muon optax.adam(learning_rate=learning_rate), optax.clip_by_global_norm(max_norm=1.), ) opt_state = optimizer.init(params) def body_fn(values: tuple[jax.Array, optax.OptState], _): params, opt_state = values loss, grad = loss_and_grad_fn( x, params, eps, precision, enable_contraction_aux_loss, enable_flatness_aux_loss, ) updates, opt_state = optimizer.update(grad, opt_state, params) new_params = optax.apply_updates(params, updates) return (new_params, opt_state), (params, loss) (params, _), (historical_params, losses) = jax.lax.scan(body_fn, (params, opt_state), length=num_steps) return params, historical_params, losses if __name__ == \"__main__\": parser = argparse.ArgumentParser() parser.add_argument( \"--num_ns_iters\", help=\"Number of Newton-Schulz iterations\", type=int, default=5 ) parser.add_argument( \"--num_train_steps\", help=\"Number of training steps\", type=int, default=10_000 ) parser.add_argument( \"--learning_rate\", help=\"Learning rate\", type=float, default=0.001 ) parser.add_argument( \"--precision\", help=\"Number of decimals in the coefficients\", type=int, default=DEFAULT_PRECISION ) parser.add_argument( \"--eps\", help=\"Epsilon\", type=float, default=DEFAULT_EPS ) parser.add_argument( \"--enable_contraction_aux_loss\", help=\"Enable contraction auxiliary loss\", action=\"store_true\", default=True ) parser.add_argument( \"--enable_flatness_aux_loss\", help=\"Enable flatness auxiliary loss\", action=\"store_true\", default=False ) args = parser.parse_args() print(args) # Reparametrize Keller Jordan's a-b-c coefficients to gamma-l-r kj_a, kj_b, kj_c = 3.4445, -4.7750, 2.0315 kj_gamma, kj_inner_radius, kj_outer_radius = abc_to_glr_reparam(kj_a, kj_b, kj_c) # Check if the reparametrization is correct kj_abc = glr_to_abc_reparam(kj_gamma, kj_inner_radius, kj_outer_radius, decimals=4) assert jnp.allclose(kj_abc, jnp.array([kj_a, kj_b, kj_c]), atol=1e-4) x = jnp.concat([ # The extra 0.1 is there to account for numerical instability jnp.linspace(0, 1.1, 2**10), # Gradients typically have low stable rank (i.e. most of the singular values are close to 0). # To simulate that, we add a couple more points near 0. jnp.linspace(0, 0.1, 2**9), ]) init_params = jnp.array([[kj_gamma, kj_inner_radius, kj_outer_radius]]*args.num_ns_iters) trained_params, historical_params, losses = train( x=x, params=init_params, learning_rate=args.learning_rate, num_steps=args.num_train_steps, eps=args.eps, precision=args.precision, enable_contraction_aux_loss=args.enable_contraction_aux_loss, enable_flatness_aux_loss=args.enable_flatness_aux_loss, ) best_params: jax.Array = historical_params[jnp.nanargmin(losses)] steepness = 1. for gamma, l, r in best_params: a, b, c = glr_to_abc_reparam(gamma, l, r, args.precision) print(f\"({a:.4f}, {b:.4f}, {c:.4f})\") steepness *= a print(f\"Steepness: {steepness: .4f}\") How to Cite @misc{cesista2025muonoptcoeffs, author = {Franz Louis Cesista and YouJiacheng and Keller Jordan}, title = {Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients}, year = {2025}, url = {http://leloykun.github.io/ponder/muon-opt-coeffs/}, } References Keller Jordan, Jeremy Bernstein, Brendan Rappazzo, @fernbear.bsky.social, Boza Vlado, Jiacheng You, Franz Cesista, Braden Koszarsky, and @Grad62304977. modded-nanogpt: Speedrunning the NanoGPT baseline. 2024. Available at: https://github.com/KellerJordan/modded-nanogpt. Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein (2024). Muon: An optimizer for hidden layers in neural networks. Available at: https://kellerjordan.github.io/posts/muon/. Surya, S., Duvvuri, Devvrit, F., Anil, R., Hsieh, C., \u0026 Dhillon, I.S. (2024). Combining Axes Preconditioners through Kronecker Approximation for Deep Learning. International Conference on Learning Representations. Vineet Gupta, Tomer Koren, Yoram Singer (2018). Shampoo: Preconditioned Stochastic Tensor Optimization. URL https://arxiv.org/abs/1802.09568 Rohan Anil et al. “Scalable second order optimization for deep learning.” arXiv preprint arXiv:2002.09018 (2020). Jeremy Bernstein and Laker Newhouse. “Old optimizer, new norm: An anthology.” arXiv preprint arXiv:2409.20325 (2024). Jeremy Bernstein (2024). “Weight erasure.” Available at: https://docs.modula.systems/examples/weight-erasure/ Franz Louis Cesista (2025). CASPR Without Accumulation is Muon. URL https://leloykun.github.io/ponder/caspr-wo-accum-is-muon/ ",
  "wordCount" : "1560",
  "inLanguage": "en",
  "image":"https://leloykun.github.io/ponder/muon-opt-coeffs/muon-022125-speedrun-record.png","datePublished": "2025-02-21T00:00:00Z",
  "dateModified": "2025-02-21T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Franz Louis Cesista"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://leloykun.github.io/ponder/muon-opt-coeffs/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Franz Louis Cesista",
    "logo": {
      "@type": "ImageObject",
      "url": "https://leloykun.github.io/favicon.ico"
    }
  }
}
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-BWCGBRX8G1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BWCGBRX8G1');
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"
  integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"
  integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js"
  integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
        { left: "\\begin{equation}", right: "\\end{equation}", display: true },
        { left: "\\begin{equation*}", right: "\\end{equation*}", display: true },
        { left: "\\begin{align}", right: "\\end{align}", display: true },
        { left: "\\begin{align*}", right: "\\end{align*}", display: true },
        { left: "\\begin{alignat}", right: "\\end{alignat}", display: true },
        { left: "\\begin{gather}", right: "\\end{gather}", display: true },
        { left: "\\begin{CD}", right: "\\end{CD}", display: true },
      ],
      throwOnError: false,
      trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
      macros: {
        "\\eqref": "\\href{###1}{(\\text{#1})}",
        "\\ref": "\\href{###1}{\\text{#1}}",
        "\\label": "\\htmlId{#1}{}"
      }
    });
  });
</script>
 


</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://leloykun.github.io/" accesskey="h" title="Franz Louis Cesista">
                <img src="https://leloykun.github.io/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Franz Louis Cesista</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://leloykun.github.io/cv.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/personal-projects/" title="Personal Projects">
                    <span>Personal Projects</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/ponder/" title="Ponder (Posts)">
                    <span>Ponder (Posts)</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients
    </h1>
    <div class="post-meta"><span title='2025-02-21 00:00:00 +0000 UTC'>February 21, 2025</span>&nbsp;&middot;&nbsp;8 min&nbsp;&middot;&nbsp;Franz Louis Cesista&nbsp;&middot;&nbsp;<a href="https://x.com/leloykun/status/1892793848163946799" rel="noopener noreferrer" target="_blank">Crossposted from X (formerly Twitter)</a>

</div>
  </header> 
<figure class="entry-cover">
            <img loading="eager"
                srcset='https://leloykun.github.io/ponder/muon-opt-coeffs/muon-022125-speedrun-record_hu_f62d7a3c3c8d2d89.png 360w,https://leloykun.github.io/ponder/muon-opt-coeffs/muon-022125-speedrun-record_hu_14dd92c906c49b54.png 480w,https://leloykun.github.io/ponder/muon-opt-coeffs/muon-022125-speedrun-record.png 578w'
                src="https://leloykun.github.io/ponder/muon-opt-coeffs/muon-022125-speedrun-record.png"
                sizes="(min-width: 768px) 720px, 100vw"
                width="578" height="455"
                alt="Cover">
        
</figure><div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#whats-muon">What&rsquo;s Muon?</a></li>
    <li><a href="#why-semi-orthogonalize-the-gradients-first">Why semi-orthogonalize the gradients first?</a></li>
    <li><a href="#why-does-muon-still-work-well-despite-only-approximately-semi-orthogonalizing-the-gradients">Why does Muon still work well despite only approximately semi-orthogonalizing the gradients?</a></li>
    <li><a href="#whats-the-problem-with-the-original-coefficients-of-muon">What&rsquo;s the problem with the original coefficients of Muon?</a></li>
    <li><a href="#how-do-we-optimize-the-coefficients">How do we optimize the coefficients?</a></li>
    <li><a href="#takeaways-for-optimizer-researchers">Takeaways for Optimizer Researchers</a></li>
    <li><a href="#code">Code</a></li>
    <li><a href="#how-to-cite">How to Cite</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="whats-muon">What&rsquo;s Muon?<a hidden class="anchor" aria-hidden="true" href="#whats-muon">#</a></h2>
<p>Muon is an optimizer for 2D parameters that approximately semi-orthogonalizes the gradient first using Newton-Schulz iteration before using it to update the parameters.</p>
<div align="center">
    <img src="muon-definition.png" style="width:70%; height:70%" />
</div>
<h2 id="why-semi-orthogonalize-the-gradients-first">Why semi-orthogonalize the gradients first?<a hidden class="anchor" aria-hidden="true" href="#why-semi-orthogonalize-the-gradients-first">#</a></h2>
<ol>
<li>
<p>Because this is the update rule you get if you turn off the preconditioner accumulation of 2nd order algorithms like Shampoo or its variants like CASPR. In theory, you can do this if you assume that your batch size is large enough for you to have enough information to have a proper estimate of the hessian.</p>
</li>
<li>
<p>You can think of Muon as doing steepest descent under the spectral norm. Why the spectral norm? Because it&rsquo;s the operator norm you induce on the parameters if your inputs and outputs are Euclidean/RMS-normed&ndash;which is a very reasonable assumption. And</p>
</li>
<li>
<p>Because it works well in practice ;P</p>
</li>
</ol>
<p><img loading="lazy" src="/ponder/muon-opt-coeffs/muon-as-shampoo-wo-accum.png#center"></p>
<p><img loading="lazy" src="/ponder/muon-opt-coeffs/muon-as-caspr-wo-accum.png#center"></p>
<p>For more details, I&rsquo;d recommend reading <a href="https://kellerjordan.github.io/posts/muon/" target="_blank">Keller&rsquo;s writeup on Muon</a> and <a href="https://arxiv.org/abs/2409.20325" target="_blank">Jeremy&rsquo;s paper on steepest descent under operator norms</a>.</p>
<h2 id="why-does-muon-still-work-well-despite-only-approximately-semi-orthogonalizing-the-gradients">Why does Muon still work well despite only approximately semi-orthogonalizing the gradients?<a hidden class="anchor" aria-hidden="true" href="#why-does-muon-still-work-well-despite-only-approximately-semi-orthogonalizing-the-gradients">#</a></h2>
<p>The reason we don&rsquo;t need to perfectly semi-orthogonalize the gradients is that we can recast Muon as steepest descent under Schatten-p norm.</p>
<ul>
<li>The Schatten-2 norm is just the Frobenius norm. Thus, steepest descent under that normed space is equivalent to the usual stochastic gradient descent. And</li>
<li>The Schatten-$\infty$ norm is just the Spectral norm. Thus, steepest descent under that normed space is equivalent to spectral gradient descent or&hellip; Muon with perfect semi-orthogonalization.</li>
</ul>
<p>And if we interpolate, all of the directions from the original gradient to its nearest semi-orthogonal matrix are all good descent directions!</p>
<h2 id="whats-the-problem-with-the-original-coefficients-of-muon">What&rsquo;s the problem with the original coefficients of Muon?<a hidden class="anchor" aria-hidden="true" href="#whats-the-problem-with-the-original-coefficients-of-muon">#</a></h2>
<p>As you can see below, Muon&rsquo;s Newton-Schulz iteration actually introduces a lot of noise to the resulting singular values&ndash;too much variance!</p>
<p><img loading="lazy" src="/ponder/muon-opt-coeffs/muon-noise.png#center"></p>
<p>And if my maths is correct, if you were doing steepest descent under the Schatten-32 norm instead, the spread of the resulting singular values would be about the same. So, in a sense, you can interpret Muon with the old coefficients as doing steepest descent under the Schatten-32 norm.</p>
<h2 id="how-do-we-optimize-the-coefficients">How do we optimize the coefficients?<a hidden class="anchor" aria-hidden="true" href="#how-do-we-optimize-the-coefficients">#</a></h2>
<p>Keller found the original coefficients using gradient descent with a bias for steep curves.</p>
<p>A key insight is that we don&rsquo;t have to use the same coefficients for all the newton-shultz iteration steps! We can initialize a <code>[NUM_STEPS, 3]</code> matrix and do gradient descent to find the optimal coefficients. We can even use Muon to improve itself!</p>
<p><img loading="lazy" src="/ponder/muon-opt-coeffs/muon-iterators.jpg#center"></p>
<p>YouJiaching, Keller, &amp; I actually experimented a lot on this a few months ago when we only had the GPT2-small track. But we didn&rsquo;t get any improvements with the trick, so we abandoned it. It&rsquo;s only now that we have a GPT2-medium track that this has started to be useful.</p>
<hr>
<p>I also added a couple of auxiliary losses for improved stability and aesthetics. E.g.:</p>
<ol>
<li>
<p>The peak of the previous iteration must be less than the right fixed point of the current iteration. Otherwise, the singular values are going to blow up.</p>
</li>
<li>
<p>The through of the current iteration must not cross the x-axis. Otherwise, the singular values are gonna switch signs.</p>
</li>
<li>
<p>The coefficients must be small and be stable even when truncating the decimals.</p>
</li>
<li>
<p>For aesthetics, I constrained the attractor basins to grow smaller at each step.</p>
</li>
</ol>
<p>And etc.</p>
<h2 id="takeaways-for-optimizer-researchers">Takeaways for Optimizer Researchers<a hidden class="anchor" aria-hidden="true" href="#takeaways-for-optimizer-researchers">#</a></h2>
<ol>
<li>
<p>In early training, the &lsquo;steepness&rsquo; of the curve matters more than noise reduction. This is because the stable rank of the gradients tends to be smaller in early training.</p>
</li>
<li>
<p>But noise reduction matters more for longer training runs. I.e., having a smaller variance in the resulting singular values after NS iterations results in lower loss overall.</p>
</li>
</ol>
<h2 id="code">Code<a hidden class="anchor" aria-hidden="true" href="#code">#</a></h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a50">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a50">Tool for optimizing the coefficients of the Newton-Schulz iterators in Muon.
</span></span></span><span style="display:flex;"><span><span style="color:#a50">
</span></span></span><span style="display:flex;"><span><span style="color:#a50">Usage notes:
</span></span></span><span style="display:flex;"><span><span style="color:#a50">- Use a high `epsilon` value to prevent the singular values from either blowing up or switching signs.
</span></span></span><span style="display:flex;"><span><span style="color:#a50">- Set --enable_flatness_aux_loss to get flatter composite curves.
</span></span></span><span style="display:flex;"><span><span style="color:#a50">&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">argparse</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">from</span> <span style="color:#0aa;text-decoration:underline">functools</span> <span style="color:#00a">import</span> partial
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">jax</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">jax.numpy</span> <span style="color:#00a">as</span> <span style="color:#0aa;text-decoration:underline">jnp</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">optax</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">sympy</span> <span style="color:#00a">as</span> <span style="color:#0aa;text-decoration:underline">sp</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>DEFAULT_EPS = <span style="color:#099">1.</span> / <span style="color:#099">16</span>
</span></span><span style="display:flex;"><span>DEFAULT_PRECISION = <span style="color:#099">4</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>gamma_ = sp.Symbol(<span style="color:#a50">&#34;gamma&#34;</span>, interval=(<span style="color:#099">5</span>/<span style="color:#099">4</span>, sp.S.Infinity), left_open=<span style="color:#00a">True</span>, right_open=<span style="color:#00a">True</span>)
</span></span><span style="display:flex;"><span>l_ = sp.Symbol(<span style="color:#a50">&#34;l&#34;</span>, interval=(<span style="color:#099">0</span>, <span style="color:#099">1</span>), left_open=<span style="color:#00a">False</span>, right_open=<span style="color:#00a">True</span>)
</span></span><span style="display:flex;"><span>r_ = sp.Symbol(<span style="color:#a50">&#34;r&#34;</span>, interval=(<span style="color:#099">0</span>, <span style="color:#099">1</span>), left_open=<span style="color:#00a">False</span>, right_open=<span style="color:#00a">True</span>)
</span></span><span style="display:flex;"><span>x_ = sp.Symbol(<span style="color:#a50">&#34;x&#34;</span>, real=<span style="color:#00a">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fp_ = [-(<span style="color:#099">1</span> + r_), -(<span style="color:#099">1</span> - l_), <span style="color:#099">0</span>, <span style="color:#099">1</span> - l_, <span style="color:#099">1</span> + r_]
</span></span><span style="display:flex;"><span>iterator_ = x_ + gamma_ * (x_ - fp_[<span style="color:#099">0</span>])*(x_ - fp_[<span style="color:#099">1</span>])*(x_ - fp_[<span style="color:#099">2</span>])*(x_ - fp_[<span style="color:#099">3</span>])*(x_ - fp_[<span style="color:#099">4</span>])
</span></span><span style="display:flex;"><span>iterator_simplified_ = sp.collect(sp.expand(iterator_), x_)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>abc_iterator_jax = jax.jit(<span style="color:#00a">lambda</span> x, a, b, c: a*x + b*x**<span style="color:#099">3</span> + c*x**<span style="color:#099">5</span>)
</span></span><span style="display:flex;"><span>glr_iterator_jax = sp.lambdify((x_, gamma_, l_, r_), iterator_simplified_, <span style="color:#a50">&#34;jax&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>a_, b_, c_ = sp.Poly(iterator_simplified_, x_).coeffs()[::-<span style="color:#099">1</span>]
</span></span><span style="display:flex;"><span>a_jax = sp.lambdify((gamma_, l_, r_), a_, <span style="color:#a50">&#34;jax&#34;</span>)
</span></span><span style="display:flex;"><span>b_jax = sp.lambdify((gamma_, l_, r_), b_, <span style="color:#a50">&#34;jax&#34;</span>)
</span></span><span style="display:flex;"><span>c_jax = sp.lambdify((gamma_, l_, r_), c_, <span style="color:#a50">&#34;jax&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">abc_to_glr_reparam</span>(a: <span style="color:#0aa">float</span>, b: <span style="color:#0aa">float</span>, c: <span style="color:#0aa">float</span>, verbose: <span style="color:#0aa">bool</span> = <span style="color:#00a">False</span>):
</span></span><span style="display:flex;"><span>    iterator_fn = a*x_ + b*x_**<span style="color:#099">3</span> + c*x_**<span style="color:#099">5</span>
</span></span><span style="display:flex;"><span>    iterator_roots = sp.nroots(iterator_fn - x_)
</span></span><span style="display:flex;"><span>    <span style="color:#00a">if</span> verbose:
</span></span><span style="display:flex;"><span>        <span style="color:#0aa">print</span>(iterator_roots)
</span></span><span style="display:flex;"><span>    iterator_roots_real = [root.evalf() <span style="color:#00a">for</span> root <span style="color:#00a">in</span> iterator_roots <span style="color:#00a">if</span> root.is_real]
</span></span><span style="display:flex;"><span>    iterator_roots = <span style="color:#0aa">sorted</span>(iterator_roots_real)
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> <span style="color:#0aa">float</span>(c), <span style="color:#0aa">float</span>(<span style="color:#099">1</span> - iterator_roots[-<span style="color:#099">2</span>]), <span style="color:#0aa">float</span>(iterator_roots[-<span style="color:#099">1</span>] - <span style="color:#099">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#888">@partial</span>(jax.jit, static_argnames=(<span style="color:#a50">&#34;decimals&#34;</span>,))
</span></span><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">glr_to_abc_reparam</span>(gamma: <span style="color:#0aa">float</span>, l: <span style="color:#0aa">float</span>, r: <span style="color:#0aa">float</span>, decimals: <span style="color:#0aa">int</span> = <span style="color:#099">4</span>):
</span></span><span style="display:flex;"><span>    abc = jnp.stack([a_jax(gamma, l, r), b_jax(gamma, l, r), c_jax(gamma, l, r)])
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> abc + jax.lax.stop_gradient(jnp.round(abc, decimals) - abc)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">loss</span>(
</span></span><span style="display:flex;"><span>    x: jax.Array,
</span></span><span style="display:flex;"><span>    params: jax.Array,
</span></span><span style="display:flex;"><span>    eps: <span style="color:#0aa">float</span> = DEFAULT_EPS,
</span></span><span style="display:flex;"><span>    precision: <span style="color:#0aa">int</span> = DEFAULT_PRECISION,
</span></span><span style="display:flex;"><span>    enable_contraction_aux_loss: <span style="color:#0aa">bool</span> = <span style="color:#00a">True</span>,
</span></span><span style="display:flex;"><span>    enable_flatness_aux_loss: <span style="color:#0aa">bool</span> = <span style="color:#00a">False</span>,
</span></span><span style="display:flex;"><span>):
</span></span><span style="display:flex;"><span>    <span style="color:#00a">def</span> <span style="color:#0a0">scan_body_fn</span>(y: jax.Array, glr: jax.Array):
</span></span><span style="display:flex;"><span>        gamma, l, r = glr
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># The peak of the previous iteration should be at most 1 + r - eps</span>
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># to prevent singular values from blowing up</span>
</span></span><span style="display:flex;"><span>        intermediate_loss = jnp.clip(y.max() - (<span style="color:#099">1</span> + r - eps), <span style="color:#0aa">min</span>=<span style="color:#099">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        a, b, c = glr_to_abc_reparam(gamma, l, r, precision)
</span></span><span style="display:flex;"><span>        new_y = abc_iterator_jax(y, a, b, c)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># The iterator must not cross the x-axis</span>
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># to prevent singular values from switching signs</span>
</span></span><span style="display:flex;"><span>        intermediate_loss += jnp.clip(eps - jnp.amin(jnp.where(y &gt; <span style="color:#099">0.5</span>, new_y, jnp.inf)), <span style="color:#0aa">min</span>=<span style="color:#099">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#00a">return</span> new_y, intermediate_loss
</span></span><span style="display:flex;"><span>    y, intermediate_losses = jax.lax.scan(scan_body_fn, x, params)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># This auxiliary loss term encourages the contraction of the</span>
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># attractor basins of the iterators</span>
</span></span><span style="display:flex;"><span>    aesthetic_aux_loss = (
</span></span><span style="display:flex;"><span>        jnp.clip(params[<span style="color:#099">1</span>:,<span style="color:#099">2</span>] - params[:-<span style="color:#099">1</span>,<span style="color:#099">2</span>], <span style="color:#0aa">min</span>=<span style="color:#099">0</span>).sum()
</span></span><span style="display:flex;"><span>        + jnp.clip(params[<span style="color:#099">1</span>:,<span style="color:#099">1</span>] - params[:-<span style="color:#099">1</span>,<span style="color:#099">1</span>], <span style="color:#0aa">min</span>=<span style="color:#099">0</span>).sum()
</span></span><span style="display:flex;"><span>        + jnp.clip(params[<span style="color:#099">1</span>:,<span style="color:#099">0</span>] - params[:-<span style="color:#099">1</span>,<span style="color:#099">0</span>], <span style="color:#0aa">min</span>=<span style="color:#099">0</span>).sum()
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># This auxiliary loss term encourages the flatness of the composite curve</span>
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># Taken from @YouJiacheng&#39;s code here: https://gist.github.com/YouJiacheng/393c90cbdc23b09d5688815ba382288b</span>
</span></span><span style="display:flex;"><span>    y_max = jnp.amax(y)
</span></span><span style="display:flex;"><span>    y_min = jnp.amin(jnp.where(x &gt; <span style="color:#099">0.05</span>, y, jnp.inf))
</span></span><span style="display:flex;"><span>    diff_ratio = (y_max - y_min) / jnp.clip(y_max, <span style="color:#0aa">min</span>=<span style="color:#099">1e-3</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    loss1 = jnp.sqrt(jnp.mean((y - <span style="color:#099">1</span>) ** <span style="color:#099">2</span>))
</span></span><span style="display:flex;"><span>    loss2 = (
</span></span><span style="display:flex;"><span>        intermediate_losses.mean()
</span></span><span style="display:flex;"><span>        + jnp.int32(enable_contraction_aux_loss) * aesthetic_aux_loss
</span></span><span style="display:flex;"><span>        + jnp.int32(enable_flatness_aux_loss) * diff_ratio
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> loss1 + loss2
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>loss_and_grad_fn = jax.jit(jax.value_and_grad(loss, argnums=<span style="color:#099">1</span>), static_argnums=(<span style="color:#099">2</span>, <span style="color:#099">3</span>, <span style="color:#099">4</span>, <span style="color:#099">5</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#888">@partial</span>(jax.jit, static_argnums=(<span style="color:#099">2</span>, <span style="color:#099">3</span>, <span style="color:#099">4</span>, <span style="color:#099">5</span>, <span style="color:#099">6</span>))
</span></span><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">train</span>(
</span></span><span style="display:flex;"><span>    x: jax.Array,
</span></span><span style="display:flex;"><span>    params: jax.Array,
</span></span><span style="display:flex;"><span>    learning_rate: <span style="color:#0aa">float</span> = <span style="color:#099">0.001</span>,
</span></span><span style="display:flex;"><span>    num_steps: <span style="color:#0aa">int</span> = <span style="color:#099">10_000</span>,
</span></span><span style="display:flex;"><span>    eps: <span style="color:#0aa">float</span> = DEFAULT_EPS,
</span></span><span style="display:flex;"><span>    precision: <span style="color:#0aa">int</span> = DEFAULT_PRECISION,
</span></span><span style="display:flex;"><span>    enable_contraction_aux_loss: <span style="color:#0aa">bool</span> = <span style="color:#00a">True</span>,
</span></span><span style="display:flex;"><span>    enable_flatness_aux_loss: <span style="color:#0aa">bool</span> = <span style="color:#00a">False</span>,
</span></span><span style="display:flex;"><span>):
</span></span><span style="display:flex;"><span>    optimizer = optax.chain(
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># can also use optax.contrib.muon</span>
</span></span><span style="display:flex;"><span>        optax.adam(learning_rate=learning_rate),
</span></span><span style="display:flex;"><span>        optax.clip_by_global_norm(max_norm=<span style="color:#099">1.</span>),
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    opt_state = optimizer.init(params)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a">def</span> <span style="color:#0a0">body_fn</span>(values: <span style="color:#0aa">tuple</span>[jax.Array, optax.OptState], _):
</span></span><span style="display:flex;"><span>        params, opt_state = values
</span></span><span style="display:flex;"><span>        loss, grad = loss_and_grad_fn(
</span></span><span style="display:flex;"><span>            x,
</span></span><span style="display:flex;"><span>            params,
</span></span><span style="display:flex;"><span>            eps,
</span></span><span style="display:flex;"><span>            precision,
</span></span><span style="display:flex;"><span>            enable_contraction_aux_loss,
</span></span><span style="display:flex;"><span>            enable_flatness_aux_loss,
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        updates, opt_state = optimizer.update(grad, opt_state, params)
</span></span><span style="display:flex;"><span>        new_params = optax.apply_updates(params, updates)
</span></span><span style="display:flex;"><span>        <span style="color:#00a">return</span> (new_params, opt_state), (params, loss)
</span></span><span style="display:flex;"><span>    (params, _), (historical_params, losses) = jax.lax.scan(body_fn, (params, opt_state), length=num_steps)
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> params, historical_params, losses
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00a">if</span> __name__ == <span style="color:#a50">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    parser = argparse.ArgumentParser()
</span></span><span style="display:flex;"><span>    parser.add_argument(
</span></span><span style="display:flex;"><span>        <span style="color:#a50">&#34;--num_ns_iters&#34;</span>, help=<span style="color:#a50">&#34;Number of Newton-Schulz iterations&#34;</span>, <span style="color:#0aa">type</span>=<span style="color:#0aa">int</span>, default=<span style="color:#099">5</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    parser.add_argument(
</span></span><span style="display:flex;"><span>        <span style="color:#a50">&#34;--num_train_steps&#34;</span>, help=<span style="color:#a50">&#34;Number of training steps&#34;</span>, <span style="color:#0aa">type</span>=<span style="color:#0aa">int</span>, default=<span style="color:#099">10_000</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    parser.add_argument(
</span></span><span style="display:flex;"><span>        <span style="color:#a50">&#34;--learning_rate&#34;</span>, help=<span style="color:#a50">&#34;Learning rate&#34;</span>, <span style="color:#0aa">type</span>=<span style="color:#0aa">float</span>, default=<span style="color:#099">0.001</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    parser.add_argument(
</span></span><span style="display:flex;"><span>        <span style="color:#a50">&#34;--precision&#34;</span>, help=<span style="color:#a50">&#34;Number of decimals in the coefficients&#34;</span>, <span style="color:#0aa">type</span>=<span style="color:#0aa">int</span>, default=DEFAULT_PRECISION
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    parser.add_argument(
</span></span><span style="display:flex;"><span>        <span style="color:#a50">&#34;--eps&#34;</span>, help=<span style="color:#a50">&#34;Epsilon&#34;</span>, <span style="color:#0aa">type</span>=<span style="color:#0aa">float</span>, default=DEFAULT_EPS
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    parser.add_argument(
</span></span><span style="display:flex;"><span>        <span style="color:#a50">&#34;--enable_contraction_aux_loss&#34;</span>, help=<span style="color:#a50">&#34;Enable contraction auxiliary loss&#34;</span>, action=<span style="color:#a50">&#34;store_true&#34;</span>, default=<span style="color:#00a">True</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    parser.add_argument(
</span></span><span style="display:flex;"><span>        <span style="color:#a50">&#34;--enable_flatness_aux_loss&#34;</span>, help=<span style="color:#a50">&#34;Enable flatness auxiliary loss&#34;</span>, action=<span style="color:#a50">&#34;store_true&#34;</span>, default=<span style="color:#00a">False</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    args = parser.parse_args()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#0aa">print</span>(args)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># Reparametrize Keller Jordan&#39;s a-b-c coefficients to gamma-l-r</span>
</span></span><span style="display:flex;"><span>    kj_a, kj_b, kj_c = <span style="color:#099">3.4445</span>, -<span style="color:#099">4.7750</span>, <span style="color:#099">2.0315</span>
</span></span><span style="display:flex;"><span>    kj_gamma, kj_inner_radius, kj_outer_radius = abc_to_glr_reparam(kj_a, kj_b, kj_c)
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># Check if the reparametrization is correct</span>
</span></span><span style="display:flex;"><span>    kj_abc = glr_to_abc_reparam(kj_gamma, kj_inner_radius, kj_outer_radius, decimals=<span style="color:#099">4</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#00a">assert</span> jnp.allclose(kj_abc, jnp.array([kj_a, kj_b, kj_c]), atol=<span style="color:#099">1e-4</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    x = jnp.concat([
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># The extra 0.1 is there to account for numerical instability</span>
</span></span><span style="display:flex;"><span>        jnp.linspace(<span style="color:#099">0</span>, <span style="color:#099">1.1</span>, <span style="color:#099">2</span>**<span style="color:#099">10</span>),
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># Gradients typically have low stable rank (i.e. most of the singular values are close to 0).</span>
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># To simulate that, we add a couple more points near 0.</span>
</span></span><span style="display:flex;"><span>        jnp.linspace(<span style="color:#099">0</span>, <span style="color:#099">0.1</span>, <span style="color:#099">2</span>**<span style="color:#099">9</span>),
</span></span><span style="display:flex;"><span>    ])
</span></span><span style="display:flex;"><span>    init_params = jnp.array([[kj_gamma, kj_inner_radius, kj_outer_radius]]*args.num_ns_iters)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    trained_params, historical_params, losses = train(
</span></span><span style="display:flex;"><span>        x=x,
</span></span><span style="display:flex;"><span>        params=init_params,
</span></span><span style="display:flex;"><span>        learning_rate=args.learning_rate,
</span></span><span style="display:flex;"><span>        num_steps=args.num_train_steps,
</span></span><span style="display:flex;"><span>        eps=args.eps,
</span></span><span style="display:flex;"><span>        precision=args.precision,
</span></span><span style="display:flex;"><span>        enable_contraction_aux_loss=args.enable_contraction_aux_loss,
</span></span><span style="display:flex;"><span>        enable_flatness_aux_loss=args.enable_flatness_aux_loss,
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    best_params: jax.Array = historical_params[jnp.nanargmin(losses)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    steepness = <span style="color:#099">1.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a">for</span> gamma, l, r <span style="color:#00a">in</span> best_params:
</span></span><span style="display:flex;"><span>        a, b, c = glr_to_abc_reparam(gamma, l, r, args.precision)
</span></span><span style="display:flex;"><span>        <span style="color:#0aa">print</span>(<span style="color:#a50">f</span><span style="color:#a50">&#34;(</span><span style="color:#a50">{</span>a<span style="color:#a50">:</span><span style="color:#a50">.4f</span><span style="color:#a50">}</span><span style="color:#a50">, </span><span style="color:#a50">{</span>b<span style="color:#a50">:</span><span style="color:#a50">.4f</span><span style="color:#a50">}</span><span style="color:#a50">, </span><span style="color:#a50">{</span>c<span style="color:#a50">:</span><span style="color:#a50">.4f</span><span style="color:#a50">}</span><span style="color:#a50">)&#34;</span>)
</span></span><span style="display:flex;"><span>        steepness *= a
</span></span><span style="display:flex;"><span>    <span style="color:#0aa">print</span>(<span style="color:#a50">f</span><span style="color:#a50">&#34;Steepness: </span><span style="color:#a50">{</span>steepness<span style="color:#a50">:</span><span style="color:#a50"> .4f</span><span style="color:#a50">}</span><span style="color:#a50">&#34;</span>)
</span></span></code></pre></div><h2 id="how-to-cite">How to Cite<a hidden class="anchor" aria-hidden="true" href="#how-to-cite">#</a></h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bibtex" data-lang="bibtex"><span style="display:flex;"><span><span style="color:#0a0;text-decoration:underline">@misc</span>{cesista2025muonoptcoeffs,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">author</span> = <span style="color:#a50">{Franz Louis Cesista and YouJiacheng and Keller Jordan}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">title</span> = <span style="color:#a50">{Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">year</span> = <span style="color:#a50">{2025}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">url</span> = <span style="color:#a50">{http://leloykun.github.io/ponder/muon-opt-coeffs/}</span>,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ol>
<li>Keller Jordan, Jeremy Bernstein, Brendan Rappazzo, @fernbear.bsky.social, Boza Vlado, Jiacheng You, Franz Cesista, Braden Koszarsky, and @Grad62304977. modded-nanogpt: Speedrunning the NanoGPT baseline. 2024. Available at: <a href="https://github.com/KellerJordan/modded-nanogpt" target="_blank">https://github.com/KellerJordan/modded-nanogpt</a>.</li>
<li>Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein (2024). Muon: An optimizer for hidden layers in neural networks. Available at: <a href="https://kellerjordan.github.io/posts/muon/" target="_blank">https://kellerjordan.github.io/posts/muon/</a>.</li>
<li>Surya, S., Duvvuri, Devvrit, F., Anil, R., Hsieh, C., &amp; Dhillon, I.S. (2024). Combining Axes Preconditioners through Kronecker Approximation for Deep Learning. International Conference on Learning Representations.</li>
<li>Vineet Gupta, Tomer Koren, Yoram Singer (2018). Shampoo: Preconditioned Stochastic Tensor Optimization. URL <a href="https://arxiv.org/abs/1802.09568" target="_blank">https://arxiv.org/abs/1802.09568</a></li>
<li>Rohan Anil et al. “Scalable second order optimization for deep learning.” arXiv preprint arXiv:2002.09018 (2020).</li>
<li>Jeremy Bernstein and Laker Newhouse. “Old optimizer, new norm: An anthology.” arXiv preprint arXiv:2409.20325 (2024).</li>
<li>Jeremy Bernstein (2024). &ldquo;Weight erasure.&rdquo; Available at: <a href="https://docs.modula.systems/examples/weight-erasure/" target="_blank">https://docs.modula.systems/examples/weight-erasure/</a></li>
<li>Franz Louis Cesista (2025). CASPR Without Accumulation is Muon. URL <a href="https://leloykun.github.io/ponder/caspr-wo-accum-is-muon/" target="_blank">https://leloykun.github.io/ponder/caspr-wo-accum-is-muon/</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://leloykun.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://leloykun.github.io/tags/muon/">Muon</a></li>
      <li><a href="https://leloykun.github.io/tags/nanogpt-speedrun/">NanoGPT Speedrun</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients on x"
            href="https://x.com/intent/tweet/?text=Squeezing%201-2%25%20Efficiency%20Gains%20Out%20of%20Muon%20by%20Optimizing%20the%20Newton-Schulz%20Coefficients&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fmuon-opt-coeffs%2f&amp;hashtags=MachineLearning%2cMuon%2cNanoGPTSpeedrun">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fmuon-opt-coeffs%2f&amp;title=Squeezing%201-2%25%20Efficiency%20Gains%20Out%20of%20Muon%20by%20Optimizing%20the%20Newton-Schulz%20Coefficients&amp;summary=Squeezing%201-2%25%20Efficiency%20Gains%20Out%20of%20Muon%20by%20Optimizing%20the%20Newton-Schulz%20Coefficients&amp;source=https%3a%2f%2fleloykun.github.io%2fponder%2fmuon-opt-coeffs%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fleloykun.github.io%2fponder%2fmuon-opt-coeffs%2f&title=Squeezing%201-2%25%20Efficiency%20Gains%20Out%20of%20Muon%20by%20Optimizing%20the%20Newton-Schulz%20Coefficients">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fleloykun.github.io%2fponder%2fmuon-opt-coeffs%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients on whatsapp"
            href="https://api.whatsapp.com/send?text=Squeezing%201-2%25%20Efficiency%20Gains%20Out%20of%20Muon%20by%20Optimizing%20the%20Newton-Schulz%20Coefficients%20-%20https%3a%2f%2fleloykun.github.io%2fponder%2fmuon-opt-coeffs%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients on telegram"
            href="https://telegram.me/share/url?text=Squeezing%201-2%25%20Efficiency%20Gains%20Out%20of%20Muon%20by%20Optimizing%20the%20Newton-Schulz%20Coefficients&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fmuon-opt-coeffs%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Squeezing 1-2% Efficiency Gains Out of Muon by Optimizing the Newton-Schulz Coefficients on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Squeezing%201-2%25%20Efficiency%20Gains%20Out%20of%20Muon%20by%20Optimizing%20the%20Newton-Schulz%20Coefficients&u=https%3a%2f%2fleloykun.github.io%2fponder%2fmuon-opt-coeffs%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://leloykun.github.io/">Franz Louis Cesista</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
