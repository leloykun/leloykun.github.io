<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds | Franz Louis Cesista</title>
<meta name="keywords" content="Machine Learning, Muon">
<meta name="description" content="Muon from first principles, what makes it different from other optimizers, and why it works so well.">
<meta name="author" content="Franz Louis Cesista">
<link rel="canonical" href="https://leloykun.github.io/ponder/steepest-descent-non-riemannian/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.253701e64acde2b382c010121092cc581e3817c116fed20d9655c14fd77bfad9.css" integrity="sha256-JTcB5krN4rOCwBASEJLMWB44F8EW/tINllXBT9d7&#43;tk=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://leloykun.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://leloykun.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://leloykun.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://leloykun.github.io/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://leloykun.github.io/ponder/steepest-descent-non-riemannian/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds" />
<meta property="og:description" content="Muon from first principles, what makes it different from other optimizers, and why it works so well." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://leloykun.github.io/ponder/steepest-descent-non-riemannian/" />
<meta property="og:image" content="https://leloykun.github.io/ponder/steepest-descent-non-riemannian/cover.png" /><meta property="article:section" content="ponder" />
<meta property="article:published_time" content="2025-03-31T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-03-31T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://leloykun.github.io/ponder/steepest-descent-non-riemannian/cover.png" />
<meta name="twitter:title" content="Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds"/>
<meta name="twitter:description" content="Muon from first principles, what makes it different from other optimizers, and why it works so well."/>
<meta name="twitter:site" content="@leloykun"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Ponder",
      "item": "https://leloykun.github.io/ponder/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds",
      "item": "https://leloykun.github.io/ponder/steepest-descent-non-riemannian/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds",
  "name": "Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds",
  "description": "Muon from first principles, what makes it different from other optimizers, and why it works so well.",
  "keywords": [
    "Machine Learning", "Muon"
  ],
  "articleBody": "A new optimizer called Muon has recently been shown to outperform Adam in both small-scale language model training (Jordan et al., 2024), and larger-scale language model training (Moonshot AI Team, 2025) by a factor of 2x in terms of flops efficiency. For non-matrix-valued parameters in a neural network, Muon falls back to Adam. But for matrix-valued parameters, Muon first semi-orthogonalizes the gradient before subtracting it from the parameter. It can also be viewed as steepest descent under the Spectral norm (Bernstein et al., 2024).\nHere, we will derive Muon’s update rule for matrix-valued parameters and discuss what makes it different from other optimizers and why it works so well.\n1. Preliminaries We consider the following optimization problem: $$\\begin{equation} \\min_{W \\in \\bm{\\mathcal{W}}} \\mathcal{L}(W) \\end{equation}$$ where $\\mathcal{L}(\\cdot): \\bm{\\mathcal{W}} \\rightarrow \\mathbb{R}$ is a bounded-below and differentiable loss function, and $\\bm{\\mathcal{W}}$ is a matrix-valued vector space equipped with a norm $||\\cdot||$ chosen a priori. If the norm admits a metric, then $\\bm{\\mathcal{W}}$ is a Riemannian manifold. Otherwise, it is a non-Riemannian (Finsler) manifold. Thus, not only does the choice of norm naturally lead to different optimization algorithms, but also to two classes of optimizers, preconditioners and dualizers, which we will discuss in the following sections.\nIn practice, $\\mathcal{L}$ often does not have a simple, closed-form solution, so we resort to iterative methods of the form $$W_{t+1} = W_{t} - \\lambda \\widehat{\\Delta W}_t,$$ where $\\lambda \u003e 0$ is a positive learning rate parameter and $-\\widehat{\\Delta W}_t$ is the direction of steepest descent at step $t$, $$ \\begin{align} -\\widehat{\\Delta W}_t \u0026= \\arg\\min_{\\substack{\\Delta W \\in T_{W_t}\\mathcal{W}\\\\ ||\\Delta W|| = 1}} d\\mathcal{L}_{W_t}(\\Delta W)\\nonumber\\\\ \\widehat{\\Delta W}_t \u0026= \\arg\\max_{\\substack{\\Delta W \\in T_{W_t}\\mathcal{W}\\\\ ||\\Delta W|| = 1}} d\\mathcal{L}_{W_t}(\\Delta W) \\end{align} $$ where $T_{W_t}\\mathcal{W}$ is the tangent space at $W_t$, $d\\mathcal{L}_{W_t}(\\cdot): T_{W_t}\\mathcal{W} \\rightarrow \\mathbb{R}$ is the differential of $\\mathcal{L}$ at $W_t$, and $d\\mathcal{L}_{W_t}(\\Delta W)$ is the directional derivative of $\\mathcal{L}$ at $W_t$ in the direction of $\\Delta W$.\nWe also often do not have access to the exact differential. However, either through, e.g., backpropagation if downstream operations are differentiable or evolutionary algorithms otherwise, we often do have access to a stochastic estimator $g_W(\\cdot, \\xi)_{\\text{coord}}$ of the differential in standard Euclidean coordinates,\nAssumption 1: Suppose that, for all $W \\in \\bm{\\mathcal{W}}$, the differential $d\\mathcal{L}_W(\\cdot)$ has a standard Euclidean coordinate representation $\\nabla \\mathcal{L}(W)_{\\text{coord}}$ such that $d\\mathcal{L}_W(\\cdot) = \\langle \\nabla \\mathcal{L}(W)_{\\text{coord}}, \\cdot \\rangle_F$. We assume that we have access to a stochastic estimator $g_W(\\cdot, \\xi)_{\\text{coord}} = \\langle \\nabla \\mathcal{L}(W)_\\xi, \\cdot \\rangle_F$ of the differential in coordinate form that is unbiased and has bounded variance. That is, $$ \\begin{align*} \u0026\\mathbb{E}_{\\xi \\sim D}[\\nabla \\mathcal{L}(W)_\\xi] = \\nabla \\mathcal{L}(W)_{\\text{coord}} \u0026\u0026 \\forall W \\in \\bm{\\mathcal{W}}\\\\ \u0026\\mathbb{E}_{\\xi \\sim D}[||\\nabla \\mathcal{L}(W)_\\xi - \\nabla \\mathcal{L}(W)_{\\text{coord}} ||_F^2] \\leq \\sigma^2 \u0026\u0026 \\forall W \\in \\bm{\\mathcal{W}} \\end{align*} $$ where $\\xi$ is a random variable sampled from a distribution $D$, $\\sigma \u003e 0$ is a positive variance parameter, $\\langle \\cdot, \\cdot \\rangle_F$ is the Frobenius inner product, and $||\\cdot||_F = \\sqrt{\\langle \\cdot, \\cdot \\rangle_F}$.\nWe also make the following standard continuity assumption on the gradient $\\nabla \\mathcal{L}(\\cdot)$,\nAssumption 2: The gradient $\\nabla \\mathcal{L}(\\cdot)$ is Lipschitz continuous with respect to the norm $||\\cdot||$ with gradient Lipschitz constant $L \u003e 0$. That is, for all $W, Z \\in \\bm{\\mathcal{W}}$, $$ \\begin{equation} ||\\nabla \\mathcal{L}(W) - \\nabla \\mathcal{L}(Z)||^\\dagger \\leq L||W - Z|| \\quad \\forall W, Z \\in \\bm{\\mathcal{W}} \\end{equation} $$ where $||\\cdot||^\\dagger$ is the dual norm of $||\\cdot||$.\nAnd in the following sections, we will also discuss optimizers that precondition the gradients,\nDefinition 1 (Preconditioning). In an optimization algorithm, a preconditioner $\\mathcal{P}(\\cdot; W): T_W\\mathcal{W} \\rightarrow T_W\\mathcal{W}$ is a point-dependent linear transform that maps empirical gradients $\\nabla \\mathcal{L}(W)_\\xi$ to update directions $\\Delta W$. That is, at any $W \\in \\mathcal{W}$, we have a matrix $P_W$ such that $\\mathcal{P}(\\nabla \\mathcal{L}(W)_\\xi; W) = P_W \\nabla \\mathcal{L}(W)_\\xi$ and, $$ \\begin{align*} \\Delta W_t \u0026= P_{W_t} \\nabla \\mathcal{L}(W_t)\\\\ W_{t+1} \u0026= W_t - \\lambda P_{W_t} \\nabla \\mathcal{L}(W_t) \\end{align*} $$ It is also common to assume that we can decompose $P_W$ into a Kronecker product $P_W = L_W \\otimes R_W$ (Li, 2015; Gupta et al., 2018, Surya et al., 2024), such that our update rule becomes $$ \\begin{align*} \\Delta W_t \u0026= P_{W_t} \\nabla \\mathcal{L}(W_t)\\\\ \\Delta W_t \u0026= L_{W_t} \\nabla \\mathcal{L}(W_t) R_{W_t}\\\\ W_{t+1} \u0026= W_t - \\lambda L_{W_t} \\nabla \\mathcal{L}(W_t) R_{W_t} \\end{align*} $$ We call $L_W$ and $R_W$ as the left and right preconditioners, respectively.\n2. Why do Steepest Descent Under the Spectral Norm? The geometry of $\\mathcal{W}$ and the optimizer we will need both depend on the choice of norm $||\\cdot||$. Our core argument is that it is most natural to do steepest descent under the spectral norm $||\\cdot||_{2 \\to 2}$ in the context of training the linear weights $W$ of a neural network. The spectral norm induces $\\mathcal{W}$ to be non-Riemannian, and therefore, intuitions on optimization we have developed in Riemannian manifolds may not apply.\n2.1. Majorization-Minimization Perspective We can upper bound our objective function $\\mathcal{L}$ by the following approximation at an arbitrary point $Z \\in \\bm{\\mathcal{W}}$, $$ \\begin{equation} \\mathcal{U}(W; Z) = \\mathcal{L}(Z) + \\langle \\nabla \\mathcal{L}(Z)_\\xi, W - Z \\rangle_F + \\frac{\\lambda}{2}||W - Z||^2 \\end{equation} $$ for some norm $||\\cdot||$. Using standard arguments, we can show that $\\mathcal{L}(W) \\leq \\mathcal{U}(W; Z)$ for all $W \\in \\bm{\\mathcal{W}}$ as long as $\\lambda \\leq L$ (Hunter et al., 2004). Minimizing this upper bound is equivalent to minimizing the original objective function; the tighter the bound, the better. And as discussed by Carlson et al. (2015), the spectral norm gives us a tight upper bound and is thus a good choice. In fact, the spectral norm gives the tightest bound among all the Schatten-$p$ norms.\n2.2 Feature Learning Perspective Suppose that we have a linear transform $x_{l+1} = W_{l} x_{l}$ at the $l$-th layer of a neural network where $x_l \\in \\mathcal{R}^{d_l}$ and $x_{l+1} \\in \\mathcal{R}^{d_{l+1}}$ are the input and output hidden representations (or “features”), respectively, and $W_l \\in \\mathcal{R}^{d_{l+1} \\times d_l}$ is the weight matrix. Additionally, let $\\Delta x_l \\in \\mathcal{R}^{d_l}$, $\\Delta x_{l+1} \\in \\mathcal{R}^{d_{l+1}}$, and $\\Delta W_l \\in \\mathcal{R}^{d_{l+1} \\times d_l}$ be their updates after a backward pass.\nIdeally, we want the sizes of both the hidden representations $x_l$ and their updates $\\Delta x_l$ to scale with the model width $d_l$. Otherwise, if the hidden representations are ’too small’, we are wasting capacity, in a sense; and if they are ’too large’, we are pushing the model towards the edge of numerical stability and prevent grokking (Prieto et al., 2025). Likewise, if the updates are ’too small’, they vanish at larger scales, slowing down convergence; and if they are ’too large’, they cause training instability. Yang et al. (2024) summarizes this as follows,\nDesideratum 1 (Feature Learning). We desire that our features $x_l$ and feature updates $\\Delta x_l$ be of size, $$ \\begin{equation} ||x_l||_2 = \\Theta(\\sqrt{d_l})\\quad\\text{and}\\quad ||\\Delta x_l||_2 = \\Theta(\\sqrt{d_l})\\quad\\text{for all layers } l = 1, 2, \\ldots, L-1 \\end{equation} $$\nWe ensure this by imposing constraints on the size of the weights $W_l$ and their updates $\\Delta W_l$:\nFrom the definition of the spectral norm, we have, $$ \\begin{align*} x_{l+1} \u0026= W_l x_l\\\\ ||x_{l+1}||_2 \u0026\\leq ||W_l||_{2\\to 2} \\cdot ||x_l||_2 \\end{align*} $$ Combining this with Desideratum 1, we have, $$ \\begin{align*} \\underbrace{||x_{l+1}||_2}_{\\Theta(\\sqrt{d_{l+1}})} \u0026\\leq ||W_l||_{2\\to 2} \\cdot \\underbrace{||x_l||_2}_{\\Theta(\\sqrt{d_l})} \\end{align*} $$ Thus the size of the weights $W_l$ must be, $$ \\begin{equation} ||W_l||_{2 \\to 2} = \\Theta\\left(\\sqrt{\\frac{d_{l+1}}{d_l}}\\right) \\end{equation} $$\nNow let’s consider the feature updates $\\Delta x_l$, $$ \\begin{align*} x_{l+1} + \\Delta x_{l+1} \u0026= (W_l + \\Delta W_l)(x_l + \\Delta x_l)\\\\ \\Delta x_{l+1} \u0026= W_l \\Delta x_l + \\Delta W_l x_l + \\Delta W_l \\Delta x_l\\\\ ||\\Delta x_{l+1}||_2 \u0026\\leq ||W_l||_{2\\to 2} \\cdot ||\\Delta x_l||_2 + ||\\Delta W_l||_{2\\to 2} \\cdot ||x_l||_2 + ||\\Delta W_l||_{2\\to 2} \\cdot ||\\Delta x_l||_2 \\end{align*} $$ Combining this with Desideratum 1 and our result above, we have, $$ \\begin{align*} \\underbrace{||\\Delta x_{l+1}||_2}_{\\Theta(\\sqrt{d_{l+1}})} \u0026\\leq \\underbrace{||W_l||_{2\\to 2}}_{\\Theta\\left(\\sqrt{\\frac{d_{l+1}}{d_l}}\\right)} \\cdot \\underbrace{||\\Delta x_l||_2}_{\\Theta\\left(\\sqrt{d_l}\\right)} + ||\\Delta W_l||_{2\\to 2} \\cdot \\underbrace{||x_l||_2}_{\\Theta\\left(\\sqrt{d_l}\\right)} + ||\\Delta W_l||_{2\\to 2} \\cdot \\underbrace{||\\Delta x_l||_2}_{\\Theta\\left(\\sqrt{d_l}\\right)} \\end{align*} $$ Thus the size of the weight updates $\\Delta W_l$ must be, $$||\\Delta W_l||_{2 \\to 2} = \\Theta\\left(\\sqrt{\\frac{d_{l+1}}{d_l}}\\right)$$\nThese become our Spectral Scaling Conditions (Yang et al., 2024),\nCondition 1 (Spectral Scaling). The spectral norms of our weights $W_l$ and weight updates $\\Delta W_l$ must be, $$ \\begin{equation} ||W_l||_{2\\to 2} = \\Theta\\left(\\sqrt{\\frac{d_{l+1}}{d_l}}\\right)\\quad\\text{and}\\quad||\\Delta W_l||_{2\\to 2} = \\Theta\\left(\\sqrt{\\frac{d_{l+1}}{d_l}}\\right)\\quad\\text{at layers } l = 1, \\ldots, L-1 \\end{equation} $$\n2.3 Input-Tensor Alignment Phenomenon [Under Construction] 3. Steepest Descent in Riemannian and Non-Riemannian Manifolds Let us consider the different cases of the geometry of $\\bm{\\mathcal{W}}$ induced by the choice of norm $||\\cdot||$.\n3.1. $\\bm{\\mathcal{W}}$ is Euclidean That is, we pick the Frobenius norm $||\\cdot||_F$ as our norm. In this case, our points, differentials, and gradients are all already in standard Euclidean coordinates and we have a canonical bijection between differentials $d\\mathcal{L}_W(\\cdot) \\in T_W^* \\bm{\\mathcal{W}}$ and gradients $\\nabla \\mathcal{L}(W) \\in T_W \\bm{\\mathcal{W}}$ such that $$d\\mathcal{L}_W(\\cdot) = \\langle \\nabla \\mathcal{L}(W), \\cdot \\rangle_F$$ Thus,\n$$ \\begin{align*} \\widehat{\\Delta W} \u0026= \\arg\\max_{\\substack{\\Delta W \\in T_W\\mathcal{W}\\\\ ||\\Delta W|| = 1}} d\\mathcal{L}_W(\\Delta W)\\\\ \u0026= \\arg\\max_{\\substack{\\Delta W \\in T_W\\mathcal{W}\\\\ ||\\Delta W|| = 1}} \\langle \\nabla \\mathcal{L}(W), \\Delta W \\rangle_F\\\\ \u0026= \\arg\\max_{\\substack{\\Delta W \\in T_W\\mathcal{W}\\\\ ||\\Delta W|| = 1}} \\langle \\nabla \\mathcal{L}(W)_{\\text{coord}}, \\Delta W \\rangle_F\\\\ \u0026= \\frac{\\nabla \\mathcal{L}(W)_{\\text{coord}}}{||\\nabla \\mathcal{L}(W)_{\\text{coord}}||_F}\\\\ \\widehat{\\Delta W} \u0026\\approx \\frac{\\nabla \\mathcal{L}(W)_\\xi}{||\\nabla \\mathcal{L}(W)_\\xi||_F}\\\\ \\end{align*} $$\nThus, our update rule becomes, $$W_{t+1} = W_t - \\hat{\\lambda} \\nabla \\mathcal{L}(W)_\\xi$$ where $\\hat{\\lambda} = \\frac{\\lambda}{||\\nabla \\mathcal{L}(W)_\\xi||_F}$. This is simply Stochastic Gradient Descent (SGD) with an adaptive learning rate.\n3.2. $\\bm{\\mathcal{W}}$ is a Riemannian Manifold That is, our choice of norm $||\\cdot||$ admits a metric $g_W(\\cdot, \\cdot): T_W \\bm{\\mathcal{W}} \\times T_W \\bm{\\mathcal{W}} \\rightarrow \\mathbb{R}$ for each $W \\in \\bm{\\mathcal{W}}$ such that $$||U|| = \\sqrt{g_W(U, U)}\\quad\\text{and}\\quad g_W(U, V) = \\langle U, V \\rangle_{G_W} = \\langle GU, V \\rangle_F.\\quad \\forall U,V \\in \\mathcal{W}$$ for some positive-definite matrix $G_W$ for each $W \\in \\bm{\\mathcal{W}}$. Case 1 above is a special case of this where $G_W = I$ for all $W \\in \\bm{\\mathcal{W}}$ and thus, $||U||_F = \\sqrt{g_W(U, U)} = \\sqrt{\\langle U, U \\rangle_F} \\forall U \\in \\mathcal{W}$.\nAn interesting property of Riemannian manifolds is that we have a canonical bijection between differentials $d\\mathcal{L}_W(\\cdot) \\in T_W^* \\bm{\\mathcal{W}}$ and gradients $\\nabla \\mathcal{L}(W) \\in T_W \\bm{\\mathcal{W}}$ such that $$d\\mathcal{L}_W(\\cdot) = \\langle \\nabla \\mathcal{L}(W), \\cdot \\rangle$$\nNow notice that, $$ \\begin{align*} d\\mathcal{L}_W(\\cdot) \u0026= \\langle \\nabla \\mathcal{L}(W), \\cdot \\rangle\\\\ d\\mathcal{L}_W(\\cdot) \u0026= \\langle G_W\\nabla \\mathcal{L}(W), \\cdot \\rangle_F\\\\ G_W\\nabla \\mathcal{L}(W) \u0026= \\nabla \\mathcal{L}(W)_{\\text{coord}}\\\\ \\nabla \\mathcal{L}(W) \u0026= G_W^{-1} \\nabla \\mathcal{L}(W)_{\\text{coord}}\\\\ \\end{align*} $$\nThus,\n$$ \\begin{align} \\widehat{\\Delta W} \u0026= \\arg\\max_{\\substack{\\Delta W \\in T_W\\mathcal{W}\\\\ ||\\Delta W|| = 1}} d\\mathcal{L}_W(\\Delta W)\\nonumber\\\\ \u0026= \\arg\\max_{\\substack{\\Delta W \\in T_W\\mathcal{W}\\\\ ||\\Delta W|| = 1}} \\langle \\nabla \\mathcal{L}(W), \\Delta W \\rangle\\nonumber\\\\ \u0026= \\arg\\max_{\\substack{\\Delta W \\in T_W\\mathcal{W}\\\\ ||\\Delta W|| = 1}} \\langle G_W^{-1} \\nabla \\mathcal{L}(W)_{\\text{coord}}, \\Delta W \\rangle\\\\ \u0026= \\frac{G_W^{-1} \\nabla \\mathcal{L}(W)_{\\text{coord}}}{||G_W^{-1}\\nabla \\mathcal{L}(W)_{\\text{coord}}||}\\nonumber\\\\ \\widehat{\\Delta W} \u0026\\approx \\frac{G_W^{-1}\\nabla \\mathcal{L}(W)_\\xi}{||G_W^{-1}\\nabla \\mathcal{L}(W)_\\xi||}\\nonumber\\\\ \\end{align} $$ where the maximum above can be achieved by aligning $\\Delta W$ with $G_W^{-1}\\nabla \\mathcal{L}(W)_{\\text{coord}}$. Thus our update rule becomes, $$W_{t+1} = W_t - \\hat{\\lambda} G_{W_t}^{-1}\\nabla \\mathcal{L}(W_t)_\\xi$$ where $\\hat{\\lambda} = \\frac{\\lambda}{||G_{W_t}^{-1}\\nabla \\mathcal{L}(W_t)_\\xi||}$. This is Riemannian Stochastic Gradient Descent (RSGD) with an adaptive learning rate. And if we let $P_W = G_W^{-1}$ be the preconditioner at point $W$, we can relate this to Preconditioned Stochastic Gradient Descent (PSGD) algorithms (Li, 2015; Pooladzandi et al., 2024).\n3.3. $\\bm{\\mathcal{W}}$ is a Non-Riemannian Manifold In this case, our choice of norm $||\\cdot||$ does not admit a well-behaved metric $g_W(\\cdot, \\cdot)$ and consequently also does not admit a well-behaved inner product $\\langle \\cdot, \\cdot \\rangle$ such that $||\\cdot|| = \\sqrt{\\langle \\cdot, \\cdot \\rangle}$ for all $W \\in \\mathcal{W}$. Our differentials $d\\mathcal{L}_W(\\cdot)$ are still well-defined, but we no longer have the bijective relationship between differentials and gradients. And so, we do not always have a unique $\\nabla \\mathcal{L}(W)$ such that $d\\mathcal{L}_W(\\cdot) = \\langle \\nabla \\mathcal{L}(W), \\cdot \\rangle$ if this inner product even exists.\nWhile we still have access to the stochastic estimator of the differential in standard Euclidean coordinates $\\nabla \\mathcal{L}(W)_{\\text{coord}}$ from Assumption 1, it no longer has geometric meaning by itself. That is, a simple change of coordinates no longer tells us information on the direction of steepest descent. We can, however, still use it to define a dualizer that maps the differentials we get empirically to update directions, $$ \\begin{align*} \\widehat{\\Delta W} \u0026= \\arg\\max_{\\substack{\\Delta W \\in T_W\\mathcal{W}\\\\ ||\\Delta W|| = 1}} d\\mathcal{L}_W(\\Delta W)\\\\ \u0026= \\arg\\max_{\\substack{\\Delta W \\in T_W\\mathcal{W}\\\\ ||\\Delta W|| = 1}} \\langle \\nabla \\mathcal{L}(W)_{\\text{coord}}, \\Delta W \\rangle_F\\\\ \u0026\\approx \\arg\\max_{\\substack{\\Delta W \\in T_W\\mathcal{W}\\\\ ||\\Delta W|| = 1}} \\langle \\nabla \\mathcal{L}(W)_\\xi, \\Delta W \\rangle_F\\\\ \\widehat{\\Delta W} \u0026= \\text{dualizer}_{||\\cdot||}(\\nabla \\mathcal{L}(W)_\\xi; W) \\end{align*} $$ where $$ \\begin{equation} \\text{dualizer}_{||\\cdot||}(\\nabla \\mathcal{L}(W)_\\xi; W) = \\arg\\max_{\\substack{\\Delta W \\in T_W\\mathcal{W}\\\\ ||\\Delta W|| = 1}} \\langle \\nabla \\mathcal{L}(W)_\\xi, \\Delta W \\rangle_F \\end{equation} $$ and to simplify our notation, we use $\\text{dualizer}_{||\\cdot||}(\\nabla \\mathcal{L}(W)_\\xi)$ if this map is independent of $W$.\n4. Muon as Steepest Descent in a Non-Riemannian Manifold 4.1. The Muon Optimizer Algorithm 1 (Muon) by Jordan et al. (2024). The weights are treated independently.\nInputs: Initial weight $W_0 \\in \\mathcal{W}$, and momentum term $M_0 \\in \\mathcal{W}$.\nParameters: Learning rate $\\lambda \u003e 0$, momentum decay $\\beta \\in [0, 1)$, and number of iterations $T \\in \\{1, 2, \\ldots\\}$\n$\\textbf{for } t = 0, 1, \\ldots, T-1 \\textbf{ do}\\\\ \\text{… Compute }G_t = \\nabla \\mathcal{L}(W)_\\xi\\\\ \\text{… Compute }W_{t+1}\\text{ and }M_{t+1}\\text{ as follows:}\\\\ \\text{……. }M_{t+1} = \\beta M_t + (1 - \\beta) G_t\\\\ \\text{……. }O_{t+1} = \\text{approx-orth}(M_{t+1})\\\\ \\text{……. }W_{t+1} = W_t - \\lambda O_{t+1} $\nOutput: $W_T \\in \\mathcal{W}$.\nAlgorithm 2 (Approximate Orthogonalization through Newton-Schulz Iteration)\ndef zeropower_via_newtonschulz(G: Tensor, steps: int=5): assert G.ndim == 2 a, b, c = (3.4445, -4.7750, 2.0315) X = G.bfloat16() X /= (X.norm() + 1e-7) if G.size(-2) \u003e G.size(-1): X = X.mT for _ in range(steps): A = X @ X.mT B = b * A + c * A @ A X = a * X + B @ X if G.size(-2) \u003e G.size(-1): X = X.mT return X Muon (Algorithm 1) is an optimizer for matrix-valued parameters in neural networks (Jordan et al., 2024). For each weight $W \\in \\mathcal{W}$, it first accumulates the momentum term, then approximately semi-orthogonalizes the result using the Newton-Schulz iteration (Algorithm 2), before applying it as an update to the weights.\nWe can fold the momentum term into $\\nabla \\mathcal{L}(W)_\\xi$ as it can be seen as a way to smooth out outlier empirical gradients. In fact, Kovalev (2025) has recently shown that, under Muon’s update rule, the momentum term does becomes a tighter approximation of the true gradient $\\nabla \\mathcal{L}(W)_{\\text{coord}}$ as the number of iterations $T$ increases.\nAnd while Muon only approximately (semi-)orthogonalizes the gradient, we have found that it still empirically performs just as well as exact orthogonalization. We will discuss this in more detail in the next sections. Muon is also not the first optimizer that does approximate orthogonalization. For example, Carlson et al.’s randomized algorithm Sketching (2015) does this explicitly, and so does Shampoo (Gupta et al., 2018), CASPR (Surya et al., 2024), and PSGD (Li, 2015) implicitly through their preconditioners. However, Muon is the first, non-randomized, preconditioner-free optimizer that explicitly aims to orthogonalize the gradient.\nAn interesting fact from prior work (Carlson et al., 2015; Flynn, 2017; Bernstein et al., 2024) is that the dualizer for steepest descent under the spectral norm $||\\cdot||_{2 \\to 2}$ is exactly this orthogonalization process, $$ \\begin{equation} \\text{dualizer}_{||\\cdot||_{2\\to 2}}(\\nabla \\mathcal{L}(W)_\\xi) = UV^T \\end{equation} $$ where $U\\Sigma V^T$ is the singular value decomposition (SVD) of $\\nabla \\mathcal{L}(W)_\\xi$. The spectral norm does not admit a well-behaved inner product. And so, Muon, and related optimizers, can be thought of as steepest descent in a non-Riemannian manifold.\nIn the next section, we discuss why Muon can be viewed as an instantaneous version of already existing optimizers such as Shampoo, CASPR, PSGD, and etc. We will also discuss an alternate perspective on how such preconditioning optimizers can be viewed as approximators to the dualizer of the spectral norm.\n4.2 Approximating Dualization with Preconditioners As we have shown in Section 3.2, the dualization process in Riemannian steepest descent always has an equivalent preconditioning process by letting $P_W = G_W^{-1}$ at every point $W \\in \\bm{\\mathcal{W}}$. And likewise, if we have a preconditioning process where every $P_W$ is invertible, then it can be thought of as Riemannian steepest descent under the metric $G_W = P_W^{-1}$.\nHowever, we may not always have an equivalent preconditioning process in non-Riemannian manifolds. And if there is, it may not be unique. Here, let’s examine multiple preconditioning processes that approximate the dualizer under the spectral norm in turn.\n4.2.1. The matrix sign function. Muon’s update rule is actually equivalent to the matrix sign function,\n$$ \\begin{align*} \\text{msign}(\\nabla \\mathcal{L}_\\xi(W)) \u0026= (\\nabla \\mathcal{L}_\\xi(W) \\nabla \\mathcal{L}_\\xi(W)^T)^{-1/2} \\nabla \\mathcal{L}_\\xi(W)\\\\ \u0026= (U\\Sigma V^T V \\Sigma U^T)^{-1/2} U\\Sigma V^T\\\\ \\text{msign}(\\nabla \\mathcal{L}_\\xi(W)) \u0026= U V^T, \\end{align*} $$ where $U\\Sigma V^T$ is the singular value decomposition (SVD) of $\\nabla \\mathcal{L}_\\xi(W)$.\nAnd if we let $P_W = (\\nabla \\mathcal{L}_\\xi(W) \\nabla \\mathcal{L}_\\xi(W)^T)^{-1/2}$, then we arrive at a preconditioner form of Muon’s update rule, $$ \\begin{align*} W_{t+1} \u0026= W_t - \\lambda P_W \\nabla \\mathcal{L}_\\xi(W_t)\\\\ \u0026= W_t - \\lambda (\\nabla \\mathcal{L}_\\xi(W_t) \\nabla \\mathcal{L}_\\xi(W_t)^T)^{-1/2} \\nabla \\mathcal{L}_\\xi(W_t)\\\\ W_{t+1} \u0026= W_t - \\lambda UV^T \\end{align*} $$\n4.2.2. Shampoo. Let $G_t = \\nabla \\mathcal{L}_\\xi(W_t)$. Then Shampoo (Gupta et al., 2018; Anil et al., 2020) has the following update rule,\n$$L_t := L_{t-1} + G_t G_t^T, \\quad\\quad R_t := R_{t-1} + G_t^T G_t$$ $$\\Delta W_t = L^{-1/4}_t G_t R^{-1/4}_t$$\nAs previously noted by Bernstein et al. (2024) and Anil (2024), Shampoo reduces to Muon if we disable the updates on the left and right preconditioners. That is, let $G_t = U\\Sigma V^T$ be the singular value decomposition (SVD) of $G_t$ and let $$ L_t := G_t G_t^T\\quad\\quad R_t := G_t^T G_t $$ Then, $$\\begin{aligned} \\Delta W_t \u0026= L^{-1/4}_t G_t R^{-1/4}_t\\\\ \u0026= (G_t G_t^T)^{-1/4} G_t (G_t^T G_t)^{-1/4}\\\\ \u0026= (U\\Sigma V^T V \\Sigma U^T)^{-1/4} U\\Sigma V^T (V \\Sigma U^T U \\Sigma V^T)^{-1/4}\\\\ \u0026= U\\left(\\frac{\\Sigma}{\\sqrt{\\Sigma^2}} \\right)V^T\\\\ \\Delta W_t \u0026= UV^T \\end{aligned}$$ which is Muon’s update rule. $\\blacksquare$\n4.2.3. CASPR. Let $G_t = \\nabla \\mathcal{L}_\\xi(W_t)$. Then CASPR (Surya et al., 2024) has the following update rule,\n$$L_t := L_{t-1} + G_t G_t^T, \\quad\\quad R_t := R_{t-1} + G_t^T G_t$$ $$\\tilde{L}_t := L_t + \\epsilon I_m, \\quad\\quad \\tilde{R}_t := R_t + \\epsilon I_n$$ $$\\Delta W_t = (\\tilde{L}^{-1/2}_t G_t + 2 \\tilde{L}^{-1/4}_t G_t \\tilde{R}^{-1/4}_t + G_t \\tilde{R}^{-1/2}_t)/4.$$\nAs previously noted by Cesista (2025), CASPR reduces to Muon if we disable the updates on the left and right preconditioners. That is, let $G_t = U\\Sigma V^T$ be the singular value decomposition (SVD) of $G_t$ and let $$ L_t := G_t G_t^T\\quad\\quad R_t := G_t^T G_t $$ Then, $$\\begin{aligned} \\Delta W_t \u0026= (\\tilde{L}^{-1/2}_t G_t + 2 \\tilde{L}^{-1/4}_t G_t \\tilde{R}^{-1/4}_t + G_t \\tilde{R}^{-1/2}_t)/4\\\\ \u0026= (1/4) \\cdot [(G_t G_t^T + \\epsilon I_m)^{-1/2} G_t\\\\ \u0026\\quad\\quad\\quad+ 2 (G_t G_t^T + \\epsilon I_m)^{-1/4} G_t (G_t^T G_t + \\epsilon I_n)^{-1/4}\\\\ \u0026\\quad\\quad\\quad+ G_t (G_t^T G_t + \\epsilon I_n)^{-1/2}]\\\\ \u0026= (1/4) \\cdot [[(U \\Sigma V^T) (U \\Sigma V^T)^T + \\epsilon U I U^T]^{-1/2}(U \\Sigma V^T)\\\\ \u0026\\quad\\quad\\quad + 2[(U \\Sigma V^T) (U \\Sigma V^T)^T + \\epsilon U I U^T]^{-1/4}(U \\Sigma V^T) [(U \\Sigma V^T)^T (U \\Sigma V^T) + \\epsilon V I V^T]^{-1/4}\\\\ \u0026\\quad\\quad\\quad + (U \\Sigma V^T) [(U \\Sigma V^T)^T (U \\Sigma V^T) + \\epsilon V I V^T]^{-1/2}]\\\\ \u0026= (1/4) \\cdot [U(\\Sigma(\\Sigma^2 + \\epsilon I)^{-1/2})V^T + U(2\\Sigma(\\Sigma^2 + \\epsilon I)^{-1/2})V^T + U(\\Sigma(\\Sigma^2 + \\epsilon I)^{-1/2})V^T]\\\\ \u0026= (1/4) \\cdot [U(1+2+1)(\\Sigma(\\Sigma^2 + \\epsilon I)^{-1/2})V^T]\\\\ \u0026= U\\left(\\frac{\\Sigma}{\\sqrt{\\Sigma^2 + \\epsilon I}} \\right)V^T \\\\ \\Delta W_t \u0026\\approx UV^T \\end{aligned}$$ which is Muon’s update rule. $\\blacksquare$\n4.2.4. PSGD Family. [Under Review] This family of optimizers (Li, 2015 \u0026 2018; Pooladzandi, 2024) explicitly tries to learn the preconditioner $\\mathcal{P}(\\cdot; W)$ according to several criteria to ensure training stability and, potentially, faster convergence. One of which is the noise suppression gain, $$ \\frac{\\mathbb{E}[||H_0^{-1}\\epsilon’||_F^2]}{\\mathbb{E}[||P\\epsilon’||_F^2]} = \\frac{\\mathbb{E}[(\\epsilon’)^T H_0^{-2} \\epsilon’]}{\\mathbb{E}[(\\epsilon’)^T P^2 \\epsilon’]} $$ where $\\epsilon’$ is some (matrix-valued) noise term on the Hessian $H$, which aims to reduce the noise of the preconditioned gradients.\nThese criteria typically result in update rules that whitens, that is decorrelates, the entries of the gradient $\\nabla \\mathcal{L}(W)_\\xi$. And so while Muon can be seen as an instantaneous version of PSGD, an important difference is that Muon merely projects the gradient to its nearest (semi-)orthogonal matrix, but not necessarily decorrelates the entries.\nFor future work, it would also be interesting to see what kind of update rules we get if we measure the noise suppression gain with respect to the spectral norm instead of the Frobenius norm. That is, $$ \\frac{\\mathbb{E}[||H_0^{-1}\\epsilon’||_{2\\to 2}]}{\\mathbb{E}[||P\\epsilon’||_{2\\to 2}]} $$\n5. Steepest Descent in Non-Riemannian Manifolds Induced by Schatten-$p$ Norms Definition 2 (Schatten-$p$ Norms). The Schatten-$p$ norm of a real-valued matrix $X \\in \\mathbb{R}^{m \\times n}$ is defined as: $$||X||_p = \\left(\\sum_{i=1}^{\\min(m, n)} |\\sigma_i(X)|^p\\right)^{1/p},$$ where $\\sigma_i(X)$ are the singular values of $X$. In a sense, the Schatten-$p$ norm of $X$ can be seen as the $p$-norm of its singular values.\nExamples:\n$p = 1$: The Nuclear norm, $||A||_{S_1} = \\sum_{i=1}^{\\min(m,n)} |\\sigma_i(A)| = ||A||_{\\text{nuc}}$ $p = 2$: The Frobenius norm, $||A||_{S_2} = \\left(\\sum_{i=1}^{\\min(m,n)} |\\sigma_i(A)|^2\\right)^{\\frac{1}{2}} = ||A||_F$ $p = \\infty$: The Spectral norm, $||A||_{S_{\\infty}} = \\max_{i} \\sigma_i(A) = ||A||_{2 \\to 2}$ A special case is when $p = 2$, and so we have the Frobenius norm. Equipping $\\mathcal{W}$ with this norm gives us the standard Euclidean manifold, which is Riemannian. However, Proposition (4) below still applies.\nAnd to find the dualizers for the Schatten-$p$ norms, we will use the following inequality.\nTheorem 3 (von Neumann’s Trace Inequality). Let $A, B \\in \\mathbb{R}^{m \\times n}$. Then the following inequality holds, $$\\text{tr}(A^TB) \\leq \\sum_{i=1}^{\\min(m,n)} \\sigma_i(A) \\sigma_i(B),$$ where $\\sigma_i(A)$ and $\\sigma_i(B)$ are the singular values of $A$ and $B$, respectively. And equality holds if and only if $A$ and $B$ share singular vectors.\n5.1. Dualizers for Schatten-$p$ Norms Here, we derive the dualizer for an arbitrary Schatten-$p$ norm $||\\cdot||_{S_p}$.\nProposition 4. The dualizer for the Schatten-$p$ norm is: $$\\text{dualizer}_{||\\cdot||_{S_p}}(X) = U \\frac{\\text{diag}\\left(\\sigma_1(X)^{q-1}, \\ldots, \\sigma_{\\min(m,n)}(X)^{q-1}\\right)}{||X||_{S_q}^{q-1}} V^T$$ where $\\frac{1}{p} + \\frac{1}{q} = 1$, and $X = U\\Sigma V^T$ is the singular value decomposition of $X \\in T_W\\mathcal{W}$ at $W \\in \\mathcal{W}$. Note that this dualizer is independent of $W$.\nProof: For a given $X \\in T_W\\mathcal{W}$ at $W \\in \\mathcal{W}$, let $T^* \\in T_W\\mathcal{W}$ be, $$ \\begin{align*} T^* \u0026= \\text{dualizer}_{||\\cdot||_{S_p}}(X; W)\\\\ T^* \u0026= \\arg\\max_{\\substack{T \\in T_W\\mathcal{W}\\\\ ||T||_{S_p} = 1}} \\langle X, T \\rangle_F\\\\ T^* \u0026= \\arg\\max_{\\substack{T \\in T_W\\mathcal{W}\\\\ ||T||_{S_p} = 1}} \\text{tr}(X^T T) \\end{align*} $$ Then from von Neumann’s Trace Inequality, we know that $T^*$ must share singular vectors with $X$ and that, $$T^* = \\arg\\max_{\\substack{T \\in T_W\\mathcal{W}\\\\ ||T||_{S_p} = 1}} \\sum_{i=1}^{\\min(m,n)} \\sigma_i(X) \\sigma_i(T)$$ Thus, our optimization problem reduces to $$\\arg\\max_{\\{\\sigma_i(T)\\}} \\sum_{i=1}^{\\min(m,n)} \\sigma_i(X) \\sigma_i(T) \\quad\\text{s.t.}\\quad \\sum_{i=1}^{\\min(m,n)} \\sigma_{i}(T)^p = 1$$ And solving via Lagrange multipliers, we have, $$\\sigma_i(T) = \\frac{\\sigma_i(X)^{q-1}}{||X||_{S_q}^{q-1}}$$ which is indepdent of $W$. Hence, $$T^* = \\text{dualizer}_{||\\cdot||_{S_p}}(X) = U \\frac{\\text{diag}\\left(\\sigma_1(X)^{q-1}, \\ldots, \\sigma_{\\min(m,n)}(X)^{q-1}\\right)}{||X||_{S_q}^{q-1}} V^T\\quad\\blacksquare$$\n5.2. Stochastic Gradient Descent and Muon as Special Cases 4.2.1. Recovering SGD. Let $p = 2$, and so we have $q = 2$ and $||\\cdot||_{S_2} = ||\\cdot||_F$. Thus for $\\nabla \\mathcal{L}(W)_\\xi \\in T_W\\mathcal{W}$ at $W \\in \\mathcal{W}$, we have, $$ \\begin{align*} \\Delta W \u0026= \\text{dualizer}_{||\\cdot||_{S_\\infty}}(\\nabla \\mathcal{L}(W)_\\xi; W)\\\\ \u0026= U \\frac{\\text{diag}\\left(\\sigma_1(\\nabla \\mathcal{L}(W)_\\xi)^{2-1}, \\ldots, \\sigma_{\\min(m,n)}(\\nabla \\mathcal{L}(W)_\\xi)^{2-1}\\right)}{||\\nabla \\mathcal{L}(W)_\\xi||_{S_2}^{2-1}} V^T\\\\ \\Delta W \u0026= \\frac{\\nabla \\mathcal{L}(W)_\\xi}{||\\nabla \\mathcal{L}(W)_\\xi||_F} \\end{align*} $$ which matches the update rule we expect from SGD.\n4.2.2. Recovering Muon. Let $p = \\infty$, and so we have $q = 1$ and $||\\cdot||_{S_\\infty} = ||\\cdot||_{2\\to 2}$. Thus for $\\nabla \\mathcal{L}(W)_\\xi \\in T_W\\mathcal{W}$ at $W \\in \\mathcal{W}$, we have, $$ \\begin{align*} \\Delta W \u0026= \\text{dualizer}_{||\\cdot||_{2 \\to 2}}(\\nabla \\mathcal{L}(W)_\\xi; W)\\\\ \u0026= U \\frac{\\text{diag}\\left(\\sigma_1(\\nabla \\mathcal{L}(W)_\\xi)^{1-1}, \\ldots, \\sigma_{\\min(m,n)}(X)^{1-1}\\right)}{||X||_{S_1}^{1-1}} V^T\\\\ \\Delta W \u0026= UV^T \\end{align*} $$ which matches the update rule we expect from Muon.\n5.3. Why Muon Still Works Well in Practice Despite the Approximate Orthogonalization We observe that, qualitatively, steepest descent under the Schatten-$p$ norm very quickly converges to steepest descent under the spectral norm as $p$ approaches $\\infty$. This is probably why optimizers like Sketching and Muon work well in practice despite not perfectly orthogonalizing the gradients.\nTo support this, we show that the (1) variance of dualized singular values, and the (2) relative size, and (3) stable rank of the dualized gradients under the Schatten-$p$ norm quadratically converges to those of the Spectral norm as $p$ approaches $\\infty$. And in fact, at $p = 32$, the results are already very close to those of the Spectral norm.\n5.3.1. On the variance of dualized singular values\nProposition 5. The variance of the dualized singular values under the Schatten-$p$ Norm converges quadratically to $0$ as $p$ approaches $\\infty$.\nProof: Let $t_i$ be the $i$-th dualized singular value. From Proposition 4 earlier, we have $$ \\begin{align*} t_i \u0026= \\left(\\frac{\\sigma_i(\\nabla L(W_l))}{||\\nabla L(W_l)||_{S_q}}\\right)^{q-1}\\\\ t_i \u0026= \\exp\\left((q-1)\\ln\\frac{\\sigma_i(\\nabla L(W_l))}{||\\nabla L(W_l)||_{S_q}}\\right)\\\\ t_i \u0026\\approx 1 + (q-1)\\ln\\frac{\\sigma_i(\\nabla L(W_l))}{||\\nabla L(W_l)||_{S_q}} \\end{align*} $$ where the last line follows from first-order Taylor approximation of $t_i$. Thus, the mean and variance are: $$ \\begin{align*} \\mathbb{E}[t_i] \u0026\\approx 1 + (q-1)\\mathbb{E}\\left[\\ln\\frac{\\sigma_i(\\nabla L(W_l))}{||\\nabla L(W_l)||_{S_q}}\\right]\\\\ \\mathbb{E}[t_i] \u0026\\approx 1 + (q-1)\\ln\\frac{\\mathbb{E}[\\sigma_i(\\nabla L(W_l))]}{||\\nabla L(W_l)||_{S_q}}\\\\ t_i - \\mathbb{E}[t_i] \u0026\\approx (q-1)\\ln\\left[\\sigma_i(\\nabla L(W_l)) - \\mathbb{E}[\\sigma_i(\\nabla L(W_l))]\\right]\\\\ Var[t_i] \u0026\\approx (q-1)^2\\mathbb{E}\\left[\\ln^2\\left[\\sigma_i(\\nabla L(W_l)) - \\mathbb{E}[\\sigma_i(\\nabla L(W_l))]\\right]\\right]\\\\ Var[t_i] \u0026\\approx \\frac{1}{(p-1)^2}\\mathbb{E}\\left[\\ln^2\\left[\\sigma_i(\\nabla L(W_l)) - \\mathbb{E}[\\sigma_i(\\nabla L(W_l))]\\right]\\right] \\end{align*} $$ Hence, the variance of the dualized singular values converges quadratically to $0$ as $p$ approaches $\\infty$.\nEmpirically, we can see this in the following plot. And at $p = 32$, the variance of the resulting singular values are already close to $0$. 5.3.2. On the relative size and stable rank of gradients\nDefinition 6: Relative Size of a Gradient. Given a norm $||\\cdot||$ chosen a priori, the relative size of a gradient-update $\\Delta W$ relative to the parameter matrix $W$ is defined as: $$\\text{relsize}(\\Delta W) = \\frac{||\\Delta W||}{||W||}$$\nDefinition 7: Stable Rank. The stable rank of a matrix $A$ is defined as $$srank(A) = \\frac{||A||_F^2}{||A||_{2 \\to 2}^2}$$\nAs we can see in the following plot, the raw gradients have very low-stable rank. But the stable rank of the gradients post-dualization under the Schatten-$p$ norm converges very quickly to that of the Spectral norm as $p$ approaches $\\infty$.\nOne can interpret this as, for some large enough $p$, the dualized gradient is already very close to being “maximal” in a sense. And increasing $p$ further would only offer rapidly diminishing returns.\nA side-effect of this is that it allows the model parameters to “escape” the small region around the initialization and explore the parameter space more effectively, contrary to prior work on SGD and Adam optimizers (Lee et al., 2020; Jesus et al., 2021). This phenomenon is known as weight erasure (Bernstein, 2024).\n6. Optimizing Muon’s Newton-Schulz Coefficients [Under Construction] 7. Convergence Guarantees [Under Construction] How to Cite @misc{cesista2025sdnr, author = {Franz Louis Cesista}, title = {Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds}, year = {2025}, url = {http://leloykun.github.io/ponder/steepest-descent-non-riemannian/}, } References Keller Jordan and Yuchen Jin and Vlado Boza and You Jiacheng and Franz Cesista and Laker Newhouse and Jeremy Bernstein (2024). Muon: An optimizer for hidden layers in neural networks. URL https://kellerjordan.github.io/posts/muon/ Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein (2024). Muon: An optimizer for hidden layers in neural networks. Available at: https://kellerjordan.github.io/posts/muon/. Moonshot AI Team (2025). Muon is Scalable for LLM Training. URL https://arxiv.org/abs/2502.16982 Jeremy Bernstein and Laker Newhouse. “Old optimizer, new norm: An anthology.” arXiv preprint arXiv:2409.20325 (2024). Jeremy Bernstein, Laker Newhouse (2024). Modular Duality in Deep Learning. URL https://arxiv.org/abs/2410.21265 Jeremy Bernstein (2024). “Weight erasure.” Available at: https://docs.modula.systems/examples/weight-erasure/ Xi-Lin Li (2015). Preconditioned Stochastic Gradient Descent. URL https://arxiv.org/abs/1512.04202 Xi-Lin Li (2018). Preconditioner on Matrix Lie Group for SGD. URL https://arxiv.org/abs/1809.10232 Omead Pooladzandi, Xi-Lin Li (2024). Curvature-Informed SGD via General Purpose Lie-Group Preconditioners. URL https://arxiv.org/abs/2402.04553 David E Carlson, Edo Collins, Ya-Ping Hsieh, Lawrence Carin, Volkan Cevher. Preconditioned Spectral Descent for Deep Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), 2015. URL https://proceedings.neurips.cc/paper_files/paper/2015/hash/f50a6c02a3fc5a3a5d4d9391f05f3efc-Abstract.html Thomas Flynn. The duality structure gradient descent algorithm: Analysis and applications to neural networks. arXiv:1708.00523, 2017. URL https://arxiv.org/abs/1708.00523 Hunter, D. R. and Lange, K. (2004). A tutorial on MM algorithms. The American Statistician, 58(1):30–37. Lucas Prieto, Melih Barsbey, Pedro A.M. Mediano, Tolga Birdal (2025). Grokking at the Edge of Numerical Stability. URL https://arxiv.org/abs/2501.04697 Vineet Gupta, Tomer Koren, Yoram Singer (2018). Shampoo: Preconditioned Stochastic Tensor Optimization. URL https://arxiv.org/abs/1802.09568 Rohan Anil et al. “Scalable second order optimization for deep learning.” arXiv preprint arXiv:2002.09018 (2020). Surya, S., Duvvuri, Devvrit, F., Anil, R., Hsieh, C., \u0026 Dhillon, I.S. (2024). Combining Axes Preconditioners through Kronecker Approximation for Deep Learning. International Conference on Learning Representations. Franz Louis Cesista (2025). {CASPR} Without Accumulation is {M}uon. URL https://leloykun.github.io/ponder/caspr-wo-accum-is-muon/ Rohan Anil. “Just some fun linear algebra.” X post, 6 Oct. 2024, Available at: https://x.com/_arohan_/status/1843050297985466565. Dmitry Kovalev (2025). Understanding Gradient Orthogonalization for Deep Learning via Non-Euclidean Trust-Region Optimization. Available at: https://arxiv.org/abs/2503.12645 Lee, Jaehoon, et al. “Wide Neural Networks of Any Depth Evolve as Linear Models under Gradient Descent.” Journal of Statistical Mechanics: Theory and Experiment, vol. 2020, no. 12, Dec. 2020, p. 124002. Crossref, https://doi.org/10.1088/1742-5468/abc62b. Jesus, Ricardo J., et al. “Effect of Initial Configuration of Weights on Training and Function of Artificial Neural Networks.” Mathematics, vol. 9, no. 18, Sept. 2021, p. 2246. Crossref, https://doi.org/10.3390/math9182246. ",
  "wordCount" : "4779",
  "inLanguage": "en",
  "image":"https://leloykun.github.io/ponder/steepest-descent-non-riemannian/cover.png","datePublished": "2025-03-31T00:00:00Z",
  "dateModified": "2025-03-31T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Franz Louis Cesista"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://leloykun.github.io/ponder/steepest-descent-non-riemannian/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Franz Louis Cesista",
    "logo": {
      "@type": "ImageObject",
      "url": "https://leloykun.github.io/favicon.ico"
    }
  }
}
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-BWCGBRX8G1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BWCGBRX8G1');
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://leloykun.github.io/" accesskey="h" title="Franz Louis Cesista">
                <img src="https://leloykun.github.io/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Franz Louis Cesista</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://leloykun.github.io/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/personal-projects/" title="Personal Projects">
                    <span>Personal Projects</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/ponder/" title="Ponder (Posts)">
                    <span>Ponder (Posts)</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds
    </h1>
    <div class="post-meta"><span title='2025-03-31 00:00:00 +0000 UTC'>March 31, 2025</span>&nbsp;&middot;&nbsp;Franz Louis Cesista&nbsp;&middot;&nbsp;<a href="https://x.com/leloykun/status/1907211629982556320" rel="noopener noreferrer" target="_blank">Crossposted from X (formerly Twitter)</a>

</div>
  </header> 
<figure class="entry-cover">
            <img loading="eager"
                srcset='https://leloykun.github.io/ponder/steepest-descent-non-riemannian/cover_hu_c44e316a75385d.png 360w,https://leloykun.github.io/ponder/steepest-descent-non-riemannian/cover_hu_eb77121be6e23620.png 480w,https://leloykun.github.io/ponder/steepest-descent-non-riemannian/cover_hu_bf45ad3619200271.png 720w,https://leloykun.github.io/ponder/steepest-descent-non-riemannian/cover_hu_3a6ce5ab175e3d0e.png 1080w,https://leloykun.github.io/ponder/steepest-descent-non-riemannian/cover_hu_f6d6a70b5c418575.png 1500w,https://leloykun.github.io/ponder/steepest-descent-non-riemannian/cover.png 1536w'
                src="https://leloykun.github.io/ponder/steepest-descent-non-riemannian/cover.png"
                sizes="(min-width: 768px) 720px, 100vw"
                width="1536" height="1024"
                alt="Cover">
        
</figure><div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#1-preliminaries">1. Preliminaries</a></li>
    <li><a href="#2-why-do-steepest-descent-under-the-spectral-norm">2. Why do Steepest Descent Under the Spectral Norm?</a>
      <ul>
        <li><a href="#21-majorization-minimization-perspective">2.1. Majorization-Minimization Perspective</a></li>
        <li><a href="#22-feature-learning-perspective">2.2 Feature Learning Perspective</a></li>
        <li><a href="#23-input-tensor-alignment-phenomenon-under-construction">2.3 Input-Tensor Alignment Phenomenon [Under Construction]</a></li>
      </ul>
    </li>
    <li><a href="#3-steepest-descent-in-riemannian-and-non-riemannian-manifolds">3. Steepest Descent in Riemannian and Non-Riemannian Manifolds</a>
      <ul>
        <li><a href="#31-bmmathcalw-is-euclidean">3.1. $\bm{\mathcal{W}}$ is Euclidean</a></li>
        <li><a href="#32-bmmathcalw-is-a-riemannian-manifold">3.2. $\bm{\mathcal{W}}$ is a Riemannian Manifold</a></li>
        <li><a href="#33-bmmathcalw-is-a-non-riemannian-manifold">3.3. $\bm{\mathcal{W}}$ is a Non-Riemannian Manifold</a></li>
      </ul>
    </li>
    <li><a href="#4-muon-as-steepest-descent-in-a-non-riemannian-manifold">4. Muon as Steepest Descent in a Non-Riemannian Manifold</a>
      <ul>
        <li><a href="#41-the-muon-optimizer">4.1. The Muon Optimizer</a></li>
        <li><a href="#42-approximating-dualization-with-preconditioners">4.2 Approximating Dualization with Preconditioners</a></li>
      </ul>
    </li>
    <li><a href="#5-steepest-descent-in-non-riemannian-manifolds-induced-by-schatten-p-norms">5. Steepest Descent in Non-Riemannian Manifolds Induced by Schatten-$p$ Norms</a>
      <ul>
        <li><a href="#51-dualizers-for-schatten-p-norms">5.1. Dualizers for Schatten-$p$ Norms</a></li>
        <li><a href="#52-stochastic-gradient-descent-and-muon-as-special-cases">5.2. Stochastic Gradient Descent and Muon as Special Cases</a></li>
        <li><a href="#53-why-muon-still-works-well-in-practice-despite-the-approximate-orthogonalization">5.3. Why Muon Still Works Well in Practice Despite the Approximate Orthogonalization</a></li>
      </ul>
    </li>
    <li><a href="#6-optimizing-muons-newton-schulz-coefficients-under-construction">6. Optimizing Muon&rsquo;s Newton-Schulz Coefficients [Under Construction]</a></li>
    <li><a href="#7-convergence-guarantees-under-construction">7. Convergence Guarantees [Under Construction]</a></li>
    <li><a href="#how-to-cite">How to Cite</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><p>A new optimizer called Muon has recently been shown to outperform Adam in both small-scale language model training (Jordan et al., 2024), and larger-scale language model training (Moonshot AI Team, 2025) by a factor of 2x in terms of flops efficiency. For non-matrix-valued parameters in a neural network, Muon falls back to Adam. But for matrix-valued parameters, Muon first semi-orthogonalizes the gradient before subtracting it from the parameter. It can also be viewed as steepest descent under the Spectral norm (Bernstein et al., 2024).</p>
<p>Here, we will derive Muon&rsquo;s update rule for matrix-valued parameters and discuss what makes it different from other optimizers and <em>why</em> it works so well.</p>
<h2 id="1-preliminaries">1. Preliminaries<a hidden class="anchor" aria-hidden="true" href="#1-preliminaries">#</a></h2>
<p>We consider the following optimization problem:
$$\begin{equation} \min_{W \in \bm{\mathcal{W}}} \mathcal{L}(W) \end{equation}$$
where $\mathcal{L}(\cdot): \bm{\mathcal{W}} \rightarrow \mathbb{R}$ is a bounded-below and differentiable loss function, and $\bm{\mathcal{W}}$ is a matrix-valued vector space equipped with a norm $||\cdot||$ chosen a priori. If the norm admits a metric, then $\bm{\mathcal{W}}$ is a Riemannian manifold. Otherwise, it is a non-Riemannian (Finsler) manifold. Thus, not only does the choice of norm naturally lead to different optimization algorithms, but also to two <em>classes</em> of optimizers, preconditioners and dualizers, which we will discuss in the following sections.</p>
<p>In practice, $\mathcal{L}$ often does not have a simple, closed-form solution, so we resort to iterative methods of the form
$$W_{t+1} = W_{t} - \lambda \widehat{\Delta W}_t,$$ where $\lambda &gt; 0$ is a positive learning rate parameter and $-\widehat{\Delta W}_t$ is the direction of steepest descent at step $t$,
$$
\begin{align}
-\widehat{\Delta W}_t &amp;= \arg\min_{\substack{\Delta W \in T_{W_t}\mathcal{W}\\ ||\Delta W|| = 1}} d\mathcal{L}_{W_t}(\Delta W)\nonumber\\
\widehat{\Delta W}_t &amp;= \arg\max_{\substack{\Delta W \in T_{W_t}\mathcal{W}\\ ||\Delta W|| = 1}} d\mathcal{L}_{W_t}(\Delta W)
\end{align}
$$
where $T_{W_t}\mathcal{W}$ is the tangent space at $W_t$, $d\mathcal{L}_{W_t}(\cdot): T_{W_t}\mathcal{W} \rightarrow \mathbb{R}$ is the differential of $\mathcal{L}$ at $W_t$, and $d\mathcal{L}_{W_t}(\Delta W)$ is the directional derivative of $\mathcal{L}$ at $W_t$ in the direction of $\Delta W$.</p>
<p>We also often do not have access to the exact differential. However, either through, e.g., backpropagation if downstream operations are differentiable or evolutionary algorithms otherwise, we often do have access to a stochastic estimator $g_W(\cdot, \xi)_{\text{coord}}$ of the differential in standard Euclidean coordinates,</p>
<blockquote>
<p><strong>Assumption 1:</strong> Suppose that, for all $W \in \bm{\mathcal{W}}$, the differential $d\mathcal{L}_W(\cdot)$ has a standard Euclidean coordinate representation $\nabla \mathcal{L}(W)_{\text{coord}}$ such that $d\mathcal{L}_W(\cdot) = \langle \nabla \mathcal{L}(W)_{\text{coord}}, \cdot \rangle_F$. We assume that we have access to a stochastic estimator $g_W(\cdot, \xi)_{\text{coord}} = \langle \nabla \mathcal{L}(W)_\xi, \cdot \rangle_F$ of the differential in coordinate form that is unbiased and has bounded variance. That is,
$$
\begin{align*}
&amp;\mathbb{E}_{\xi \sim D}[\nabla \mathcal{L}(W)_\xi] = \nabla \mathcal{L}(W)_{\text{coord}} &amp;&amp; \forall W \in \bm{\mathcal{W}}\\
&amp;\mathbb{E}_{\xi \sim D}[||\nabla \mathcal{L}(W)_\xi - \nabla \mathcal{L}(W)_{\text{coord}} ||_F^2] \leq \sigma^2 &amp;&amp; \forall W \in \bm{\mathcal{W}}
\end{align*}
$$
where $\xi$ is a random variable sampled from a  distribution $D$, $\sigma &gt; 0$ is a positive variance parameter, $\langle \cdot, \cdot \rangle_F$ is the Frobenius inner product, and $||\cdot||_F = \sqrt{\langle \cdot, \cdot \rangle_F}$.</p></blockquote>
<p>We also make the following standard continuity assumption on the gradient $\nabla \mathcal{L}(\cdot)$,</p>
<blockquote>
<p><strong>Assumption 2:</strong> The gradient $\nabla \mathcal{L}(\cdot)$ is Lipschitz continuous with respect to the norm $||\cdot||$ with gradient Lipschitz constant $L &gt; 0$. That is, for all $W, Z \in \bm{\mathcal{W}}$,
$$
\begin{equation}
||\nabla \mathcal{L}(W) - \nabla \mathcal{L}(Z)||^\dagger \leq L||W - Z|| \quad \forall W, Z \in \bm{\mathcal{W}}
\end{equation}
$$
where $||\cdot||^\dagger$ is the dual norm of $||\cdot||$.</p></blockquote>
<p>And in the following sections, we will also discuss optimizers that precondition the gradients,</p>
<blockquote>
<p><strong>Definition 1 (Preconditioning).</strong> In an optimization algorithm, a preconditioner $\mathcal{P}(\cdot; W): T_W\mathcal{W} \rightarrow T_W\mathcal{W}$ is a point-dependent linear transform that maps empirical gradients $\nabla \mathcal{L}(W)_\xi$ to update directions $\Delta W$. That is, at any $W \in \mathcal{W}$, we have a matrix $P_W$ such that $\mathcal{P}(\nabla \mathcal{L}(W)_\xi; W) = P_W \nabla \mathcal{L}(W)_\xi$ and,
$$
\begin{align*}
\Delta W_t &amp;= P_{W_t} \nabla \mathcal{L}(W_t)\\
W_{t+1} &amp;= W_t - \lambda P_{W_t} \nabla \mathcal{L}(W_t)
\end{align*}
$$
It is also common to assume that we can decompose $P_W$ into a Kronecker product $P_W = L_W \otimes R_W$ (Li, 2015; Gupta et al., 2018, Surya et al., 2024), such that our update rule becomes
$$
\begin{align*}
\Delta W_t &amp;= P_{W_t} \nabla \mathcal{L}(W_t)\\
\Delta W_t &amp;= L_{W_t} \nabla \mathcal{L}(W_t) R_{W_t}\\
W_{t+1} &amp;= W_t - \lambda L_{W_t} \nabla \mathcal{L}(W_t) R_{W_t}
\end{align*}
$$
We call $L_W$ and $R_W$ as the left and right preconditioners, respectively.</p></blockquote>
<h2 id="2-why-do-steepest-descent-under-the-spectral-norm">2. Why do Steepest Descent Under the Spectral Norm?<a hidden class="anchor" aria-hidden="true" href="#2-why-do-steepest-descent-under-the-spectral-norm">#</a></h2>
<p>The geometry of $\mathcal{W}$ and the optimizer we will need both depend on the choice of norm $||\cdot||$. Our core argument is that it is most natural to do steepest descent under the spectral norm $||\cdot||_{2 \to 2}$ in the context of training the linear weights $W$ of a neural network. The spectral norm induces $\mathcal{W}$ to be non-Riemannian, and therefore, intuitions on optimization we have developed in Riemannian manifolds may not apply.</p>
<h3 id="21-majorization-minimization-perspective">2.1. Majorization-Minimization Perspective<a hidden class="anchor" aria-hidden="true" href="#21-majorization-minimization-perspective">#</a></h3>
<p>We can upper bound our objective function $\mathcal{L}$ by the following approximation at an arbitrary point $Z \in \bm{\mathcal{W}}$,
$$
\begin{equation}
\mathcal{U}(W; Z) = \mathcal{L}(Z) + \langle \nabla \mathcal{L}(Z)_\xi, W - Z \rangle_F + \frac{\lambda}{2}||W - Z||^2
\end{equation}
$$
for some norm $||\cdot||$. Using standard arguments, we can show that $\mathcal{L}(W) \leq \mathcal{U}(W; Z)$ for all $W \in \bm{\mathcal{W}}$ as long as $\lambda \leq L$ (Hunter et al., 2004). Minimizing this upper bound is equivalent to minimizing the original objective function; the tighter the bound, the better. And as discussed by Carlson et al. (2015), the spectral norm gives us a tight upper bound and is thus a good choice. In fact, the spectral norm gives the tightest bound among all the Schatten-$p$ norms.</p>
<h3 id="22-feature-learning-perspective">2.2 Feature Learning Perspective<a hidden class="anchor" aria-hidden="true" href="#22-feature-learning-perspective">#</a></h3>
<p>Suppose that we have a linear transform $x_{l+1} = W_{l} x_{l}$ at the $l$-th layer of a neural network where $x_l \in \mathcal{R}^{d_l}$ and $x_{l+1} \in \mathcal{R}^{d_{l+1}}$ are the input and output hidden representations (or &ldquo;features&rdquo;), respectively, and $W_l \in \mathcal{R}^{d_{l+1} \times d_l}$ is the weight matrix. Additionally, let $\Delta x_l \in \mathcal{R}^{d_l}$, $\Delta x_{l+1} \in \mathcal{R}^{d_{l+1}}$, and $\Delta W_l \in \mathcal{R}^{d_{l+1} \times d_l}$ be their updates after a backward pass.</p>
<p>Ideally, we want the sizes of both the hidden representations $x_l$ and their updates $\Delta x_l$ to scale with the model width $d_l$. Otherwise, if the hidden representations are &rsquo;too small&rsquo;, we are wasting capacity, in a sense; and if they are &rsquo;too large&rsquo;, we are pushing the model towards the edge of numerical stability and prevent grokking (Prieto et al., 2025). Likewise, if the updates are &rsquo;too small&rsquo;, they vanish at larger scales, slowing down convergence; and if they are &rsquo;too large&rsquo;, they cause training instability. Yang et al. (2024) summarizes this as follows,</p>
<blockquote>
<p><strong>Desideratum 1 (Feature Learning).</strong> We desire that our features $x_l$ and feature updates $\Delta x_l$ be of size,
$$
\begin{equation}
||x_l||_2 = \Theta(\sqrt{d_l})\quad\text{and}\quad ||\Delta x_l||_2 = \Theta(\sqrt{d_l})\quad\text{for all layers } l = 1, 2, \ldots, L-1
\end{equation}
$$</p></blockquote>
<p>We ensure this by imposing constraints on the size of the weights $W_l$ and their updates $\Delta W_l$:</p>
<ol>
<li>
<p>From the definition of the spectral norm, we have,
$$
\begin{align*}
x_{l+1} &amp;= W_l x_l\\
||x_{l+1}||_2 &amp;\leq ||W_l||_{2\to 2} \cdot ||x_l||_2
\end{align*}
$$
Combining this with Desideratum 1, we have,
$$
\begin{align*}
\underbrace{||x_{l+1}||_2}_{\Theta(\sqrt{d_{l+1}})}
&amp;\leq ||W_l||_{2\to 2} \cdot \underbrace{||x_l||_2}_{\Theta(\sqrt{d_l})}
\end{align*}
$$
Thus the size of the weights $W_l$ must be,
$$
\begin{equation}
||W_l||_{2 \to 2} = \Theta\left(\sqrt{\frac{d_{l+1}}{d_l}}\right)
\end{equation}
$$</p>
</li>
<li>
<p>Now let&rsquo;s consider the feature updates $\Delta x_l$,
$$
\begin{align*}
x_{l+1} + \Delta x_{l+1} &amp;= (W_l + \Delta W_l)(x_l + \Delta x_l)\\
\Delta x_{l+1} &amp;= W_l \Delta x_l + \Delta W_l x_l + \Delta W_l \Delta x_l\\
||\Delta x_{l+1}||_2 &amp;\leq ||W_l||_{2\to 2} \cdot ||\Delta x_l||_2 + ||\Delta W_l||_{2\to 2} \cdot ||x_l||_2 + ||\Delta W_l||_{2\to 2} \cdot ||\Delta x_l||_2
\end{align*}
$$
Combining this with Desideratum 1 and our result above, we have,
$$
\begin{align*}
\underbrace{||\Delta x_{l+1}||_2}_{\Theta(\sqrt{d_{l+1}})}
&amp;\leq
\underbrace{||W_l||_{2\to 2}}_{\Theta\left(\sqrt{\frac{d_{l+1}}{d_l}}\right)} \cdot \underbrace{||\Delta x_l||_2}_{\Theta\left(\sqrt{d_l}\right)}
+ ||\Delta W_l||_{2\to 2} \cdot \underbrace{||x_l||_2}_{\Theta\left(\sqrt{d_l}\right)}
+ ||\Delta W_l||_{2\to 2} \cdot \underbrace{||\Delta x_l||_2}_{\Theta\left(\sqrt{d_l}\right)}
\end{align*}
$$
Thus the size of the weight updates $\Delta W_l$ must be,
$$||\Delta W_l||_{2 \to 2} = \Theta\left(\sqrt{\frac{d_{l+1}}{d_l}}\right)$$</p>
</li>
</ol>
<p>These become our <em>Spectral Scaling Conditions</em> (Yang et al., 2024),</p>
<blockquote>
<p><strong>Condition 1 (Spectral Scaling).</strong> The spectral norms of our weights $W_l$ and weight updates $\Delta W_l$ must be,
$$
\begin{equation}
||W_l||_{2\to 2} = \Theta\left(\sqrt{\frac{d_{l+1}}{d_l}}\right)\quad\text{and}\quad||\Delta W_l||_{2\to 2} = \Theta\left(\sqrt{\frac{d_{l+1}}{d_l}}\right)\quad\text{at layers } l = 1, \ldots, L-1
\end{equation}
$$</p></blockquote>
<h3 id="23-input-tensor-alignment-phenomenon-under-construction">2.3 Input-Tensor Alignment Phenomenon [Under Construction]<a hidden class="anchor" aria-hidden="true" href="#23-input-tensor-alignment-phenomenon-under-construction">#</a></h3>
<h2 id="3-steepest-descent-in-riemannian-and-non-riemannian-manifolds">3. Steepest Descent in Riemannian and Non-Riemannian Manifolds<a hidden class="anchor" aria-hidden="true" href="#3-steepest-descent-in-riemannian-and-non-riemannian-manifolds">#</a></h2>
<p>Let us consider the different cases of the geometry of $\bm{\mathcal{W}}$ induced by the choice of norm $||\cdot||$.</p>
<h3 id="31-bmmathcalw-is-euclidean">3.1. $\bm{\mathcal{W}}$ is Euclidean<a hidden class="anchor" aria-hidden="true" href="#31-bmmathcalw-is-euclidean">#</a></h3>
<p>That is, we pick the Frobenius norm $||\cdot||_F$ as our norm. In this case, our points, differentials, and gradients are all already in standard Euclidean coordinates and we have a canonical bijection between differentials $d\mathcal{L}_W(\cdot) \in T_W^* \bm{\mathcal{W}}$ and gradients $\nabla \mathcal{L}(W) \in T_W \bm{\mathcal{W}}$ such that $$d\mathcal{L}_W(\cdot) = \langle \nabla \mathcal{L}(W), \cdot \rangle_F$$
Thus,</p>
<p>$$
\begin{align*}
\widehat{\Delta W} &amp;= \arg\max_{\substack{\Delta W \in T_W\mathcal{W}\\ ||\Delta W|| = 1}} d\mathcal{L}_W(\Delta W)\\
&amp;= \arg\max_{\substack{\Delta W \in T_W\mathcal{W}\\ ||\Delta W|| = 1}} \langle \nabla \mathcal{L}(W), \Delta W \rangle_F\\
&amp;= \arg\max_{\substack{\Delta W \in T_W\mathcal{W}\\ ||\Delta W|| = 1}} \langle \nabla \mathcal{L}(W)_{\text{coord}}, \Delta W \rangle_F\\
&amp;= \frac{\nabla \mathcal{L}(W)_{\text{coord}}}{||\nabla \mathcal{L}(W)_{\text{coord}}||_F}\\
\widehat{\Delta W} &amp;\approx \frac{\nabla \mathcal{L}(W)_\xi}{||\nabla \mathcal{L}(W)_\xi||_F}\\
\end{align*}
$$</p>
<p>Thus, our update rule becomes,
$$W_{t+1} = W_t - \hat{\lambda} \nabla \mathcal{L}(W)_\xi$$
where $\hat{\lambda} = \frac{\lambda}{||\nabla \mathcal{L}(W)_\xi||_F}$. This is simply Stochastic Gradient Descent (SGD) with an adaptive learning rate.</p>
<h3 id="32-bmmathcalw-is-a-riemannian-manifold">3.2. $\bm{\mathcal{W}}$ is a Riemannian Manifold<a hidden class="anchor" aria-hidden="true" href="#32-bmmathcalw-is-a-riemannian-manifold">#</a></h3>
<p>That is, our choice of norm $||\cdot||$ admits a metric $g_W(\cdot, \cdot): T_W \bm{\mathcal{W}} \times T_W \bm{\mathcal{W}} \rightarrow \mathbb{R}$ for each $W \in \bm{\mathcal{W}}$ such that
$$||U|| = \sqrt{g_W(U, U)}\quad\text{and}\quad g_W(U, V) = \langle U, V \rangle_{G_W} = \langle GU, V \rangle_F.\quad \forall U,V \in \mathcal{W}$$
for some positive-definite matrix $G_W$ for each $W \in \bm{\mathcal{W}}$. Case 1 above is a special case of this where $G_W = I$ for all $W \in \bm{\mathcal{W}}$ and thus, $||U||_F = \sqrt{g_W(U, U)} = \sqrt{\langle U, U \rangle_F} \forall U \in \mathcal{W}$.</p>
<p>An interesting property of Riemannian manifolds is that we have a canonical bijection between differentials $d\mathcal{L}_W(\cdot) \in T_W^* \bm{\mathcal{W}}$ and gradients $\nabla \mathcal{L}(W) \in T_W \bm{\mathcal{W}}$ such that $$d\mathcal{L}_W(\cdot) = \langle \nabla \mathcal{L}(W), \cdot \rangle$$</p>
<p>Now notice that,
$$
\begin{align*}
d\mathcal{L}_W(\cdot) &amp;= \langle \nabla \mathcal{L}(W), \cdot \rangle\\
d\mathcal{L}_W(\cdot) &amp;= \langle G_W\nabla \mathcal{L}(W), \cdot \rangle_F\\
G_W\nabla \mathcal{L}(W) &amp;= \nabla \mathcal{L}(W)_{\text{coord}}\\
\nabla \mathcal{L}(W) &amp;= G_W^{-1} \nabla \mathcal{L}(W)_{\text{coord}}\\
\end{align*}
$$</p>
<p>Thus,</p>
<p>$$
\begin{align}
\widehat{\Delta W} &amp;= \arg\max_{\substack{\Delta W \in T_W\mathcal{W}\\ ||\Delta W|| = 1}} d\mathcal{L}_W(\Delta W)\nonumber\\
&amp;= \arg\max_{\substack{\Delta W \in T_W\mathcal{W}\\ ||\Delta W|| = 1}} \langle \nabla \mathcal{L}(W), \Delta W \rangle\nonumber\\
&amp;= \arg\max_{\substack{\Delta W \in T_W\mathcal{W}\\ ||\Delta W|| = 1}} \langle G_W^{-1} \nabla \mathcal{L}(W)_{\text{coord}}, \Delta W \rangle\\
&amp;= \frac{G_W^{-1} \nabla \mathcal{L}(W)_{\text{coord}}}{||G_W^{-1}\nabla \mathcal{L}(W)_{\text{coord}}||}\nonumber\\
\widehat{\Delta W} &amp;\approx \frac{G_W^{-1}\nabla \mathcal{L}(W)_\xi}{||G_W^{-1}\nabla \mathcal{L}(W)_\xi||}\nonumber\\
\end{align}
$$
where the maximum above can be achieved by aligning $\Delta W$ with $G_W^{-1}\nabla \mathcal{L}(W)_{\text{coord}}$. Thus our update rule becomes,
$$W_{t+1} = W_t - \hat{\lambda} G_{W_t}^{-1}\nabla \mathcal{L}(W_t)_\xi$$
where $\hat{\lambda} = \frac{\lambda}{||G_{W_t}^{-1}\nabla \mathcal{L}(W_t)_\xi||}$. This is Riemannian Stochastic Gradient Descent (RSGD) with an adaptive learning rate. And if we let $P_W = G_W^{-1}$ be the preconditioner at point $W$, we can relate this to Preconditioned Stochastic Gradient Descent (PSGD) algorithms (Li, 2015; Pooladzandi et al., 2024).</p>
<h3 id="33-bmmathcalw-is-a-non-riemannian-manifold">3.3. $\bm{\mathcal{W}}$ is a Non-Riemannian Manifold<a hidden class="anchor" aria-hidden="true" href="#33-bmmathcalw-is-a-non-riemannian-manifold">#</a></h3>
<p>In this case, our choice of norm $||\cdot||$ does not admit a well-behaved metric $g_W(\cdot, \cdot)$ and consequently also does not admit a well-behaved inner product $\langle \cdot, \cdot \rangle$ such that $||\cdot|| = \sqrt{\langle \cdot, \cdot \rangle}$ for all $W \in \mathcal{W}$. Our differentials $d\mathcal{L}_W(\cdot)$ are still well-defined, but we no longer have the bijective relationship between differentials and gradients. And so, we do not always have a unique $\nabla \mathcal{L}(W)$ such that $d\mathcal{L}_W(\cdot) = \langle \nabla \mathcal{L}(W), \cdot \rangle$ if this inner product even exists.</p>
<p>While we still have access to the stochastic estimator of the differential in standard Euclidean coordinates $\nabla \mathcal{L}(W)_{\text{coord}}$ from Assumption 1, it no longer has geometric meaning by itself. That is, a simple change of coordinates no longer tells us information on the direction of steepest descent. We can, however, still use it to define a dualizer that maps the differentials we get empirically to update directions,
$$
\begin{align*}
\widehat{\Delta W}
&amp;= \arg\max_{\substack{\Delta W \in T_W\mathcal{W}\\ ||\Delta W|| = 1}} d\mathcal{L}_W(\Delta W)\\
&amp;= \arg\max_{\substack{\Delta W \in T_W\mathcal{W}\\ ||\Delta W|| = 1}} \langle \nabla \mathcal{L}(W)_{\text{coord}}, \Delta W \rangle_F\\
&amp;\approx \arg\max_{\substack{\Delta W \in T_W\mathcal{W}\\ ||\Delta W|| = 1}} \langle \nabla \mathcal{L}(W)_\xi, \Delta W \rangle_F\\
\widehat{\Delta W} &amp;= \text{dualizer}_{||\cdot||}(\nabla \mathcal{L}(W)_\xi; W)
\end{align*}
$$
where
$$
\begin{equation}
\text{dualizer}_{||\cdot||}(\nabla \mathcal{L}(W)_\xi; W) = \arg\max_{\substack{\Delta W \in T_W\mathcal{W}\\ ||\Delta W|| = 1}} \langle \nabla \mathcal{L}(W)_\xi, \Delta W \rangle_F
\end{equation}
$$
and to simplify our notation, we use $\text{dualizer}_{||\cdot||}(\nabla \mathcal{L}(W)_\xi)$ if this map is independent of $W$.</p>
<h2 id="4-muon-as-steepest-descent-in-a-non-riemannian-manifold">4. Muon as Steepest Descent in a Non-Riemannian Manifold<a hidden class="anchor" aria-hidden="true" href="#4-muon-as-steepest-descent-in-a-non-riemannian-manifold">#</a></h2>
<h3 id="41-the-muon-optimizer">4.1. The Muon Optimizer<a hidden class="anchor" aria-hidden="true" href="#41-the-muon-optimizer">#</a></h3>
<blockquote>
<p><strong>Algorithm 1 (Muon)</strong> by Jordan et al. (2024). The weights are treated independently.</p>
<p><strong>Inputs:</strong> Initial weight $W_0 \in \mathcal{W}$, and momentum term $M_0 \in \mathcal{W}$.</p>
<p><strong>Parameters:</strong> Learning rate $\lambda &gt; 0$, momentum decay $\beta \in [0, 1)$, and number of iterations $T \in \{1, 2, \ldots\}$</p>
<p>$\textbf{for } t = 0, 1, \ldots, T-1 \textbf{ do}\\
\text{&hellip; Compute }G_t = \nabla \mathcal{L}(W)_\xi\\
\text{&hellip; Compute }W_{t+1}\text{ and }M_{t+1}\text{ as follows:}\\
\text{&hellip;&hellip;. }M_{t+1} = \beta M_t + (1 - \beta) G_t\\
\text{&hellip;&hellip;. }O_{t+1} = \text{approx-orth}(M_{t+1})\\
\text{&hellip;&hellip;. }W_{t+1} = W_t - \lambda O_{t+1}
$</p>
<p><strong>Output:</strong> $W_T \in \mathcal{W}$.</p></blockquote>
<blockquote>
<p><strong>Algorithm 2 (Approximate Orthogonalization through Newton-Schulz Iteration)</strong></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">zeropower_via_newtonschulz</span>(G: Tensor, steps: <span style="color:#0aa">int</span>=<span style="color:#099">5</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#00a">assert</span> G.ndim == <span style="color:#099">2</span>
</span></span><span style="display:flex;"><span>    a, b, c = (<span style="color:#099">3.4445</span>, -<span style="color:#099">4.7750</span>, <span style="color:#099">2.0315</span>)
</span></span><span style="display:flex;"><span>    X = G.bfloat16()
</span></span><span style="display:flex;"><span>    X /= (X.norm() + <span style="color:#099">1e-7</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#00a">if</span> G.size(-<span style="color:#099">2</span>) &gt; G.size(-<span style="color:#099">1</span>):
</span></span><span style="display:flex;"><span>        X = X.mT
</span></span><span style="display:flex;"><span>    <span style="color:#00a">for</span> _ <span style="color:#00a">in</span> <span style="color:#0aa">range</span>(steps):
</span></span><span style="display:flex;"><span>        A = X @ X.mT
</span></span><span style="display:flex;"><span>        B = b * A + c * A @ A
</span></span><span style="display:flex;"><span>        X = a * X + B @ X
</span></span><span style="display:flex;"><span>    <span style="color:#00a">if</span> G.size(-<span style="color:#099">2</span>) &gt; G.size(-<span style="color:#099">1</span>):
</span></span><span style="display:flex;"><span>        X = X.mT
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> X
</span></span></code></pre></div></blockquote>
<p>Muon (Algorithm 1) is an optimizer for matrix-valued parameters in neural networks (Jordan et al., 2024). For each weight $W \in \mathcal{W}$, it first accumulates the momentum term, then approximately semi-orthogonalizes the result using the Newton-Schulz iteration (Algorithm 2), before applying it as an update to the weights.</p>
<p>We can fold the momentum term into $\nabla \mathcal{L}(W)_\xi$ as it can be seen as a way to smooth out outlier empirical gradients. In fact, Kovalev (2025) has recently shown that, under Muon&rsquo;s update rule, the momentum term does becomes a tighter approximation of the true gradient $\nabla \mathcal{L}(W)_{\text{coord}}$ as the number of iterations $T$ increases.</p>
<p>And while Muon only approximately (semi-)orthogonalizes the gradient, we have found that it still empirically performs just as well as exact orthogonalization. We will discuss this in more detail in the next sections. Muon is also not the first optimizer that does approximate orthogonalization. For example, Carlson et al.&rsquo;s randomized algorithm Sketching (2015) does this explicitly, and so does Shampoo (Gupta et al., 2018), CASPR (Surya et al., 2024), and PSGD (Li, 2015) implicitly through their preconditioners. However, Muon is the first, non-randomized, preconditioner-free optimizer that explicitly aims to orthogonalize the gradient.</p>
<p>An interesting fact from prior work (Carlson et al., 2015; Flynn, 2017; Bernstein et al., 2024) is that the dualizer for steepest descent under the spectral norm $||\cdot||_{2 \to 2}$ is exactly this orthogonalization process,
$$
\begin{equation}
\text{dualizer}_{||\cdot||_{2\to 2}}(\nabla \mathcal{L}(W)_\xi) = UV^T
\end{equation}
$$
where $U\Sigma V^T$ is the singular value decomposition (SVD) of $\nabla \mathcal{L}(W)_\xi$. The spectral norm does not admit a well-behaved inner product. And so, Muon, and related optimizers, can be thought of as steepest descent in a non-Riemannian manifold.</p>
<p>In the next section, we discuss why Muon can be viewed as an instantaneous version of already existing optimizers such as Shampoo, CASPR, PSGD, and etc. We will also discuss an alternate perspective on how such preconditioning optimizers can be viewed as approximators to the dualizer of the spectral norm.</p>
<h3 id="42-approximating-dualization-with-preconditioners">4.2 Approximating Dualization with Preconditioners<a hidden class="anchor" aria-hidden="true" href="#42-approximating-dualization-with-preconditioners">#</a></h3>
<p>As we have shown in Section 3.2, the dualization process in Riemannian steepest descent always has an equivalent preconditioning process by letting $P_W = G_W^{-1}$ at every point $W \in \bm{\mathcal{W}}$. And likewise, if we have a preconditioning process where every $P_W$ is invertible, then it can be thought of as Riemannian steepest descent under the metric $G_W = P_W^{-1}$.</p>
<p>However, we may not always have an equivalent preconditioning process in non-Riemannian manifolds. And if there is, it may not be unique. Here, let&rsquo;s examine multiple preconditioning processes that approximate the dualizer under the spectral norm in turn.</p>
<p><strong>4.2.1. The matrix sign function.</strong> Muon&rsquo;s update rule is actually equivalent to the matrix sign function,</p>
<p>$$
\begin{align*}
\text{msign}(\nabla \mathcal{L}_\xi(W))
&amp;= (\nabla \mathcal{L}_\xi(W) \nabla \mathcal{L}_\xi(W)^T)^{-1/2} \nabla \mathcal{L}_\xi(W)\\
&amp;= (U\Sigma V^T V \Sigma U^T)^{-1/2} U\Sigma V^T\\
\text{msign}(\nabla \mathcal{L}_\xi(W)) &amp;= U V^T,
\end{align*}
$$
where $U\Sigma V^T$ is the singular value decomposition (SVD) of $\nabla \mathcal{L}_\xi(W)$.</p>
<p>And if we let $P_W = (\nabla \mathcal{L}_\xi(W) \nabla \mathcal{L}_\xi(W)^T)^{-1/2}$, then we arrive at a preconditioner form of Muon&rsquo;s update rule,
$$
\begin{align*}
W_{t+1} &amp;= W_t - \lambda P_W \nabla \mathcal{L}_\xi(W_t)\\
&amp;= W_t - \lambda (\nabla \mathcal{L}_\xi(W_t) \nabla \mathcal{L}_\xi(W_t)^T)^{-1/2} \nabla \mathcal{L}_\xi(W_t)\\
W_{t+1} &amp;= W_t - \lambda UV^T
\end{align*}
$$</p>
<p><strong>4.2.2. Shampoo.</strong> Let $G_t = \nabla \mathcal{L}_\xi(W_t)$. Then Shampoo (Gupta et al., 2018; Anil et al., 2020) has the following update rule,</p>
<p>$$L_t := L_{t-1} + G_t G_t^T, \quad\quad R_t := R_{t-1} + G_t^T G_t$$
$$\Delta W_t = L^{-1/4}_t G_t R^{-1/4}_t$$</p>
<p>As previously noted by Bernstein et al. (2024) and Anil (2024), Shampoo reduces to Muon if we disable the updates on the left and right preconditioners. That is, let $G_t = U\Sigma V^T$ be the singular value decomposition (SVD) of $G_t$ and let
$$
L_t := G_t G_t^T\quad\quad R_t := G_t^T G_t
$$
Then,
$$\begin{aligned}
\Delta W_t &amp;= L^{-1/4}_t G_t R^{-1/4}_t\\
&amp;= (G_t G_t^T)^{-1/4} G_t (G_t^T G_t)^{-1/4}\\
&amp;= (U\Sigma V^T V \Sigma U^T)^{-1/4} U\Sigma V^T (V \Sigma U^T U \Sigma V^T)^{-1/4}\\
&amp;= U\left(\frac{\Sigma}{\sqrt{\Sigma^2}} \right)V^T\\
\Delta W_t &amp;= UV^T
\end{aligned}$$
which is Muon&rsquo;s update rule. $\blacksquare$</p>
<p><strong>4.2.3. CASPR.</strong> Let $G_t = \nabla \mathcal{L}_\xi(W_t)$. Then CASPR (Surya et al., 2024) has the following update rule,</p>
<p>$$L_t := L_{t-1} + G_t G_t^T, \quad\quad R_t := R_{t-1} + G_t^T G_t$$
$$\tilde{L}_t := L_t + \epsilon I_m, \quad\quad \tilde{R}_t := R_t + \epsilon I_n$$
$$\Delta W_t = (\tilde{L}^{-1/2}_t G_t + 2 \tilde{L}^{-1/4}_t G_t \tilde{R}^{-1/4}_t + G_t \tilde{R}^{-1/2}_t)/4.$$</p>
<p>As previously noted by Cesista (2025), CASPR reduces to Muon if we disable the updates on the left and right preconditioners. That is, let $G_t = U\Sigma V^T$ be the singular value decomposition (SVD) of $G_t$ and let
$$
L_t := G_t G_t^T\quad\quad R_t := G_t^T G_t
$$
Then,
$$\begin{aligned}
\Delta W_t
&amp;= (\tilde{L}^{-1/2}_t G_t + 2 \tilde{L}^{-1/4}_t G_t \tilde{R}^{-1/4}_t + G_t \tilde{R}^{-1/2}_t)/4\\
&amp;= (1/4) \cdot [(G_t G_t^T + \epsilon I_m)^{-1/2} G_t\\
&amp;\quad\quad\quad+ 2 (G_t G_t^T + \epsilon I_m)^{-1/4} G_t (G_t^T G_t + \epsilon I_n)^{-1/4}\\
&amp;\quad\quad\quad+ G_t (G_t^T G_t + \epsilon I_n)^{-1/2}]\\
&amp;= (1/4) \cdot [[(U \Sigma V^T) (U \Sigma V^T)^T + \epsilon U I U^T]^{-1/2}(U \Sigma V^T)\\
&amp;\quad\quad\quad + 2[(U \Sigma V^T) (U \Sigma V^T)^T + \epsilon U I U^T]^{-1/4}(U \Sigma V^T) [(U \Sigma V^T)^T (U \Sigma V^T) + \epsilon V I V^T]^{-1/4}\\
&amp;\quad\quad\quad + (U \Sigma V^T) [(U \Sigma V^T)^T (U \Sigma V^T) + \epsilon V I V^T]^{-1/2}]\\
&amp;= (1/4) \cdot [U(\Sigma(\Sigma^2 + \epsilon I)^{-1/2})V^T + U(2\Sigma(\Sigma^2 + \epsilon I)^{-1/2})V^T + U(\Sigma(\Sigma^2 + \epsilon I)^{-1/2})V^T]\\
&amp;= (1/4) \cdot [U(1+2+1)(\Sigma(\Sigma^2 + \epsilon I)^{-1/2})V^T]\\
&amp;= U\left(\frac{\Sigma}{\sqrt{\Sigma^2 + \epsilon I}} \right)V^T \\
\Delta W_t &amp;\approx UV^T
\end{aligned}$$
which is Muon&rsquo;s update rule. $\blacksquare$</p>
<p><strong>4.2.4. PSGD Family. [Under Review]</strong> This family of optimizers (Li, 2015 &amp; 2018; Pooladzandi, 2024) explicitly tries to learn the preconditioner $\mathcal{P}(\cdot; W)$ according to several criteria to ensure training stability and, potentially, faster convergence. One of which is the noise suppression gain,
$$
\frac{\mathbb{E}[||H_0^{-1}\epsilon&rsquo;||_F^2]}{\mathbb{E}[||P\epsilon&rsquo;||_F^2]}
= \frac{\mathbb{E}[(\epsilon&rsquo;)^T H_0^{-2} \epsilon&rsquo;]}{\mathbb{E}[(\epsilon&rsquo;)^T P^2 \epsilon&rsquo;]}
$$
where $\epsilon&rsquo;$ is some (matrix-valued) noise term on the Hessian $H$, which aims to reduce the noise of the preconditioned gradients.</p>
<p>These criteria typically result in update rules that <em>whitens</em>, that is decorrelates, the entries of the gradient $\nabla \mathcal{L}(W)_\xi$. And so while Muon can be seen as an instantaneous version of PSGD, an important difference is that Muon merely projects the gradient to its nearest (semi-)orthogonal matrix, but not necessarily decorrelates the entries.</p>
<p>For future work, it would also be interesting to see what kind of update rules we get if we measure the noise suppression gain with respect to the spectral norm instead of the Frobenius norm. That is,
$$
\frac{\mathbb{E}[||H_0^{-1}\epsilon&rsquo;||_{2\to 2}]}{\mathbb{E}[||P\epsilon&rsquo;||_{2\to 2}]}
$$</p>
<h2 id="5-steepest-descent-in-non-riemannian-manifolds-induced-by-schatten-p-norms">5. Steepest Descent in Non-Riemannian Manifolds Induced by Schatten-$p$ Norms<a hidden class="anchor" aria-hidden="true" href="#5-steepest-descent-in-non-riemannian-manifolds-induced-by-schatten-p-norms">#</a></h2>
<blockquote>
<p><strong>Definition 2 (Schatten-$p$ Norms).</strong> The Schatten-$p$ norm of a real-valued matrix $X \in \mathbb{R}^{m \times n}$ is defined as:
$$||X||_p = \left(\sum_{i=1}^{\min(m, n)} |\sigma_i(X)|^p\right)^{1/p},$$
where $\sigma_i(X)$ are the singular values of $X$. In a sense, the Schatten-$p$ norm of $X$ can be seen as the $p$-norm of its singular values.</p>
<p><strong>Examples:</strong></p>
<ol>
<li>$p = 1$: The Nuclear norm, $||A||_{S_1} = \sum_{i=1}^{\min(m,n)} |\sigma_i(A)| = ||A||_{\text{nuc}}$</li>
<li>$p = 2$: The Frobenius norm, $||A||_{S_2} = \left(\sum_{i=1}^{\min(m,n)} |\sigma_i(A)|^2\right)^{\frac{1}{2}} = ||A||_F$</li>
<li>$p = \infty$: The Spectral norm, $||A||_{S_{\infty}} = \max_{i} \sigma_i(A) = ||A||_{2 \to 2}$</li>
</ol></blockquote>
<p>A special case is when $p = 2$, and so we have the Frobenius norm. Equipping $\mathcal{W}$ with this norm gives us the standard Euclidean manifold, which is Riemannian. However, Proposition (4) below still applies.</p>
<p>And to find the dualizers for the Schatten-$p$ norms, we will use the following inequality.</p>
<blockquote>
<p><strong>Theorem 3 (von Neumann&rsquo;s Trace Inequality).</strong> Let $A, B \in \mathbb{R}^{m \times n}$. Then the following inequality holds,
$$\text{tr}(A^TB) \leq \sum_{i=1}^{\min(m,n)} \sigma_i(A) \sigma_i(B),$$
where $\sigma_i(A)$ and $\sigma_i(B)$ are the singular values of $A$ and $B$, respectively. And equality holds if and only if $A$ and $B$ share singular vectors.</p></blockquote>
<h3 id="51-dualizers-for-schatten-p-norms">5.1. Dualizers for Schatten-$p$ Norms<a hidden class="anchor" aria-hidden="true" href="#51-dualizers-for-schatten-p-norms">#</a></h3>
<p>Here, we derive the dualizer for an arbitrary Schatten-$p$ norm $||\cdot||_{S_p}$.</p>
<blockquote>
<p><strong>Proposition 4.</strong> The dualizer for the Schatten-$p$ norm is:
$$\text{dualizer}_{||\cdot||_{S_p}}(X) = U \frac{\text{diag}\left(\sigma_1(X)^{q-1}, \ldots, \sigma_{\min(m,n)}(X)^{q-1}\right)}{||X||_{S_q}^{q-1}} V^T$$
where $\frac{1}{p} + \frac{1}{q} = 1$, and $X = U\Sigma V^T$ is the singular value decomposition of $X \in T_W\mathcal{W}$ at $W \in \mathcal{W}$. Note that this dualizer is independent of $W$.</p></blockquote>
<blockquote>
<p><strong>Proof:</strong> For a given $X \in T_W\mathcal{W}$ at $W \in \mathcal{W}$, let $T^* \in T_W\mathcal{W}$ be,
$$
\begin{align*}
T^* &amp;= \text{dualizer}_{||\cdot||_{S_p}}(X; W)\\
T^* &amp;= \arg\max_{\substack{T \in T_W\mathcal{W}\\ ||T||_{S_p} = 1}} \langle X, T \rangle_F\\
T^* &amp;= \arg\max_{\substack{T \in T_W\mathcal{W}\\ ||T||_{S_p} = 1}} \text{tr}(X^T T)
\end{align*}
$$
Then from von Neumann&rsquo;s Trace Inequality, we know that $T^*$ must share singular vectors with $X$ and that,
$$T^* = \arg\max_{\substack{T \in T_W\mathcal{W}\\ ||T||_{S_p} = 1}} \sum_{i=1}^{\min(m,n)} \sigma_i(X) \sigma_i(T)$$
Thus, our optimization problem reduces to
$$\arg\max_{\{\sigma_i(T)\}} \sum_{i=1}^{\min(m,n)} \sigma_i(X) \sigma_i(T) \quad\text{s.t.}\quad \sum_{i=1}^{\min(m,n)} \sigma_{i}(T)^p = 1$$
And solving via Lagrange multipliers, we have,
$$\sigma_i(T) = \frac{\sigma_i(X)^{q-1}}{||X||_{S_q}^{q-1}}$$
which is indepdent of $W$. Hence,
$$T^* = \text{dualizer}_{||\cdot||_{S_p}}(X) = U \frac{\text{diag}\left(\sigma_1(X)^{q-1}, \ldots, \sigma_{\min(m,n)}(X)^{q-1}\right)}{||X||_{S_q}^{q-1}} V^T\quad\blacksquare$$</p></blockquote>
<h3 id="52-stochastic-gradient-descent-and-muon-as-special-cases">5.2. Stochastic Gradient Descent and Muon as Special Cases<a hidden class="anchor" aria-hidden="true" href="#52-stochastic-gradient-descent-and-muon-as-special-cases">#</a></h3>
<p><strong>4.2.1. Recovering SGD.</strong> Let $p = 2$, and so we have $q = 2$ and $||\cdot||_{S_2} = ||\cdot||_F$. Thus for $\nabla \mathcal{L}(W)_\xi \in T_W\mathcal{W}$ at $W \in \mathcal{W}$, we have,
$$
\begin{align*}
\Delta W
&amp;= \text{dualizer}_{||\cdot||_{S_\infty}}(\nabla \mathcal{L}(W)_\xi; W)\\
&amp;= U \frac{\text{diag}\left(\sigma_1(\nabla \mathcal{L}(W)_\xi)^{2-1}, \ldots, \sigma_{\min(m,n)}(\nabla \mathcal{L}(W)_\xi)^{2-1}\right)}{||\nabla \mathcal{L}(W)_\xi||_{S_2}^{2-1}} V^T\\
\Delta W &amp;= \frac{\nabla \mathcal{L}(W)_\xi}{||\nabla \mathcal{L}(W)_\xi||_F}
\end{align*}
$$
which matches the update rule we expect from SGD.</p>
<p><strong>4.2.2. Recovering Muon.</strong> Let $p = \infty$, and so we have $q = 1$ and $||\cdot||_{S_\infty} = ||\cdot||_{2\to 2}$. Thus for $\nabla \mathcal{L}(W)_\xi \in T_W\mathcal{W}$ at $W \in \mathcal{W}$, we have,
$$
\begin{align*}
\Delta W
&amp;= \text{dualizer}_{||\cdot||_{2 \to 2}}(\nabla \mathcal{L}(W)_\xi; W)\\
&amp;= U \frac{\text{diag}\left(\sigma_1(\nabla \mathcal{L}(W)_\xi)^{1-1}, \ldots, \sigma_{\min(m,n)}(X)^{1-1}\right)}{||X||_{S_1}^{1-1}} V^T\\
\Delta W &amp;= UV^T
\end{align*}
$$
which matches the update rule we expect from Muon.</p>
<h3 id="53-why-muon-still-works-well-in-practice-despite-the-approximate-orthogonalization">5.3. Why Muon Still Works Well in Practice Despite the Approximate Orthogonalization<a hidden class="anchor" aria-hidden="true" href="#53-why-muon-still-works-well-in-practice-despite-the-approximate-orthogonalization">#</a></h3>
<p>We observe that, qualitatively, steepest descent under the Schatten-$p$ norm very quickly converges to steepest descent under the spectral norm as $p$ approaches $\infty$. This is probably why optimizers like Sketching and Muon work well in practice despite not perfectly orthogonalizing the gradients.</p>
<p>To support this, we show that the (1) variance of dualized singular values, and the (2) relative size, and (3) stable rank of the dualized gradients under the Schatten-$p$ norm quadratically converges to those of the Spectral norm as $p$ approaches $\infty$. And in fact, at $p = 32$, the results are already very close to those of the Spectral norm.</p>
<p><strong>5.3.1. On the variance of dualized singular values</strong></p>
<blockquote>
<p><strong>Proposition 5.</strong> The variance of the dualized singular values under the Schatten-$p$ Norm converges quadratically to $0$ as $p$ approaches $\infty$.</p></blockquote>
<blockquote>
<p><strong>Proof:</strong> Let $t_i$ be the $i$-th dualized singular value. From Proposition 4 earlier, we have
$$
\begin{align*}
t_i &amp;= \left(\frac{\sigma_i(\nabla L(W_l))}{||\nabla L(W_l)||_{S_q}}\right)^{q-1}\\
t_i &amp;= \exp\left((q-1)\ln\frac{\sigma_i(\nabla L(W_l))}{||\nabla L(W_l)||_{S_q}}\right)\\
t_i &amp;\approx 1 + (q-1)\ln\frac{\sigma_i(\nabla L(W_l))}{||\nabla L(W_l)||_{S_q}}
\end{align*}
$$
where the last line follows from first-order Taylor approximation of $t_i$. Thus, the mean and variance are:
$$
\begin{align*}
\mathbb{E}[t_i] &amp;\approx 1 + (q-1)\mathbb{E}\left[\ln\frac{\sigma_i(\nabla L(W_l))}{||\nabla L(W_l)||_{S_q}}\right]\\
\mathbb{E}[t_i] &amp;\approx 1 + (q-1)\ln\frac{\mathbb{E}[\sigma_i(\nabla L(W_l))]}{||\nabla L(W_l)||_{S_q}}\\
t_i - \mathbb{E}[t_i] &amp;\approx (q-1)\ln\left[\sigma_i(\nabla L(W_l)) - \mathbb{E}[\sigma_i(\nabla L(W_l))]\right]\\
Var[t_i] &amp;\approx (q-1)^2\mathbb{E}\left[\ln^2\left[\sigma_i(\nabla L(W_l)) - \mathbb{E}[\sigma_i(\nabla L(W_l))]\right]\right]\\
Var[t_i] &amp;\approx \frac{1}{(p-1)^2}\mathbb{E}\left[\ln^2\left[\sigma_i(\nabla L(W_l)) - \mathbb{E}[\sigma_i(\nabla L(W_l))]\right]\right]
\end{align*}
$$
Hence, the variance of the dualized singular values converges quadratically to $0$ as $p$ approaches $\infty$.</p></blockquote>
<p>Empirically, we can see this in the following plot. And at $p = 32$, the variance of the resulting singular values are already close to $0$.
<img loading="lazy" src="../steepest-descent-schatten-p/var_sv_dualizer.png#center"></p>
<p><strong>5.3.2. On the relative size and stable rank of gradients</strong></p>
<blockquote>
<p><strong>Definition 6: Relative Size of a Gradient.</strong> Given a norm $||\cdot||$ chosen a priori, the relative size of a gradient-update $\Delta W$ relative to the parameter matrix $W$ is defined as:
$$\text{relsize}(\Delta W) = \frac{||\Delta W||}{||W||}$$</p></blockquote>
<blockquote>
<p><strong>Definition 7: Stable Rank.</strong> The stable rank of a matrix $A$ is defined as $$srank(A) = \frac{||A||_F^2}{||A||_{2 \to 2}^2}$$</p></blockquote>
<p>As we can see in the following plot, the raw gradients have very low-stable rank. But the stable rank of the gradients post-dualization under the Schatten-$p$ norm converges very quickly to that of the Spectral norm as $p$ approaches $\infty$.</p>
<p><img loading="lazy" src="../steepest-descent-schatten-p/srank_sv_dualizer.png#center"></p>
<p>One can interpret this as, for some large enough $p$, the dualized gradient is already very close to being &ldquo;maximal&rdquo; in a sense. And increasing $p$ further would only offer rapidly diminishing returns.</p>
<p>A side-effect of this is that it allows the model parameters to &ldquo;escape&rdquo; the small region around the initialization and explore the parameter space more effectively, contrary to prior work on SGD and Adam optimizers (Lee et al., 2020; Jesus et al., 2021). This phenomenon is known as <em>weight erasure</em> (Bernstein, 2024).</p>
<p><img loading="lazy" src="../steepest-descent-schatten-p/weight-erasure.png"></p>
<h2 id="6-optimizing-muons-newton-schulz-coefficients-under-construction">6. Optimizing Muon&rsquo;s Newton-Schulz Coefficients [Under Construction]<a hidden class="anchor" aria-hidden="true" href="#6-optimizing-muons-newton-schulz-coefficients-under-construction">#</a></h2>
<h2 id="7-convergence-guarantees-under-construction">7. Convergence Guarantees [Under Construction]<a hidden class="anchor" aria-hidden="true" href="#7-convergence-guarantees-under-construction">#</a></h2>
<h2 id="how-to-cite">How to Cite<a hidden class="anchor" aria-hidden="true" href="#how-to-cite">#</a></h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bibtex" data-lang="bibtex"><span style="display:flex;"><span><span style="color:#0a0;text-decoration:underline">@misc</span>{cesista2025sdnr,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">author</span> = <span style="color:#a50">{Franz Louis Cesista}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">title</span> = <span style="color:#a50">{Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">year</span> = <span style="color:#a50">{2025}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">url</span> = <span style="color:#a50">{http://leloykun.github.io/ponder/steepest-descent-non-riemannian/}</span>,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ol>
<li>Keller Jordan and Yuchen Jin and Vlado Boza and You Jiacheng and Franz Cesista and Laker Newhouse and Jeremy Bernstein (2024). Muon: An optimizer for hidden layers in neural networks. URL <a href="https://kellerjordan.github.io/posts/muon/" target="_blank">https://kellerjordan.github.io/posts/muon/</a></li>
<li>Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein (2024). Muon: An optimizer for hidden layers in neural networks. Available at: <a href="https://kellerjordan.github.io/posts/muon/" target="_blank">https://kellerjordan.github.io/posts/muon/</a>.</li>
<li>Moonshot AI Team (2025). Muon is Scalable for LLM Training. URL <a href="https://arxiv.org/abs/2502.16982" target="_blank">https://arxiv.org/abs/2502.16982</a></li>
<li>Jeremy Bernstein and Laker Newhouse. “Old optimizer, new norm: An anthology.” arXiv preprint arXiv:2409.20325 (2024).</li>
<li>Jeremy Bernstein, Laker Newhouse (2024). Modular Duality in Deep Learning. URL <a href="https://arxiv.org/abs/2410.21265" target="_blank">https://arxiv.org/abs/2410.21265</a></li>
<li>Jeremy Bernstein (2024). &ldquo;Weight erasure.&rdquo; Available at: <a href="https://docs.modula.systems/examples/weight-erasure/" target="_blank">https://docs.modula.systems/examples/weight-erasure/</a></li>
<li>Xi-Lin Li (2015). Preconditioned Stochastic Gradient Descent. URL <a href="https://arxiv.org/abs/1512.04202" target="_blank">https://arxiv.org/abs/1512.04202</a></li>
<li>Xi-Lin Li (2018). Preconditioner on Matrix Lie Group for SGD. URL <a href="https://arxiv.org/abs/1809.10232" target="_blank">https://arxiv.org/abs/1809.10232</a></li>
<li>Omead Pooladzandi, Xi-Lin Li (2024). Curvature-Informed SGD via General Purpose Lie-Group Preconditioners. URL <a href="https://arxiv.org/abs/2402.04553" target="_blank">https://arxiv.org/abs/2402.04553</a></li>
<li>David E Carlson, Edo Collins, Ya-Ping Hsieh, Lawrence Carin, Volkan Cevher. Preconditioned Spectral Descent for Deep Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), 2015. URL <a href="https://proceedings.neurips.cc/paper_files/paper/2015/hash/f50a6c02a3fc5a3a5d4d9391f05f3efc-Abstract.html" target="_blank">https://proceedings.neurips.cc/paper_files/paper/2015/hash/f50a6c02a3fc5a3a5d4d9391f05f3efc-Abstract.html</a></li>
<li>Thomas Flynn. The duality structure gradient descent algorithm: Analysis and applications to neural networks. arXiv:1708.00523, 2017. URL <a href="https://arxiv.org/abs/1708.00523" target="_blank">https://arxiv.org/abs/1708.00523</a></li>
<li>Hunter, D. R. and Lange, K. (2004). A tutorial on MM algorithms. The American Statistician, 58(1):30–37.</li>
<li>Lucas Prieto, Melih Barsbey, Pedro A.M. Mediano, Tolga Birdal (2025). Grokking at the Edge of Numerical Stability. URL <a href="https://arxiv.org/abs/2501.04697" target="_blank">https://arxiv.org/abs/2501.04697</a></li>
<li>Vineet Gupta, Tomer Koren, Yoram Singer (2018). Shampoo: Preconditioned Stochastic Tensor Optimization. URL <a href="https://arxiv.org/abs/1802.09568" target="_blank">https://arxiv.org/abs/1802.09568</a></li>
<li>Rohan Anil et al. “Scalable second order optimization for deep learning.” arXiv preprint arXiv:2002.09018 (2020).</li>
<li>Surya, S., Duvvuri, Devvrit, F., Anil, R., Hsieh, C., &amp; Dhillon, I.S. (2024). Combining Axes Preconditioners through Kronecker Approximation for Deep Learning. International Conference on Learning Representations.</li>
<li>Franz Louis Cesista (2025). {CASPR} Without Accumulation is {M}uon. URL <a href="https://leloykun.github.io/ponder/caspr-wo-accum-is-muon/" target="_blank">https://leloykun.github.io/ponder/caspr-wo-accum-is-muon/</a></li>
<li>Rohan Anil. “Just some fun linear algebra.” X post, 6 Oct. 2024, Available at: <a href="https://x.com/_arohan_/status/1843050297985466565" target="_blank">https://x.com/_arohan_/status/1843050297985466565</a>.</li>
<li>Dmitry Kovalev (2025). Understanding Gradient Orthogonalization for Deep Learning via Non-Euclidean Trust-Region Optimization. Available at: <a href="https://arxiv.org/abs/2503.12645" target="_blank">https://arxiv.org/abs/2503.12645</a></li>
<li>Lee, Jaehoon, et al. “Wide Neural Networks of Any Depth Evolve as Linear Models under Gradient Descent.” Journal of Statistical Mechanics: Theory and Experiment, vol. 2020, no. 12, Dec. 2020, p. 124002. Crossref, <a href="https://doi.org/10.1088/1742-5468/abc62b" target="_blank">https://doi.org/10.1088/1742-5468/abc62b</a>.</li>
<li>Jesus, Ricardo J., et al. “Effect of Initial Configuration of Weights on Training and Function of Artificial Neural Networks.” Mathematics, vol. 9, no. 18, Sept. 2021, p. 2246. Crossref, <a href="https://doi.org/10.3390/math9182246" target="_blank">https://doi.org/10.3390/math9182246</a>.</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://leloykun.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://leloykun.github.io/tags/muon/">Muon</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds on x"
            href="https://x.com/intent/tweet/?text=Muon%20and%20a%20Selective%20Survey%20on%20Steepest%20Descent%20in%20Riemannian%20and%20Non-Riemannian%20Manifolds&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-non-riemannian%2f&amp;hashtags=MachineLearning%2cMuon">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-non-riemannian%2f&amp;title=Muon%20and%20a%20Selective%20Survey%20on%20Steepest%20Descent%20in%20Riemannian%20and%20Non-Riemannian%20Manifolds&amp;summary=Muon%20and%20a%20Selective%20Survey%20on%20Steepest%20Descent%20in%20Riemannian%20and%20Non-Riemannian%20Manifolds&amp;source=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-non-riemannian%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-non-riemannian%2f&title=Muon%20and%20a%20Selective%20Survey%20on%20Steepest%20Descent%20in%20Riemannian%20and%20Non-Riemannian%20Manifolds">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-non-riemannian%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds on whatsapp"
            href="https://api.whatsapp.com/send?text=Muon%20and%20a%20Selective%20Survey%20on%20Steepest%20Descent%20in%20Riemannian%20and%20Non-Riemannian%20Manifolds%20-%20https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-non-riemannian%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds on telegram"
            href="https://telegram.me/share/url?text=Muon%20and%20a%20Selective%20Survey%20on%20Steepest%20Descent%20in%20Riemannian%20and%20Non-Riemannian%20Manifolds&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-non-riemannian%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Muon and a Selective Survey on Steepest Descent in Riemannian and Non-Riemannian Manifolds on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Muon%20and%20a%20Selective%20Survey%20on%20Steepest%20Descent%20in%20Riemannian%20and%20Non-Riemannian%20Manifolds&u=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-non-riemannian%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://leloykun.github.io/">Franz Louis Cesista</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
