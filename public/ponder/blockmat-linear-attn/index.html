<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Blocked Matrix Formulation of Linear Attention Mechanisms | Franz Louis Cesista</title>
<meta name="keywords" content="Machine Learning, Linear Attention, Test-Time Regression">
<meta name="description" content="The blocked matrix formulation of linear attention mechanisms, multi-step online gradient descent at inference time, and chunk-wise parallelism.">
<meta name="author" content="Franz Louis Cesista">
<link rel="canonical" href="https://leloykun.github.io/ponder/blockmat-linear-attn/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.7695d65198b4eb0454d08822ddd0b9bc75437b4a2a995eef4f73b0ae209c4e92.css" integrity="sha256-dpXWUZi06wRU0Igi3dC5vHVDe0oqmV7vT3OwriCcTpI=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://leloykun.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://leloykun.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://leloykun.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://leloykun.github.io/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://leloykun.github.io/ponder/blockmat-linear-attn/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Blocked Matrix Formulation of Linear Attention Mechanisms" />
<meta property="og:description" content="The blocked matrix formulation of linear attention mechanisms, multi-step online gradient descent at inference time, and chunk-wise parallelism." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://leloykun.github.io/ponder/blockmat-linear-attn/" /><meta property="article:section" content="ponder" />
<meta property="article:published_time" content="2025-03-16T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-03-16T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Blocked Matrix Formulation of Linear Attention Mechanisms"/>
<meta name="twitter:description" content="The blocked matrix formulation of linear attention mechanisms, multi-step online gradient descent at inference time, and chunk-wise parallelism."/>
<meta name="twitter:site" content="@leloykun"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Ponder",
      "item": "https://leloykun.github.io/ponder/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Blocked Matrix Formulation of Linear Attention Mechanisms",
      "item": "https://leloykun.github.io/ponder/blockmat-linear-attn/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Blocked Matrix Formulation of Linear Attention Mechanisms",
  "name": "Blocked Matrix Formulation of Linear Attention Mechanisms",
  "description": "The blocked matrix formulation of linear attention mechanisms, multi-step online gradient descent at inference time, and chunk-wise parallelism.",
  "keywords": [
    "Machine Learning", "Linear Attention", "Test-Time Regression"
  ],
  "articleBody": "In the previous post, we derived several linear attention mechanisms from scratch by formulating them as test-time online regression problems. Here, we’ll discuss a more intuitive way to represent the update rules of the internal states of these linear attention mechanisms using a blocked matrix formulation. Then, we’ll discuss how to use it to (1) derive the update rules for linear attention mechanisms that take multiple gradient descent steps per token and (2) derive the update rules for chunk-wise parallelism of already-existing linear attention mechanisms.\nRecap: Linear Attention Mechanisms Linear attention mechanisms typically have an update rule of the form: $$S_i = S_{i-1}A_i + B_i$$ where $S_{i-1}$ is the (old) state after processing the first $i-1$ tokens, $S_i$ is the (new) state after processing the first $i$ tokens, and $A_i$ and $B_i$ are update matrices. Think of $A_i$ as an operation that modifies some information already stored in the state while $B_i$ adds new information to the state. In most cases where $A_i \\neq I$, $A_i$ typically removes some (old) information from the state. But if we allow $A_i$ to have negative eigenvalues, then we can also think of it as an operation that, in a sense, inverts information instead.\nHere are a couple of examples:\nLinear Attention Mechanism $A_i$ $B_i$ Vanilla Linear Attention $I$ $\\bm{v}_i \\bm{k}_i^T$ Mamba 2 $\\text{diag}\\left(\\alpha_i I\\right)$ $\\bm{v}_i \\bm{k}_i^T$ DeltaNet $I - \\beta_i \\bm{k}_i \\bm{k}_i^T$ $\\beta_i \\bm{v}_i \\bm{k}_i^T$ Gated DeltaNet $\\alpha_i(I - \\beta_i \\bm{k}_i \\bm{k}_i^T)$ $\\beta_i \\bm{v}_i \\bm{k}_i^T$ RWKV-7 $\\text{diag}(\\bm{w}_i) - \\bm{\\hat{\\kappa}}_i(\\bm{a}_i \\odot\\bm{\\hat{\\kappa}}_i^T)$ $\\bm{v}_i \\bm{k}_i^T$ where $\\bm{k}_i \\in \\mathbb{R}^{d_k}$ and $\\bm{v}_i \\in \\mathbb{R}^{d_v}$ are the corresponding key-value pair for the $i$-th token; $\\alpha_i \\in [0, 1]$ can be thought of as a date-dependent weight decay that controls how much of the previous state to keep or forget; and $\\beta_i \\in [0, 1]$ can be thought of as a date-dependent learning rate that controls how much of the new information to add to the state.\nIf we let $\\alpha_i \\in [-1, 1]$ for Mamba 2 and $\\beta_i \\in [0, 2]$ for (Gated) DeltaNet, then $A_i$ can have negative eigenvalues while still having norm $||A_i|| \\leq 1$. This allows the models to learn more complex patterns while maintaining training stability (Grazzi et al., 2025).\nBlocked Matrix Formulation of Linear Attention Mechanisms Notice that we can rewrite the update rule above as,\n$$ \\begin{align*} S_i \u0026= S_{i-1}A_i + B_i\\\\ S_{i} \u0026= \\begin{bmatrix} S_{i-1} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_i \\\\ B_i \\end{bmatrix} \\end{align*} $$ or, equivalently, $$ \\begin{bmatrix} S_{i} \u0026 I \\end{bmatrix} = \\begin{bmatrix} S_{i-1} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_i \u0026 0 \\\\ B_i \u0026 I \\end{bmatrix} $$\nAt training time, we need all of the intermediary states, not just the final state. Thus, we need an efficient way to compute $S_N$ for all token indices $N$. To do this, let’s unroll the recurrence above:\n$$ \\begin{align*} \\begin{bmatrix} S_{N} \u0026 I \\end{bmatrix} \u0026= \\begin{bmatrix} S_{N-1} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_N \u0026 0 \\\\ B_N \u0026 I \\end{bmatrix}\\\\ \\begin{bmatrix} S_{N} \u0026 I \\end{bmatrix} \u0026= \\begin{bmatrix} S_{N-2} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_{N-1} \u0026 0 \\\\ B_{N-1} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_N \u0026 0 \\\\ B_N \u0026 I \\end{bmatrix}\\\\ \u0026\\vdots\\\\ \\begin{bmatrix} S_{N} \u0026 I \\end{bmatrix} \u0026= \\begin{bmatrix} S_{0} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_1 \u0026 0 \\\\ B_1 \u0026 I \\end{bmatrix} \\begin{bmatrix} A_2 \u0026 0 \\\\ B_2 \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A_N \u0026 0 \\\\ B_N \u0026 I \\end{bmatrix}\\\\ S_N \u0026= \\begin{bmatrix} S_{0} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_1 \u0026 0 \\\\ B_1 \u0026 I \\end{bmatrix} \\begin{bmatrix} A_2 \u0026 0 \\\\ B_2 \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A_N \u0026 0 \\\\ B_N \u0026 I \\end{bmatrix} \\begin{bmatrix} I \\\\ 0 \\end{bmatrix} \\end{align*} $$\nIn practice, we usually initialize $S_0$ as the zero matrix. Thus,\n$$ \\begin{align} S_N \u0026= \\begin{bmatrix} 0 \u0026 I \\end{bmatrix} \\begin{bmatrix} A_1 \u0026 0 \\\\ B_1 \u0026 I \\end{bmatrix} \\begin{bmatrix} A_2 \u0026 0 \\\\ B_2 \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A_N \u0026 0 \\\\ B_N \u0026 I \\end{bmatrix} \\begin{bmatrix} I \\\\ 0 \\end{bmatrix}\\\\ S_N \u0026= \\begin{bmatrix} 0 \u0026 I \\end{bmatrix} \\begin{bmatrix} \\prod_{i=1}^{N} A_i \u0026 0 \\\\ \\sum_{i=1}^{N} \\left(B_i \\prod_{j=i+1}^{N} A_j\\right) \u0026 I \\end{bmatrix} \\begin{bmatrix} I \\\\ 0 \\end{bmatrix}\\\\ S_N \u0026= \\sum_{i=1}^{N} \\left(B_i \\prod_{j=i+1}^{N} A_j\\right) \\end{align} $$ where $(1) \\rightarrow (2)$ can be proven by induction.\nEquation $(1)$ makes it obvious why and how we can parallelize computation of $S_N$, for all $N$, at training time: the updates are merely (blocked) matrix multiplications; matrix multiplications are associative; thus, we can use the (fully-parallel) associative scan algorithm to compute all the intermediary states in $O(N)$ time!\nOne-Step Online Gradient Descent per Token Let’s derive $S_N$ for each of the linear attention mechanisms in the table above.\nVanilla Linear Attention Show derivation of $S_N$ $$A_i = I \\quad\\quad B_i = \\bm{v}_i \\bm{k}_i^T$$ From Equation $(3)$ above, we get: $$ \\begin{align*} S_N \u0026= \\sum_{i=1}^{N} \\left(\\bm{v}_i \\bm{k}_i^T \\prod_{j=i+1}^{N} I\\right)\\\\ S_N \u0026= \\sum_{i=1}^{N} \\bm{v}_i \\bm{k}_i^T \\end{align*} $$ Mamba 2 Show derivation of $S_N$ $$A_i = \\text{diag}\\left(\\alpha_i I\\right) \\quad\\quad B_i = \\bm{v}_i \\bm{k}_i^T$$ Thus, $$ \\begin{align*} S_N \u0026= \\sum_{i=1}^{N} \\left(\\bm{v}_i \\bm{k}_i^T \\prod_{j=i+1}^{N} \\text{diag}\\left(\\alpha_j I\\right)\\right)\\\\ S_N \u0026= \\sum_{i=1}^{N} \\left( \\prod_{j=i+1}^{N} \\alpha_j \\right) \\bm{v}_i \\bm{k}_i^T \\end{align*} $$ DeltaNet Show derivation of $S_N$ $$A_i = I - \\beta_i \\bm{k}_i \\bm{k}_i^T \\quad\\quad B_i = \\beta_i \\bm{v}_i \\bm{k}_i^T$$ Thus, $$S_N = \\sum_{i=1}^{N} \\left(\\beta_i \\bm{v}_i \\bm{k}_i^T \\prod_{j=i+1}^{N} \\left(I - \\beta_j \\bm{k}_j \\bm{k}_j^T\\right)\\right)$$ Gated DeltaNet Show derivation of $S_N$ $$A_i = \\alpha_i(I - \\beta_i \\bm{k}_i \\bm{k}_i^T) \\quad\\quad B_i = \\beta_i \\bm{v}_i \\bm{k}_i^T$$ Thus, $$ \\begin{align*} S_N \u0026= \\sum_{i=1}^{N} \\left(\\beta_i \\bm{v}_i \\bm{k}_i^T \\prod_{j=i+1}^{N} \\alpha_j \\left(I - \\beta_j \\bm{k}_j \\bm{k}_j^T\\right)\\right)\\\\ S_N \u0026= \\sum_{i=1}^{N} \\left(\\left(\\beta_i \\prod_{j=i+1}^{N} \\alpha_j \\right) \\bm{v}_i \\bm{k}_i^T \\prod_{j=i+1}^{N} \\left(I - \\beta_j \\bm{k}_j \\bm{k}_j^T\\right)\\right) \\end{align*} $$ RWKV-7 Show derivation of $S_N$ $$A_i = \\text{diag}(\\bm{w}_i) - \\bm{\\hat{\\kappa}}_i(\\bm{a}_i \\odot\\bm{\\hat{\\kappa}}_i^T) \\quad\\quad B_i = \\bm{v}_i \\bm{k}_i^T$$ Thus, $$S_N = \\sum_{i=1}^{N} \\left(\\bm{v}_i \\bm{k}_i^T \\prod_{j=i+1}^{N} \\left(\\text{diag}(\\bm{w}_j) - \\bm{\\hat{\\kappa}}_j(\\bm{a}_j \\odot\\bm{\\hat{\\kappa}}_j^T)\\right)\\right)$$ Easy!\nMulti-Step Online Gradient Descent per Token Now, what if we take $n_h$ gradient descent steps per token?\nTo do this, we can follow the procedure outlined in the DeltaProduct (Siems et al., 2025) paper where they:\nRecurrently generate $n_h$ key-value pairs for each input token, Update the state using the $n_h$ key-value pairs, and Keep only the final key-value pair and discard the rest. In our formulation, this is equivalent to replacing each update with a product of $n_h$ updates:\n$$ \\begin{bmatrix} A_{i} \u0026 0 \\\\ B_{i} \u0026 I \\end{bmatrix} \\longrightarrow \\begin{bmatrix} A_{i,1} \u0026 0 \\\\ B_{i,1} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_{i,2} \u0026 0 \\\\ B_{i,2} \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A_{i,n_h} \u0026 0 \\\\ B_{i,n_h} \u0026 I \\end{bmatrix} $$ where $A_{i,j}$ and $B_{i,j}$ are the update matrices for the $j$-th gradient descent step for the $i$-th token.\nThus, Equation $(1)$ becomes: $$ \\begin{align} S_N = \\begin{bmatrix} 0 \u0026 I \\end{bmatrix} \\begin{bmatrix} A_{1,1} \u0026 0 \\\\ B_{1,1} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_{1,2} \u0026 0 \\\\ B_{1,2} \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A_{1,n_h} \u0026 0 \\\\ B_{1,n_h} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_{2,1} \u0026 0 \\\\ B_{2,1} \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A_{N, n_h} \u0026 0 \\\\ B_{N, n_h} \u0026 I \\end{bmatrix} \\begin{bmatrix} I \\\\ 0 \\end{bmatrix} \\end{align} $$\nAnd if we reindex this as $[\\cdot]_k = [\\cdot]_{\\lceil k/n_h \\rceil,\\space (k-1) \\% n_h + 1}$, then from equation $(3)$ above, we get: $$ \\begin{align} S_N = \\sum_{k=1}^{Nn_h} \\left( B_k \\prod_{k’=k+1}^{Nn_h} A_{k’}\\right) \\end{align} $$\nAlternatively, we can also combine the updates for each token into a single update matrix first before multiplying them together:\n$$ \\begin{align} \\begin{bmatrix} A’_{i} \u0026 0 \\\\ B’_{i} \u0026 I \\end{bmatrix} = \\prod_{j=1}^{n_h} \\begin{bmatrix} A_{i,j} \u0026 0 \\\\ B_{i,j} \u0026 I \\end{bmatrix} = \\begin{bmatrix} \\prod_{j=1}^{n_h} A_{i,j} \u0026 0 \\\\ \\sum_{j=1}^{n_h} \\left(B_{i,j} \\prod_{j’=j+1}^{n_h} A_{i,j’}\\right) \u0026 I \\end{bmatrix} \\end{align} $$\n$$ \\begin{align} S_N \u0026= \\begin{bmatrix} 0 \u0026 I \\end{bmatrix} \\begin{bmatrix} A’_1 \u0026 0 \\\\ B’_1 \u0026 I \\end{bmatrix} \\begin{bmatrix} A’_2 \u0026 0 \\\\ B’_2 \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A’_N \u0026 0 \\\\ B’_N \u0026 I \\end{bmatrix} \\begin{bmatrix} I \\\\ 0 \\end{bmatrix}\\\\ S_N \u0026= \\begin{bmatrix} 0 \u0026 I \\end{bmatrix} \\begin{bmatrix} \\prod_{i=1}^N A’_i \u0026 0 \\\\ \\sum_{i=1}^N \\left( B’_i \\prod_{i’=i+1}^N A’_{i’} \\right) \u0026 I \\end{bmatrix} \\begin{bmatrix} I \\\\ 0 \\end{bmatrix}\\\\ S_N \u0026= \\sum_{i=1}^N \\left( B’_i \\prod_{i’=i+1}^N A’_{i’} \\right)\\\\ S_N \u0026= \\sum_{i=1}^N \\sum_{j=1}^{n_h} \\left( B_{i,j} \\underline{\\left(\\prod_{j’=j+1}^{n_h} A_{i,j’}\\right) \\left(\\prod_{i’=i+1}^N \\prod_{j’=1}^{n_h} A_{i’,j’} \\right)}\\right) \\end{align} $$\nwhich, again, if we reindex this as $[\\cdot]_k = [\\cdot]_{\\lceil k/n_h \\rceil,\\space (k-1) \\% n_h + 1}$, we get:\n$$S_N = \\sum_{k=1}^{Nn_h} \\left( B_k \\prod_{k’=k+1}^{Nn_h} A_{k’}\\right)$$ as expected.\nNow, let’s derive the $S_N$ for the linear attention mechanisms in the table above, but this time, with $n_h$ gradient descent steps per token.\nMambaSum* Show derivation of $S_N$ $$A_{i,j} = \\text{diag}\\left(\\alpha_{i,j} I\\right) \\quad\\quad B_{i,j} = \\bm{v}_{i,j} \\bm{k}_{i,j}^T$$ Thus, from Equation $(10)$ above, $$ \\begin{align*} S_N \u0026= \\sum_{i=1}^N \\sum_{j=1}^{n_h} \\left( \\bm{v}_{i,j} \\bm{k}_{i,j}^T \\left(\\prod_{j’=j+1}^{n_h} \\text{diag}\\left(\\alpha_{i,j’} I\\right)\\right) \\left(\\prod_{i’=i+1}^N \\prod_{j’=1}^{n_h} \\text{diag}\\left(\\alpha_{i’,j’} I\\right) \\right)\\right)\\\\ S_N \u0026= \\sum_{i=1}^N \\sum_{j=1}^{n_h} \\left(\\underline{\\left( \\prod_{j’=j+1}^{n_h} \\alpha_{i,j’}\\right) \\left(\\prod_{i’=i+1}^N \\prod_{j’=1}^{n_h} \\alpha_{i’,j’} \\right)} \\right) \\bm{v}_{i,j} \\bm{k}_{i,j}^T\\\\ S_N \u0026= \\sum_{k=1}^{Nn_h} \\left(\\prod_{k’=k+1}^{Nn_h} \\alpha_{k’}\\right) \\bm{v}_k \\bm{k}_k^T \\end{align*} $$ *I’m not actually sure if MambaSum already exists under a different name. If it does, please let me know!\nDeltaProduct Show derivation of $S_N$ $$A_{i,j} = I - \\beta_{i,j} \\bm{k}_{i,j} \\bm{k}_{i,j}^T \\quad\\quad B_{i,j} = \\beta_{i,j} \\bm{v}_{i,j} \\bm{k}_{i,j}^T$$ Thus, $$ \\begin{align*} S_N \u0026= \\sum_{i=1}^N \\sum_{j=1}^{n_h} \\left( \\beta_{i,j} \\bm{v}_{i,j} \\bm{k}_{i,j}^T \\underline{\\left(\\prod_{j’=j+1}^{n_h} \\left(I - \\beta_{i,j’} \\bm{k}_{i,j’} \\bm{k}_{i,j’}^T\\right)\\right) \\left(\\prod_{i’=i+1}^N \\prod_{j’=1}^{n_h} \\left(I - \\beta_{i’,j’} \\bm{k}_{i’,j’} \\bm{k}_{i’,j’}^T\\right) \\right)}\\right)\\\\ S_N \u0026= \\sum_{k=1}^{Nn_h} \\left(\\beta_k \\bm{v}_k \\bm{k}_k^T \\prod_{k’=k+1}^{Nn_h} \\left(I - \\beta_{k’} \\bm{k}_{k’} \\bm{k}_{k’}^T\\right)\\right) \\end{align*} $$ Gated DeltaProduct Show derivation of $S_N$ $$A_{i,j} = \\alpha_{i,j}(I - \\beta_{i,j} \\bm{k}_{i,j} \\bm{k}_{i,j}^T) \\quad\\quad B_{i,j} = \\beta_{i,j} \\bm{v}_{i,j} \\bm{k}_{i,j}^T$$ Thus, $$ \\begin{align*} S_N \u0026= \\sum_{i=1}^N \\sum_{j=1}^{n_h} \\left( \\beta_{i,j} \\bm{v}_{i,j} \\bm{k}_{i,j}^T \\underline{\\left(\\prod_{j’=j+1}^{n_h} \\alpha_{i,j’} \\left(I - \\beta_{i,j’} \\bm{k}_{i,j’} \\bm{k}_{i,j’}^T\\right)\\right) \\left(\\prod_{i’=i+1}^N \\prod_{j’=1}^{n_h} \\alpha_{i’,j’} \\left(I - \\beta_{i’,j’} \\bm{k}_{i’,j’} \\bm{k}_{i’,j’}^T\\right) \\right)}\\right)\\\\ S_N \u0026= \\sum_{k=1}^{Nn_h} \\left(\\beta_k \\bm{v}_k \\bm{k}_k^T \\prod_{k’=k+1}^{Nn_h} \\alpha_{k’} \\left(I - \\beta_{k’} \\bm{k}_{k’} \\bm{k}_{k’}^T\\right)\\right)\\\\ S_N \u0026= \\sum_{k=1}^{Nn_h} \\left(\\left( \\beta_k \\prod_{k’=k+1}^{Nn_h} \\alpha_{k’} \\right) \\bm{v}_k \\bm{k}_k^T \\prod_{k’=k+1}^{Nn_h} \\left(I - \\beta_{k’} \\bm{k}_{k’} \\bm{k}_{k’}^T\\right)\\right) \\end{align*} $$ RWKV-7P Show derivation of $S_N$ $$A_{i,j} = \\text{diag}(\\bm{w}_{i,j}) - \\bm{\\hat{\\kappa}}_{i,j}(\\bm{a}_{i,j} \\odot\\bm{\\hat{\\kappa}}_{i,j}^T) \\quad\\quad B_{i,j} = \\bm{v}_{i,j} \\bm{k}_{i,j}^T$$ Thus, $$ \\begin{align*} S_N \u0026= \\sum_{i=1}^N \\sum_{j=1}^{n_h} \\left( \\bm{v}_{i,j} \\bm{k}_{i,j}^T \\underline{\\left(\\prod_{j’=j+1}^{n_h} \\left(\\text{diag}(\\bm{w}_{i,j’}) - \\bm{\\hat{\\kappa}}_{i,j’}(\\bm{a}_{i,j’} \\odot\\bm{\\hat{\\kappa}}_{i,j’}^T)\\right)\\right) \\left(\\prod_{i’=i+1}^N \\prod_{j’=1}^{n_h} \\left(\\text{diag}(\\bm{w}_{i’,j’}) - \\bm{\\hat{\\kappa}}_{i’,j’}(\\bm{a}_{i’,j’} \\odot\\bm{\\hat{\\kappa}}_{i’,j’}^T)\\right) \\right)}\\right)\\\\ S_N \u0026= \\sum_{k=1}^{Nn_h} \\left(\\bm{v}_k \\bm{k}_k^T \\prod_{k’=k+1}^{Nn_h} \\left(\\text{diag}(\\bm{w}_k’) - \\bm{\\hat{\\kappa}}_k’(\\bm{a}_k’ \\odot\\bm{\\hat{\\kappa}}_k’^T)\\right)\\right) \\end{align*} $$ Chunk-wise Parallelism Since the update operations of linear attention mechanisms we discussed above are associative–i.e., the order in which we “combine” the updates doesn’t matter–we can perform the computations in multiple ways:\nThe Fully Recurrent Form where we update the state as we loop through the tokens/update matrices one by one, The Fully-Parallel Associative Scan Form where we hierarchically combine the updates in a tree-like structure, and The Chunk-wise Parallel Form (Hua et al., 2022; Sun et al., 2023) which is a compromise between the two where we divide the sequence into chunks first, combine intra-chunk updates in parallel, and then combine the chunk-level updates in a recurrent manner. At inference time, the recurrent form works best*. But at training time, we have to be more hardware-aware to squeeze out as much performance as possible. We will discuss more about this in a separate post. But for now, there are two important things to keep in mind:\nThe GPU Memory Hierarchy. NVIDIA GPUs have a “global”, high-bandwidth memory (HBM) that all threads in all processing units can access, and a smaller, shared memory (SMem) that threads in the same processing unit can access. The shared memory, being more “local”, has a much lower latency than the HBM. Thus, as much as possible, we want to limit communications between the processing units and the HBM and use the SMem instead. The Tensor Cores. Modern NVIDIA GPUs have tensor cores that can perform matrix multiplications much faster. Thus, ideally, we want to maximize the use of matrix multiplications and limit other operations. Now, parallel associative scan might seem the best choice, and indeed it already suffices for some architectures like Mamba 1. However, it requires a lot more (shared) memory and communication between the processing units (and therefore materialization to the HBM). And it also doesn’t fully utilize the tensor cores. But with chunk-wise parallelism, we only need to store the current state in the shared memory, and use matrix multiplications to compute the next chunk-level state. This way, we don’t have to materialize the $S_N$s to the HBM at all, and we can fully utilize the tensor cores. Hence why most flash linear attention kernels use chunk-wise parallelism.\n*At inference time, we need to process the input tokens first before generating outputs. This is called the “pre-filling” stage. And chunk-wise parallelism works better here. After that, we can then use the recurrent form to generate the outputs.\nA better way to think of chunk-wise parallelism is as multi-step online gradient descent, but instead of updating the state $n_h$ times per token, we update the state $n_c$ times per chunk where $n_c = N/C$ is the number of tokens per chunk and $C$ is the number of chunks. Thus, we just reuse our results from the previous section!\nTo make the connection more explicit, let’s reindex Equation $(1)$ as $[\\cdot]_i = [\\cdot]_{\\lceil i/n_c \\rceil,\\space (i-1) \\% n_c + 1}$: $$ \\begin{align*} S_N \u0026= \\begin{bmatrix} 0 \u0026 I \\end{bmatrix} \\begin{bmatrix} A_{1} \u0026 0 \\\\ B_{1} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_{2} \u0026 0 \\\\ B_{2} \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A_{n_c} \u0026 0 \\\\ B_{n_c} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_{n_c + 1} \u0026 0 \\\\ B_{n_c + 1} \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A_{N} \u0026 0 \\\\ B_{N} \u0026 I \\end{bmatrix} \\begin{bmatrix} I \\\\ 0 \\end{bmatrix}\\\\ S_N \u0026= \\begin{bmatrix} 0 \u0026 I \\end{bmatrix} \\begin{bmatrix} A_{1,1} \u0026 0 \\\\ B_{1,1} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_{1,2} \u0026 0 \\\\ B_{1,2} \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A_{1,n_c} \u0026 0 \\\\ B_{1,n_c} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_{2,1} \u0026 0 \\\\ B_{2,1} \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A_{C, n_c} \u0026 0 \\\\ B_{C, n_c} \u0026 I \\end{bmatrix} \\begin{bmatrix} I \\\\ 0 \\end{bmatrix}\\\\ \\end{align*} $$ where $A_{c,i}$ and $B_{c,i}$ are now the update matrices for the $i$-th token within the $c$-th chunk.\nAnd by combining the updates for each chunk as in Equation $(6)$ above, we get: $$ \\begin{align} \\begin{bmatrix} A^*_{c} \u0026 0 \\\\ B^*_{c} \u0026 I \\end{bmatrix} = \\prod_{i=1}^{n_c} \\begin{bmatrix} A_{c,i} \u0026 0 \\\\ B_{c,i} \u0026 I \\end{bmatrix} = \\begin{bmatrix} \\prod_{i=1}^{n_c} A_{c,i} \u0026 0 \\\\ \\sum_{i=1}^{n_c} \\left(B_{c,i} \\prod_{i’=i+1}^{n_c} A_{c,i’}\\right) \u0026 I \\end{bmatrix} \\end{align} $$ $$ S_C = \\underline{ \\begin{bmatrix} 0 \u0026 I \\end{bmatrix} \\begin{bmatrix} A^*_1 \u0026 0 \\\\ B^*_1 \u0026 I \\end{bmatrix} \\begin{bmatrix} A^*_2 \u0026 0 \\\\ B^*_2 \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A^*_{C-1} \u0026 0 \\\\ B^*_{C-1} \u0026 I \\end{bmatrix} } \\begin{bmatrix} A^*_C \u0026 0 \\\\ B^*_C \u0026 I \\end{bmatrix} \\begin{bmatrix} I \\\\ 0 \\end{bmatrix} $$ which has the equivalent cross-chunk recurrent form: $$ \\begin{align} \\begin{bmatrix} S_{C} \u0026 I \\end{bmatrix} \u0026= \\begin{bmatrix} S_{C-1} \u0026 I \\end{bmatrix} \\begin{bmatrix} A^*_C \u0026 0 \\\\ B^*_C \u0026 I \\end{bmatrix}\\\\ S_C \u0026= S_{C-1}A^*_C + B^*_C \\end{align} $$\nNow, let’s derive the $S_C$ for the linear attention mechanisms in the table above.\nChunk-wise Mamba 2 Show derivation of $S_C$ $$ \\begin{align*} A_{c,i} \u0026= \\text{diag}\\left(\\alpha_{c,i} I\\right) \u0026 B_{c,i} \u0026= \\bm{v}_{c,i} \\bm{k}_{c,i}^T\\\\ A^*_C \u0026= \\prod_{i=1}^{n_c} \\text{diag}\\left(\\alpha_{C,i} I\\right) \\quad \u0026 B^*_C \u0026= \\sum_{i=1}^{n_c} \\left(\\bm{v}_{C,i} \\bm{k}_{C,i}^T \\prod_{i’=i+1}^{n_c} \\text{diag}\\left(\\alpha_{C,i’} I\\right)\\right) \\end{align*} $$ Thus, from Equation $(13)$ above, $$ \\begin{align*} S_C \u0026= S_{C-1}A^*_C + B^*_C\\\\ S_C \u0026= S_{C-1} \\prod_{i=1}^{n_c} \\text{diag}\\left(\\alpha_{C,i} I\\right) + \\sum_{i=1}^{n_c} \\left(\\bm{v}_{C,i} \\bm{k}_{C,i}^T \\prod_{i’=i+1}^{n_c} \\text{diag}\\left(\\alpha_{C,i’} I\\right)\\right)\\\\ S_C \u0026= S_{C-1} \\prod_{i=1}^{n_c} \\alpha_{C,i} + \\sum_{i=1}^{n_c} \\left(\\prod_{i’=i+1}^{n_c} \\alpha_{C,i’}\\right) \\bm{v}_{C,i} \\bm{k}_{C,i}^T \\end{align*} $$ Chunk-wise DeltaNet Show derivation of $S_C$ $$ \\begin{align*} A_{c,i} \u0026= I - \\beta_{c,i} \\bm{k}_{c,i} \\bm{k}_{c,i}^T \u0026 B_{c,i} \u0026= \\beta_{c,i} \\bm{v}_{c,i} \\bm{k}_{c,i}^T\\\\ A^*_C \u0026= \\prod_{i=1}^{n_c} \\left(I - \\beta_{C,i} \\bm{k}_{C,i} \\bm{k}_{C,i}^T\\right) \\quad \u0026 B^*_C \u0026= \\sum_{i=1}^{n_c} \\left(\\beta_{C,i} \\bm{v}_{C,i} \\bm{k}_{C,i}^T \\prod_{i’=i+1}^{n_c} \\left(I - \\beta_{C,i’} \\bm{k}_{C,i’} \\bm{k}_{C,i’}^T\\right)\\right) \\end{align*} $$ Thus, $$ \\begin{align*} S_C \u0026= S_{C-1}A^*_C + B^*_C\\\\ S_C \u0026= S_{C-1} \\prod_{i=1}^{n_c} \\left(I - \\beta_{C,i} \\bm{k}_{C,i} \\bm{k}_{C,i}^T\\right) + \\sum_{i=1}^{n_c} \\left(\\beta_{C,i} \\bm{v}_{C,i} \\bm{k}_{C,i}^T \\prod_{i’=i+1}^{n_c} \\left(I - \\beta_{C,i’} \\bm{k}_{C,i’} \\bm{k}_{C,i’}^T\\right)\\right) \\end{align*} $$ Chunk-wise Gated DeltaNet Show derivation of $S_C$ $$ \\begin{align*} A_{c,i} \u0026= \\alpha_{c,i}(I - \\beta_{c,i} \\bm{k}_{c,i} \\bm{k}_{c,i}^T) \u0026 B_{c,i} \u0026= \\beta_{c,i} \\bm{v}_{c,i} \\bm{k}_{c,i}^T\\\\ A^*_C \u0026= \\prod_{i=1}^{n_c} \\alpha_{C,i} \\left(I - \\beta_{C,i} \\bm{k}_{C,i} \\bm{k}_{C,i}^T\\right) \\quad \u0026 B^*_C \u0026= \\sum_{i=1}^{n_c} \\left(\\beta_{C,i} \\bm{v}_{C,i} \\bm{k}_{C,i}^T \\prod_{i’=i+1}^{n_c} \\alpha_{C,i’} \\left(I - \\beta_{C,i’} \\bm{k}_{C,i’} \\bm{k}_{C,i’}^T\\right)\\right) \\end{align*} $$ Thus, $$ \\begin{align*} S_C \u0026= S_{C-1}A^*_C + B^*_C\\\\ S_C \u0026= S_{C-1} \\prod_{i=1}^{n_c} \\alpha_{C,i} \\left(I - \\beta_{C,i} \\bm{k}_{C,i} \\bm{k}_{C,i}^T\\right) + \\sum_{i=1}^{n_c} \\left(\\beta_{C,i} \\bm{v}_{C,i} \\bm{k}_{C,i}^T \\prod_{i’=i+1}^{n_c} \\alpha_{C,i’} \\left(I - \\beta_{C,i’} \\bm{k}_{C,i’} \\bm{k}_{C,i’}^T\\right)\\right)\\\\ S_C \u0026= S_{C-1} \\left(\\prod_{i=1}^{n_c} \\alpha_{C,i} \\right) \\left(\\prod_{i=1}^{n_c} \\left(I - \\beta_{C,i} \\bm{k}_{C,i} \\bm{k}_{C,i}^T\\right)\\right) + \\sum_{i=1}^{n_c} \\left(\\left(\\beta_{C,i} \\prod_{i’=i+1}^{n_c} \\alpha_{C,i’} \\right) \\bm{v}_{C,i} \\bm{k}_{C,i}^T \\prod_{i’=i+1}^{n_c} \\left(I - \\beta_{C,i’} \\bm{k}_{C,i’} \\bm{k}_{C,i’}^T\\right)\\right) \\end{align*} $$ Chunk-wise RWKV-7 Show derivation of $S_C$ $$ \\begin{align*} A_{c,i} \u0026= \\text{diag}\\left(\\bm{w}_{c,i}\\right) - \\bm{\\hat{\\kappa}}_{c,i}(\\bm{a}_{c,i} \\odot\\bm{\\hat{\\kappa}}_{c,i}^T) \u0026 B_{c,i} \u0026= \\bm{v}_{c,i} \\bm{k}_{c,i}^T\\\\ A^*_C \u0026= \\prod_{i=1}^{n_c} \\left(\\text{diag}\\left(\\bm{w}_{C,i}\\right) - \\bm{\\hat{\\kappa}}_{C,i}(\\bm{a}_{C,i} \\odot\\bm{\\hat{\\kappa}}_{C,i}^T)\\right) \\quad \u0026 B^*_C \u0026= \\sum_{i=1}^{n_c} \\left(\\bm{v}_{C,i} \\bm{k}_{C,i}^T \\prod_{i’=i+1}^{n_c} \\left(\\text{diag}\\left(\\bm{w}_{C,i’}\\right) - \\bm{\\hat{\\kappa}}_{C,i’}(\\bm{a}_{C,i’} \\odot\\bm{\\hat{\\kappa}}_{C,i’}^T)\\right)\\right) \\end{align*} $$ Thus, $$ \\begin{align*} S_C \u0026= S_{C-1}A^*_C + B^*_C\\\\ S_C \u0026= S_{C-1} \\prod_{i=1}^{n_c} \\left(\\text{diag}\\left(\\bm{w}_{C,i}\\right) - \\bm{\\hat{\\kappa}}_{C,i}(\\bm{a}_{C,i} \\odot\\bm{\\hat{\\kappa}}_{C,i}^T)\\right) + \\sum_{i=1}^{n_c} \\left(\\bm{v}_{C,i} \\bm{k}_{C,i}^T \\prod_{i’=i+1}^{n_c} \\left(\\text{diag}\\left(\\bm{w}_{C,i’}\\right) - \\bm{\\hat{\\kappa}}_{C,i’}(\\bm{a}_{C,i’} \\odot\\bm{\\hat{\\kappa}}_{C,i’}^T)\\right)\\right) \\end{align*} $$ Multi-Step Online Gradient Descent per Token with Chunk-wise Parallelism Let’s combine the two techniques we’ve discussed so far: multi-step online gradient descent per token and chunk-wise parallelism.\nThe strategy We can do this either way, but suppose we chunk the updates first then expand the each of the updates within the chunks into a product of $n_h$ updates. I.e., we have:\n$$ \\begin{bmatrix} A_{(c-1)*n_c + i} \u0026 0 \\\\ B_{(c-1)*n_c + i} \u0026 I \\end{bmatrix} \\xrightarrow{\\text{reindex}} \\begin{bmatrix} A_{c,i} \u0026 0 \\\\ B_{c,i} \u0026 I \\end{bmatrix} \\xrightarrow{\\text{expand}} \\begin{bmatrix} A_{c,i,1} \u0026 0 \\\\ B_{c,i,1} \u0026 I \\end{bmatrix} \\begin{bmatrix} A_{c,i,2} \u0026 0 \\\\ B_{c,i,2} \u0026 I \\end{bmatrix} \\cdots \\begin{bmatrix} A_{c,i,n_h} \u0026 0 \\\\ B_{c,i,n_h} \u0026 I \\end{bmatrix} $$ where $A_{c,i,j}$ and $B_{c,i,j}$ are the update matrices for the $j$-th gradient descent step for the $i$-th token within the $c$-th chunk.\nAnd from equations $(6)$, $(10)$, and $(11)$, we have: $$ \\begin{align*} \\begin{bmatrix} A^*_{c} \u0026 0 \\\\ B^*_{c} \u0026 I \\end{bmatrix} \u0026= \\prod_{i=1}^{n_c} \\begin{bmatrix} A’_{c,i} \u0026 0 \\\\ B’_{c,i} \u0026 I \\end{bmatrix} = \\prod_{i=1}^{n_c} \\prod_{j=1}^{n_h} \\begin{bmatrix} A_{c,i,j} \u0026 0 \\\\ B_{c,i,j} \u0026 I \\end{bmatrix}\\\\ \\begin{bmatrix} A^*_{c} \u0026 0 \\\\ B^*_{c} \u0026 I \\end{bmatrix} \u0026= \\begin{bmatrix} \\prod_{i=1}^{n_c} \\prod_{j=1}^{n_h} A_{c,i,j} \u0026 0 \\\\ \\sum_{i=1}^{n_c}\\sum_{j=1}^{n_h} \\left( B_{c,i,j} \\left(\\prod_{j’=j+1}^{n_h} A_{c,i,j’}\\right) \\left(\\prod_{i’=i+1}^{n_c} \\prod_{j=1}^{n_h} A_{c,i,j}\\right)\\right) \u0026 I \\end{bmatrix} \\end{align*} $$ Thus, $$ \\begin{align*} A^*_{c} \u0026= \\prod_{i=1}^{n_c} \\prod_{j=1}^{n_h} A_{c,i,j} \\\\ B^*_{c} \u0026= \\sum_{i=1}^{n_c}\\sum_{j=1}^{n_h} \\left( B_{c,i,j} \\left(\\prod_{j’=j+1}^{n_h} A_{c,i,j’}\\right) \\left(\\prod_{i’=i+1}^{n_c} \\prod_{j=1}^{n_h} A_{c,i,j}\\right)\\right) \\end{align*} $$ which we can then plug into Equation $(13)$ to get the cross-chunk recurrence:\n$$ \\begin{align*} S_C \u0026= S_{C-1}A^*_C + B^*_C\\\\ S_C \u0026= S_{C-1} \\prod_{i=1}^{n_c} \\prod_{j=1}^{n_h} A_{C,i,j} + \\sum_{i=1}^{n_c}\\sum_{j=1}^{n_h} \\left( B_{C,i,j} \\left(\\prod_{j’=j+1}^{n_h} A_{C,i,j’}\\right) \\left(\\prod_{i’=i+1}^{n_c} \\prod_{j=1}^{n_h} A_{C,i,j}\\right)\\right) \\end{align*} $$\nor, if we reindex this as $[\\cdot]_{C,k} = [\\cdot]_{C,\\space \\lceil k/n_h \\rceil,\\space (k-1) \\% n_h + 1}$, we get:\n$$ \\begin{align*} S_C \u0026= S_{C-1} \\prod_{k=1}^{n_c n_h} A_{C,k} + \\sum_{k=1}^{n_c n_h} \\left( B_{C,k} \\prod_{k’=k+1}^{n_c n_h} A_{C,k’}\\right) \\end{align*} $$\nAs an exercise, try deriving the cross-chunk recurrence for MambaSum, DeltaProduct, Gated DeltaProduct, and RWKV-7P.\nConclusion And that’s it!\nNot only is the blocked matrix formulation of linear attention mechanisms intuitive, it also makes the connections between different algorithms and computational forms much more obvious. I’d even go as far as to say that we now have the proper abstraction to do an evolutionary search for new linear attention mechanisms ;)\nIn the next post, we’ll talk about faster ways to calculate $A^*_{c}$ and $B^*_{c}$ for diagonal and diagonal-plus-low-rank $A^*_{c}$ using the WY Representations and the UT Transform. Stay tuned!\nAcknowledgements Big thanks to Songlin Yang, Julien Siems, and @Smerky, @BeeGass, @safelix, and @jacobbuckman for their feedback and discussions!\nHow to Cite @misc{cesista2025blockmatlinearattn, author = {Franz Louis Cesista}, title = {Blocked Matrix Formulation of Linear Attention Mechanisms}, year = {2025}, url = {https://leloykun.github.io/ponder/blockmat-linear-attn/}, } References Riccardo Grazzi, Julien Siems, Jörg K.H. Franke, Arber Zela, Frank Hutter, Massimiliano Pontil (2025). Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues. URL https://arxiv.org/abs/2411.12537 Julien Siems, Timur Carstensen, Arber Zela, Frank Hutter, Massimiliano Pontil, Riccardo Grazzi (2025). DeltaProduct: Increasing the Expressivity of DeltaNet Through Products of Householders. URL https://arxiv.org/abs/2502.10297 Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 5156–5165. PMLR, 2020b. URL http://proceedings.mlr.press/v119/katharopoulos20a.html. Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In Proceedings of the 41st International Conference on MachineLearning, volume 235 of Proceedingsof Machine Learning Research, pp. 10041–10071. PMLR, 2024b. URL https://proceedings.mlr.press/v235/dao24a.html. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim (2025). Parallelizing Linear Transformers with the Delta Rule over Sequence Length. URL https://arxiv.org/abs/2406.06484 Songlin Yang, Jan Kautz, Ali Hatamizadeh (2025). Gated Delta Networks: Improving Mamba2 with Delta Rule. URL https://arxiv.org/abs/2412.06464 Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 9099–9117. PMLR, 2022b. URL https://proceedings.mlr.press/v162/hua22a.html. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. ArXiv preprint, abs/2307.08621, 2023. URL https://arxiv.org/abs/2307.08621. Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Haowen Hou, Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, Nathan Wilce, Johan S. Wind, Tianyi Wu, Daniel Wuttke, Christian Zhou-Zheng (2025). RWKV-7 “Goose” with Expressive Dynamic State Evolution. URL https://arxiv.org/abs/2503.14456 ",
  "wordCount" : "3520",
  "inLanguage": "en",
  "datePublished": "2025-03-16T00:00:00Z",
  "dateModified": "2025-03-16T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Franz Louis Cesista"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://leloykun.github.io/ponder/blockmat-linear-attn/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Franz Louis Cesista",
    "logo": {
      "@type": "ImageObject",
      "url": "https://leloykun.github.io/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://leloykun.github.io/" accesskey="h" title="Franz Louis Cesista">
                <img src="https://leloykun.github.io/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Franz Louis Cesista</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://leloykun.github.io/cv.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/personal-projects/" title="Personal Projects">
                    <span>Personal Projects</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/ponder/" title="Ponder (Posts)">
                    <span>Ponder (Posts)</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Blocked Matrix Formulation of Linear Attention Mechanisms
    </h1>
    <div class="post-meta"><span title='2025-03-16 00:00:00 +0000 UTC'>March 16, 2025</span>&nbsp;&middot;&nbsp;Franz Louis Cesista&nbsp;&middot;&nbsp;<a href="https://x.com/leloykun/status/1901267939267162351" rel="noopener noreferrer" target="_blank">Crossposted from X (formerly Twitter)</a>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#recap-linear-attention-mechanisms">Recap: Linear Attention Mechanisms</a></li>
    <li><a href="#blocked-matrix-formulation-of-linear-attention-mechanisms">Blocked Matrix Formulation of Linear Attention Mechanisms</a></li>
    <li><a href="#one-step-online-gradient-descent-per-token">One-Step Online Gradient Descent per Token</a></li>
    <li><a href="#multi-step-online-gradient-descent-per-token">Multi-Step Online Gradient Descent per Token</a></li>
    <li><a href="#chunk-wise-parallelism">Chunk-wise Parallelism</a></li>
    <li><a href="#multi-step-online-gradient-descent-per-token-with-chunk-wise-parallelism">Multi-Step Online Gradient Descent per Token with Chunk-wise Parallelism</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#acknowledgements">Acknowledgements</a></li>
    <li><a href="#how-to-cite">How to Cite</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><p>In the <a href="../test-time-regression/">previous post</a>, we derived several linear attention mechanisms from scratch by formulating them as test-time online regression problems. Here, we&rsquo;ll discuss a more intuitive way to represent the update rules of the internal states of these linear attention mechanisms using a blocked matrix formulation. Then, we&rsquo;ll discuss how to use it to (1) derive the update rules for linear attention mechanisms that take multiple gradient descent steps per token and (2) derive the update rules for chunk-wise parallelism of already-existing linear attention mechanisms.</p>
<h2 id="recap-linear-attention-mechanisms">Recap: Linear Attention Mechanisms<a hidden class="anchor" aria-hidden="true" href="#recap-linear-attention-mechanisms">#</a></h2>
<p>Linear attention mechanisms typically have an update rule of the form:
$$S_i = S_{i-1}A_i + B_i$$
where $S_{i-1}$ is the (old) state after processing the first $i-1$ tokens, $S_i$ is the (new) state after processing the first $i$ tokens, and $A_i$ and $B_i$ are update matrices. Think of $A_i$ as an operation that <em>modifies</em> some information already stored in the state while $B_i$ <em>adds</em> new information to the state. In most cases where $A_i \neq I$, $A_i$ typically <em>removes</em> some (old) information from the state. But if we allow $A_i$ to have negative eigenvalues, then we can also think of it as an operation that, in a sense, <em>inverts</em> information instead.</p>
<p>Here are a couple of examples:</p>
<table>
<thead>
<tr>
<th>Linear Attention Mechanism</th>
<th style="text-align:right"><strong>$A_i$</strong></th>
<th style="text-align:right"><strong>$B_i$</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Vanilla Linear Attention</td>
<td style="text-align:right">$I$</td>
<td style="text-align:right">$\bm{v}_i \bm{k}_i^T$</td>
</tr>
<tr>
<td>Mamba 2</td>
<td style="text-align:right">$\text{diag}\left(\alpha_i I\right)$</td>
<td style="text-align:right">$\bm{v}_i \bm{k}_i^T$</td>
</tr>
<tr>
<td>DeltaNet</td>
<td style="text-align:right">$I - \beta_i \bm{k}_i \bm{k}_i^T$</td>
<td style="text-align:right">$\beta_i \bm{v}_i \bm{k}_i^T$</td>
</tr>
<tr>
<td>Gated DeltaNet</td>
<td style="text-align:right">$\alpha_i(I - \beta_i \bm{k}_i \bm{k}_i^T)$</td>
<td style="text-align:right">$\beta_i \bm{v}_i \bm{k}_i^T$</td>
</tr>
<tr>
<td>RWKV-7</td>
<td style="text-align:right">$\text{diag}(\bm{w}_i) - \bm{\hat{\kappa}}_i(\bm{a}_i \odot\bm{\hat{\kappa}}_i^T)$</td>
<td style="text-align:right">$\bm{v}_i \bm{k}_i^T$</td>
</tr>
</tbody>
</table>
<p>where $\bm{k}_i  \in \mathbb{R}^{d_k}$ and $\bm{v}_i \in \mathbb{R}^{d_v}$ are the corresponding key-value pair for the $i$-th token; $\alpha_i \in [0, 1]$ can be thought of as a date-dependent weight decay that controls how much of the previous state to keep or forget; and $\beta_i \in [0, 1]$ can be thought of as a date-dependent learning rate that controls how much of the new information to add to the state.</p>
<p>If we let $\alpha_i \in [-1, 1]$ for Mamba 2 and $\beta_i \in [0, 2]$ for (Gated) DeltaNet, then $A_i$ can have negative eigenvalues while still having norm $||A_i|| \leq 1$. This allows the models to learn more complex patterns while maintaining training stability (Grazzi et al., 2025).</p>
<h2 id="blocked-matrix-formulation-of-linear-attention-mechanisms">Blocked Matrix Formulation of Linear Attention Mechanisms<a hidden class="anchor" aria-hidden="true" href="#blocked-matrix-formulation-of-linear-attention-mechanisms">#</a></h2>
<p>Notice that we can rewrite the update rule above as,</p>
<p>$$
\begin{align*}
S_i &amp;= S_{i-1}A_i + B_i\\
S_{i} &amp;=
\begin{bmatrix}
S_{i-1} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_i \\
B_i
\end{bmatrix}
\end{align*}
$$
or, equivalently,
$$
\begin{bmatrix}
S_{i} &amp; I
\end{bmatrix} =
\begin{bmatrix}
S_{i-1} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_i &amp; 0 \\
B_i &amp; I
\end{bmatrix}
$$</p>
<p>At training time, we need <em>all</em> of the intermediary states, not just the final state. Thus, we need an efficient way to compute $S_N$ for all token indices $N$. To do this, let&rsquo;s unroll the recurrence above:</p>
<p>$$
\begin{align*}
\begin{bmatrix}
S_{N} &amp; I
\end{bmatrix} &amp;=
\begin{bmatrix}
S_{N-1} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_N &amp; 0 \\
B_N &amp; I
\end{bmatrix}\\
\begin{bmatrix}
S_{N} &amp; I
\end{bmatrix} &amp;=
\begin{bmatrix}
S_{N-2} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_{N-1} &amp; 0 \\
B_{N-1} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_N &amp; 0 \\
B_N &amp; I
\end{bmatrix}\\
&amp;\vdots\\
\begin{bmatrix}
S_{N} &amp; I
\end{bmatrix} &amp;=
\begin{bmatrix}
S_{0} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_1 &amp; 0 \\
B_1 &amp; I
\end{bmatrix}
\begin{bmatrix}
A_2 &amp; 0 \\
B_2 &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A_N &amp; 0 \\
B_N &amp; I
\end{bmatrix}\\
S_N &amp;=
\begin{bmatrix}
S_{0} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_1 &amp; 0 \\
B_1 &amp; I
\end{bmatrix}
\begin{bmatrix}
A_2 &amp; 0 \\
B_2 &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A_N &amp; 0 \\
B_N &amp; I
\end{bmatrix}
\begin{bmatrix}
I \\
0
\end{bmatrix}
\end{align*}
$$</p>
<p>In practice, we usually initialize $S_0$ as the zero matrix. Thus,</p>
<p>$$
\begin{align}
S_N &amp;=
\begin{bmatrix}
0 &amp; I
\end{bmatrix}
\begin{bmatrix}
A_1 &amp; 0 \\
B_1 &amp; I
\end{bmatrix}
\begin{bmatrix}
A_2 &amp; 0 \\
B_2 &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A_N &amp; 0 \\
B_N &amp; I
\end{bmatrix}
\begin{bmatrix}
I \\
0
\end{bmatrix}\\
S_N &amp;=
\begin{bmatrix}
0 &amp; I
\end{bmatrix}
\begin{bmatrix}
\prod_{i=1}^{N} A_i &amp; 0 \\
\sum_{i=1}^{N} \left(B_i \prod_{j=i+1}^{N} A_j\right) &amp; I
\end{bmatrix}
\begin{bmatrix}
I \\
0
\end{bmatrix}\\
S_N &amp;= \sum_{i=1}^{N} \left(B_i \prod_{j=i+1}^{N} A_j\right)
\end{align}
$$
where $(1) \rightarrow (2)$ can be proven by induction.</p>
<p>Equation $(1)$ makes it obvious <em>why</em> and <em>how</em> we can parallelize computation of $S_N$, for all $N$, at training time: the updates are merely (blocked) matrix multiplications; matrix multiplications are associative; thus, we can use the (fully-parallel) associative scan algorithm to compute all the intermediary states in $O(N)$ time!</p>
<h2 id="one-step-online-gradient-descent-per-token">One-Step Online Gradient Descent per Token<a hidden class="anchor" aria-hidden="true" href="#one-step-online-gradient-descent-per-token">#</a></h2>
<p>Let&rsquo;s derive $S_N$ for each of the linear attention mechanisms in the table above.</p>
<h3 id="vanilla-linear-attention">Vanilla Linear Attention<a hidden class="anchor" aria-hidden="true" href="#vanilla-linear-attention">#</a></h3>


<p><details  open=true >
  <summary markdown="span">Show derivation of $S_N$</summary>
  $$A_i = I \quad\quad B_i = \bm{v}_i \bm{k}_i^T$$
From Equation $(3)$ above, we get:
$$
\begin{align*}
S_N &amp;= \sum_{i=1}^{N} \left(\bm{v}_i \bm{k}_i^T \prod_{j=i+1}^{N} I\right)\\
S_N &amp;= \sum_{i=1}^{N} \bm{v}_i \bm{k}_i^T
\end{align*}
$$
</details></p>

<h3 id="mamba-2">Mamba 2<a hidden class="anchor" aria-hidden="true" href="#mamba-2">#</a></h3>


<p><details >
  <summary markdown="span">Show derivation of $S_N$</summary>
  $$A_i = \text{diag}\left(\alpha_i I\right) \quad\quad B_i = \bm{v}_i \bm{k}_i^T$$
Thus,
$$
\begin{align*}
S_N &amp;= \sum_{i=1}^{N} \left(\bm{v}_i \bm{k}_i^T \prod_{j=i+1}^{N} \text{diag}\left(\alpha_j I\right)\right)\\
S_N &amp;= \sum_{i=1}^{N} \left( \prod_{j=i+1}^{N} \alpha_j \right) \bm{v}_i \bm{k}_i^T
\end{align*}
$$
</details></p>

<h3 id="deltanet">DeltaNet<a hidden class="anchor" aria-hidden="true" href="#deltanet">#</a></h3>


<p><details >
  <summary markdown="span">Show derivation of $S_N$</summary>
  $$A_i = I - \beta_i \bm{k}_i \bm{k}_i^T \quad\quad B_i = \beta_i \bm{v}_i \bm{k}_i^T$$
Thus,
$$S_N = \sum_{i=1}^{N} \left(\beta_i \bm{v}_i \bm{k}_i^T \prod_{j=i+1}^{N} \left(I - \beta_j \bm{k}_j \bm{k}_j^T\right)\right)$$
</details></p>

<h3 id="gated-deltanet">Gated DeltaNet<a hidden class="anchor" aria-hidden="true" href="#gated-deltanet">#</a></h3>


<p><details >
  <summary markdown="span">Show derivation of $S_N$</summary>
  $$A_i = \alpha_i(I - \beta_i \bm{k}_i \bm{k}_i^T) \quad\quad B_i = \beta_i \bm{v}_i \bm{k}_i^T$$
Thus,
$$
\begin{align*}
S_N &amp;= \sum_{i=1}^{N} \left(\beta_i \bm{v}_i \bm{k}_i^T \prod_{j=i+1}^{N} \alpha_j \left(I - \beta_j \bm{k}_j \bm{k}_j^T\right)\right)\\
S_N &amp;= \sum_{i=1}^{N} \left(\left(\beta_i \prod_{j=i+1}^{N} \alpha_j \right) \bm{v}_i \bm{k}_i^T \prod_{j=i+1}^{N} \left(I - \beta_j \bm{k}_j \bm{k}_j^T\right)\right)
\end{align*}
$$
</details></p>

<h3 id="rwkv-7">RWKV-7<a hidden class="anchor" aria-hidden="true" href="#rwkv-7">#</a></h3>


<p><details >
  <summary markdown="span">Show derivation of $S_N$</summary>
  $$A_i = \text{diag}(\bm{w}_i) - \bm{\hat{\kappa}}_i(\bm{a}_i \odot\bm{\hat{\kappa}}_i^T) \quad\quad B_i = \bm{v}_i \bm{k}_i^T$$
Thus,
$$S_N = \sum_{i=1}^{N} \left(\bm{v}_i \bm{k}_i^T \prod_{j=i+1}^{N} \left(\text{diag}(\bm{w}_j) - \bm{\hat{\kappa}}_j(\bm{a}_j \odot\bm{\hat{\kappa}}_j^T)\right)\right)$$
</details></p>

<p>Easy!</p>
<hr>
<h2 id="multi-step-online-gradient-descent-per-token">Multi-Step Online Gradient Descent per Token<a hidden class="anchor" aria-hidden="true" href="#multi-step-online-gradient-descent-per-token">#</a></h2>
<p>Now, what if we take $n_h$ gradient descent steps per token?</p>
<p>To do this, we can follow the procedure outlined in the DeltaProduct (Siems et al., 2025) paper where they:</p>
<ol>
<li>Recurrently generate $n_h$ key-value pairs for each input token,</li>
<li>Update the state using the $n_h$ key-value pairs, and</li>
<li>Keep only the final key-value pair and discard the rest.</li>
</ol>
<p>In our formulation, this is equivalent to replacing each update with a product of $n_h$ updates:</p>
<p>$$
\begin{bmatrix}
A_{i} &amp; 0 \\
B_{i} &amp; I
\end{bmatrix}
\longrightarrow
\begin{bmatrix}
A_{i,1} &amp; 0 \\
B_{i,1} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_{i,2} &amp; 0 \\
B_{i,2} &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A_{i,n_h} &amp; 0 \\
B_{i,n_h} &amp; I
\end{bmatrix}
$$
where $A_{i,j}$ and $B_{i,j}$ are the update matrices for the $j$-th gradient descent step for the $i$-th token.</p>
<p>Thus, Equation $(1)$ becomes:
$$
\begin{align}
S_N =
\begin{bmatrix}
0 &amp; I
\end{bmatrix}
\begin{bmatrix}
A_{1,1} &amp; 0 \\
B_{1,1} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_{1,2} &amp; 0 \\
B_{1,2} &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A_{1,n_h} &amp; 0 \\
B_{1,n_h} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_{2,1} &amp; 0 \\
B_{2,1} &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A_{N, n_h} &amp; 0 \\
B_{N, n_h} &amp; I
\end{bmatrix}
\begin{bmatrix}
I \\
0
\end{bmatrix}
\end{align}
$$</p>
<p>And if we reindex this as $[\cdot]_k = [\cdot]_{\lceil k/n_h \rceil,\space (k-1) \% n_h + 1}$, then from equation $(3)$ above, we get:
$$
\begin{align}
S_N = \sum_{k=1}^{Nn_h} \left( B_k \prod_{k&rsquo;=k+1}^{Nn_h} A_{k&rsquo;}\right)
\end{align}
$$</p>
<p>Alternatively, we can also combine the updates for each token into a single update matrix first before multiplying them together:</p>
<p>$$
\begin{align}
\begin{bmatrix}
A&rsquo;_{i} &amp; 0 \\
B&rsquo;_{i} &amp; I
\end{bmatrix}
= \prod_{j=1}^{n_h}
\begin{bmatrix}
A_{i,j} &amp; 0 \\
B_{i,j} &amp; I
\end{bmatrix}
= \begin{bmatrix}
\prod_{j=1}^{n_h} A_{i,j} &amp; 0 \\
\sum_{j=1}^{n_h} \left(B_{i,j} \prod_{j&rsquo;=j+1}^{n_h} A_{i,j&rsquo;}\right) &amp; I
\end{bmatrix}
\end{align}
$$</p>
<p>$$
\begin{align}
S_N &amp;=
\begin{bmatrix}
0 &amp; I
\end{bmatrix}
\begin{bmatrix}
A&rsquo;_1 &amp; 0 \\
B&rsquo;_1 &amp; I
\end{bmatrix}
\begin{bmatrix}
A&rsquo;_2 &amp; 0 \\
B&rsquo;_2 &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A&rsquo;_N &amp; 0 \\
B&rsquo;_N &amp; I
\end{bmatrix}
\begin{bmatrix}
I \\
0
\end{bmatrix}\\
S_N &amp;=
\begin{bmatrix}
0 &amp; I
\end{bmatrix}
\begin{bmatrix}
\prod_{i=1}^N A&rsquo;_i &amp; 0 \\
\sum_{i=1}^N \left( B&rsquo;_i \prod_{i&rsquo;=i+1}^N A&rsquo;_{i&rsquo;} \right) &amp; I
\end{bmatrix}
\begin{bmatrix}
I \\
0
\end{bmatrix}\\
S_N &amp;= \sum_{i=1}^N \left( B&rsquo;_i \prod_{i&rsquo;=i+1}^N A&rsquo;_{i&rsquo;} \right)\\
S_N &amp;= \sum_{i=1}^N \sum_{j=1}^{n_h} \left( B_{i,j} \underline{\left(\prod_{j&rsquo;=j+1}^{n_h} A_{i,j&rsquo;}\right) \left(\prod_{i&rsquo;=i+1}^N \prod_{j&rsquo;=1}^{n_h} A_{i&rsquo;,j&rsquo;} \right)}\right)
\end{align}
$$</p>
<p>which, again, if we reindex this as $[\cdot]_k = [\cdot]_{\lceil k/n_h \rceil,\space (k-1) \% n_h + 1}$, we get:</p>
<p>$$S_N = \sum_{k=1}^{Nn_h} \left( B_k \prod_{k&rsquo;=k+1}^{Nn_h} A_{k&rsquo;}\right)$$
as expected.</p>
<hr>
<p>Now, let&rsquo;s derive the $S_N$ for the linear attention mechanisms in the table above, but this time, with $n_h$ gradient descent steps per token.</p>
<h3 id="mambasum">MambaSum*<a hidden class="anchor" aria-hidden="true" href="#mambasum">#</a></h3>


<p><details  open=true >
  <summary markdown="span">Show derivation of $S_N$</summary>
  $$A_{i,j} = \text{diag}\left(\alpha_{i,j} I\right) \quad\quad B_{i,j} = \bm{v}_{i,j} \bm{k}_{i,j}^T$$
Thus, from Equation $(10)$ above,
$$
\begin{align*}
S_N &amp;= \sum_{i=1}^N \sum_{j=1}^{n_h} \left( \bm{v}_{i,j} \bm{k}_{i,j}^T \left(\prod_{j&rsquo;=j+1}^{n_h} \text{diag}\left(\alpha_{i,j&rsquo;} I\right)\right) \left(\prod_{i&rsquo;=i+1}^N \prod_{j&rsquo;=1}^{n_h} \text{diag}\left(\alpha_{i&rsquo;,j&rsquo;} I\right) \right)\right)\\
S_N &amp;= \sum_{i=1}^N \sum_{j=1}^{n_h} \left(\underline{\left( \prod_{j&rsquo;=j+1}^{n_h} \alpha_{i,j&rsquo;}\right) \left(\prod_{i&rsquo;=i+1}^N \prod_{j&rsquo;=1}^{n_h} \alpha_{i&rsquo;,j&rsquo;} \right)} \right) \bm{v}_{i,j} \bm{k}_{i,j}^T\\
S_N &amp;= \sum_{k=1}^{Nn_h} \left(\prod_{k&rsquo;=k+1}^{Nn_h} \alpha_{k&rsquo;}\right) \bm{v}_k \bm{k}_k^T
\end{align*}
$$
</details></p>

<blockquote>
<p>*I&rsquo;m not actually sure if MambaSum already exists under a different name. If it does, please let me know!</p>
</blockquote>
<h3 id="deltaproduct">DeltaProduct<a hidden class="anchor" aria-hidden="true" href="#deltaproduct">#</a></h3>


<p><details >
  <summary markdown="span">Show derivation of $S_N$</summary>
  $$A_{i,j} = I - \beta_{i,j} \bm{k}_{i,j} \bm{k}_{i,j}^T \quad\quad B_{i,j} = \beta_{i,j} \bm{v}_{i,j} \bm{k}_{i,j}^T$$
Thus,
$$
\begin{align*}
S_N &amp;= \sum_{i=1}^N \sum_{j=1}^{n_h} \left( \beta_{i,j} \bm{v}_{i,j} \bm{k}_{i,j}^T \underline{\left(\prod_{j&rsquo;=j+1}^{n_h} \left(I - \beta_{i,j&rsquo;} \bm{k}_{i,j&rsquo;} \bm{k}_{i,j&rsquo;}^T\right)\right) \left(\prod_{i&rsquo;=i+1}^N \prod_{j&rsquo;=1}^{n_h} \left(I - \beta_{i&rsquo;,j&rsquo;} \bm{k}_{i&rsquo;,j&rsquo;} \bm{k}_{i&rsquo;,j&rsquo;}^T\right) \right)}\right)\\
S_N &amp;= \sum_{k=1}^{Nn_h} \left(\beta_k \bm{v}_k \bm{k}_k^T \prod_{k&rsquo;=k+1}^{Nn_h} \left(I - \beta_{k&rsquo;} \bm{k}_{k&rsquo;} \bm{k}_{k&rsquo;}^T\right)\right)
\end{align*}
$$
</details></p>

<h3 id="gated-deltaproduct">Gated DeltaProduct<a hidden class="anchor" aria-hidden="true" href="#gated-deltaproduct">#</a></h3>


<p><details >
  <summary markdown="span">Show derivation of $S_N$</summary>
  $$A_{i,j} = \alpha_{i,j}(I - \beta_{i,j} \bm{k}_{i,j} \bm{k}_{i,j}^T) \quad\quad B_{i,j} = \beta_{i,j} \bm{v}_{i,j} \bm{k}_{i,j}^T$$
Thus,
$$
\begin{align*}
S_N &amp;= \sum_{i=1}^N \sum_{j=1}^{n_h} \left( \beta_{i,j} \bm{v}_{i,j} \bm{k}_{i,j}^T \underline{\left(\prod_{j&rsquo;=j+1}^{n_h} \alpha_{i,j&rsquo;} \left(I - \beta_{i,j&rsquo;} \bm{k}_{i,j&rsquo;} \bm{k}_{i,j&rsquo;}^T\right)\right) \left(\prod_{i&rsquo;=i+1}^N \prod_{j&rsquo;=1}^{n_h} \alpha_{i&rsquo;,j&rsquo;} \left(I - \beta_{i&rsquo;,j&rsquo;} \bm{k}_{i&rsquo;,j&rsquo;} \bm{k}_{i&rsquo;,j&rsquo;}^T\right) \right)}\right)\\
S_N &amp;= \sum_{k=1}^{Nn_h} \left(\beta_k \bm{v}_k \bm{k}_k^T \prod_{k&rsquo;=k+1}^{Nn_h} \alpha_{k&rsquo;} \left(I - \beta_{k&rsquo;} \bm{k}_{k&rsquo;} \bm{k}_{k&rsquo;}^T\right)\right)\\
S_N &amp;= \sum_{k=1}^{Nn_h} \left(\left( \beta_k \prod_{k&rsquo;=k+1}^{Nn_h} \alpha_{k&rsquo;} \right) \bm{v}_k \bm{k}_k^T \prod_{k&rsquo;=k+1}^{Nn_h} \left(I - \beta_{k&rsquo;} \bm{k}_{k&rsquo;} \bm{k}_{k&rsquo;}^T\right)\right)
\end{align*}
$$
</details></p>

<h3 id="rwkv-7p">RWKV-7P<a hidden class="anchor" aria-hidden="true" href="#rwkv-7p">#</a></h3>


<p><details >
  <summary markdown="span">Show derivation of $S_N$</summary>
  $$A_{i,j} = \text{diag}(\bm{w}_{i,j}) - \bm{\hat{\kappa}}_{i,j}(\bm{a}_{i,j} \odot\bm{\hat{\kappa}}_{i,j}^T) \quad\quad B_{i,j} = \bm{v}_{i,j} \bm{k}_{i,j}^T$$
Thus,
$$
\begin{align*}
S_N &amp;= \sum_{i=1}^N \sum_{j=1}^{n_h} \left( \bm{v}_{i,j} \bm{k}_{i,j}^T \underline{\left(\prod_{j&rsquo;=j+1}^{n_h} \left(\text{diag}(\bm{w}_{i,j&rsquo;}) - \bm{\hat{\kappa}}_{i,j&rsquo;}(\bm{a}_{i,j&rsquo;} \odot\bm{\hat{\kappa}}_{i,j&rsquo;}^T)\right)\right) \left(\prod_{i&rsquo;=i+1}^N \prod_{j&rsquo;=1}^{n_h} \left(\text{diag}(\bm{w}_{i&rsquo;,j&rsquo;}) - \bm{\hat{\kappa}}_{i&rsquo;,j&rsquo;}(\bm{a}_{i&rsquo;,j&rsquo;} \odot\bm{\hat{\kappa}}_{i&rsquo;,j&rsquo;}^T)\right) \right)}\right)\\
S_N &amp;= \sum_{k=1}^{Nn_h} \left(\bm{v}_k \bm{k}_k^T \prod_{k&rsquo;=k+1}^{Nn_h} \left(\text{diag}(\bm{w}_k&rsquo;) - \bm{\hat{\kappa}}_k&rsquo;(\bm{a}_k&rsquo; \odot\bm{\hat{\kappa}}_k&rsquo;^T)\right)\right)
\end{align*}
$$
</details></p>

<hr>
<h2 id="chunk-wise-parallelism">Chunk-wise Parallelism<a hidden class="anchor" aria-hidden="true" href="#chunk-wise-parallelism">#</a></h2>
<div align="center">
    <img src="../test-time-regression/linear-attn-comp-forms.png" style="width:75%; height:75%;" />
</div>
<p>Since the update operations of linear attention mechanisms we discussed above are associative&ndash;i.e., the order in which we &ldquo;combine&rdquo; the updates doesn&rsquo;t matter&ndash;we can perform the computations in multiple ways:</p>
<ol>
<li>The <strong>Fully Recurrent Form</strong> where we update the state as we loop through the tokens/update matrices one by one,</li>
<li>The <strong>Fully-Parallel Associative Scan Form</strong> where we hierarchically combine the updates in a tree-like structure, and</li>
<li>The <strong>Chunk-wise Parallel Form</strong> (Hua et al., 2022; Sun et al., 2023) which is a compromise between the two where we divide the sequence into chunks first, combine intra-chunk updates in parallel, and then combine the chunk-level updates in a recurrent manner.</li>
</ol>
<p>At inference time, the recurrent form works best*. But at training time, we have to be more hardware-aware to squeeze out as much performance as possible. We will discuss more about this in a separate post. But for now, there are two important things to keep in mind:</p>
<ol>
<li>The <strong>GPU Memory Hierarchy</strong>. NVIDIA GPUs have a &ldquo;global&rdquo;, high-bandwidth memory (HBM) that all threads in all processing units can access, and a smaller, shared memory (SMem) that threads in the same processing unit can access. The shared memory, being more &ldquo;local&rdquo;, has a much lower latency than the HBM. Thus, as much as possible, we want to limit communications between the processing units and the HBM and use the SMem instead.</li>
<li>The <strong>Tensor Cores</strong>. Modern NVIDIA GPUs have tensor cores that can perform matrix multiplications much faster. Thus, ideally, we want to maximize the use of matrix multiplications and limit other operations.</li>
</ol>
<p>Now, parallel associative scan might seem the best choice, and indeed it already suffices for some architectures like Mamba 1. However, it requires a lot more (shared) memory and communication between the processing units (and therefore materialization to the HBM). And it also doesn&rsquo;t fully utilize the tensor cores. But with chunk-wise parallelism, we only need to store the current state in the shared memory, and use matrix multiplications to compute the next chunk-level state. This way, we don&rsquo;t have to materialize the $S_N$s to the HBM at all, and we can fully utilize the tensor cores. Hence why most flash linear attention kernels use chunk-wise parallelism.</p>
<blockquote>
<p>*At inference time, we need to process the input tokens first before generating outputs. This is called the &ldquo;pre-filling&rdquo; stage. And chunk-wise parallelism works better here. After that, we can then use the recurrent form to generate the outputs.</p>
</blockquote>
<hr>
<p>A better way to think of chunk-wise parallelism is as multi-step online gradient descent, but instead of updating the state $n_h$ times per token, we update the state $n_c$ times per chunk where $n_c = N/C$ is the number of tokens per chunk and $C$ is the number of chunks. Thus, we just reuse our results from the previous section!</p>
<p>To make the connection more explicit, let&rsquo;s reindex Equation $(1)$ as $[\cdot]_i = [\cdot]_{\lceil i/n_c \rceil,\space (i-1) \% n_c + 1}$:
$$
\begin{align*}
S_N &amp;=
\begin{bmatrix}
0 &amp; I
\end{bmatrix}
\begin{bmatrix}
A_{1} &amp; 0 \\
B_{1} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_{2} &amp; 0 \\
B_{2} &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A_{n_c} &amp; 0 \\
B_{n_c} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_{n_c + 1} &amp; 0 \\
B_{n_c + 1} &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A_{N} &amp; 0 \\
B_{N} &amp; I
\end{bmatrix}
\begin{bmatrix}
I \\
0
\end{bmatrix}\\
S_N &amp;=
\begin{bmatrix}
0 &amp; I
\end{bmatrix}
\begin{bmatrix}
A_{1,1} &amp; 0 \\
B_{1,1} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_{1,2} &amp; 0 \\
B_{1,2} &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A_{1,n_c} &amp; 0 \\
B_{1,n_c} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_{2,1} &amp; 0 \\
B_{2,1} &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A_{C, n_c} &amp; 0 \\
B_{C, n_c} &amp; I
\end{bmatrix}
\begin{bmatrix}
I \\
0
\end{bmatrix}\\
\end{align*}
$$
where $A_{c,i}$ and $B_{c,i}$ are now the update matrices for the $i$-th token within the $c$-th chunk.</p>
<p>And by combining the updates for each chunk as in Equation $(6)$ above, we get:
$$
\begin{align}
\begin{bmatrix}
A^*_{c} &amp; 0 \\
B^*_{c} &amp; I
\end{bmatrix}
= \prod_{i=1}^{n_c} \begin{bmatrix}
A_{c,i} &amp; 0 \\
B_{c,i} &amp; I
\end{bmatrix}
= \begin{bmatrix}
\prod_{i=1}^{n_c} A_{c,i} &amp; 0 \\
\sum_{i=1}^{n_c} \left(B_{c,i} \prod_{i&rsquo;=i+1}^{n_c} A_{c,i&rsquo;}\right) &amp; I
\end{bmatrix}
\end{align}
$$
$$
S_C =
\underline{
\begin{bmatrix}
0 &amp; I
\end{bmatrix}
\begin{bmatrix}
A^*_1 &amp; 0 \\
B^*_1 &amp; I
\end{bmatrix}
\begin{bmatrix}
A^*_2 &amp; 0 \\
B^*_2 &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A^*_{C-1} &amp; 0 \\
B^*_{C-1} &amp; I
\end{bmatrix}
}
\begin{bmatrix}
A^*_C &amp; 0 \\
B^*_C &amp; I
\end{bmatrix}
\begin{bmatrix}
I \\
0
\end{bmatrix}
$$
which has the equivalent cross-chunk recurrent form:
$$
\begin{align}
\begin{bmatrix}
S_{C} &amp; I
\end{bmatrix} &amp;=
\begin{bmatrix}
S_{C-1} &amp; I
\end{bmatrix}
\begin{bmatrix}
A^*_C &amp; 0 \\
B^*_C &amp; I
\end{bmatrix}\\
S_C &amp;= S_{C-1}A^*_C + B^*_C
\end{align}
$$</p>
<hr>
<p>Now, let&rsquo;s derive the $S_C$ for the linear attention mechanisms in the table above.</p>
<h3 id="chunk-wise-mamba-2">Chunk-wise Mamba 2<a hidden class="anchor" aria-hidden="true" href="#chunk-wise-mamba-2">#</a></h3>


<p><details  open=true >
  <summary markdown="span">Show derivation of $S_C$</summary>
  $$
\begin{align*}
A_{c,i} &amp;= \text{diag}\left(\alpha_{c,i} I\right) &amp; B_{c,i} &amp;= \bm{v}_{c,i} \bm{k}_{c,i}^T\\
A^*_C &amp;= \prod_{i=1}^{n_c} \text{diag}\left(\alpha_{C,i} I\right) \quad &amp; B^*_C &amp;= \sum_{i=1}^{n_c} \left(\bm{v}_{C,i} \bm{k}_{C,i}^T \prod_{i&rsquo;=i+1}^{n_c} \text{diag}\left(\alpha_{C,i&rsquo;} I\right)\right)
\end{align*}
$$
Thus, from Equation $(13)$ above,
$$
\begin{align*}
S_C &amp;= S_{C-1}A^*_C + B^*_C\\
S_C &amp;= S_{C-1} \prod_{i=1}^{n_c} \text{diag}\left(\alpha_{C,i} I\right) + \sum_{i=1}^{n_c} \left(\bm{v}_{C,i} \bm{k}_{C,i}^T \prod_{i&rsquo;=i+1}^{n_c} \text{diag}\left(\alpha_{C,i&rsquo;} I\right)\right)\\
S_C &amp;= S_{C-1} \prod_{i=1}^{n_c} \alpha_{C,i} + \sum_{i=1}^{n_c} \left(\prod_{i&rsquo;=i+1}^{n_c} \alpha_{C,i&rsquo;}\right) \bm{v}_{C,i} \bm{k}_{C,i}^T
\end{align*}
$$
</details></p>

<h3 id="chunk-wise-deltanet">Chunk-wise DeltaNet<a hidden class="anchor" aria-hidden="true" href="#chunk-wise-deltanet">#</a></h3>


<p><details >
  <summary markdown="span">Show derivation of $S_C$</summary>
  $$
\begin{align*}
A_{c,i} &amp;= I - \beta_{c,i} \bm{k}_{c,i} \bm{k}_{c,i}^T &amp; B_{c,i} &amp;= \beta_{c,i} \bm{v}_{c,i} \bm{k}_{c,i}^T\\
A^*_C &amp;= \prod_{i=1}^{n_c} \left(I - \beta_{C,i} \bm{k}_{C,i} \bm{k}_{C,i}^T\right) \quad &amp; B^*_C &amp;= \sum_{i=1}^{n_c} \left(\beta_{C,i} \bm{v}_{C,i} \bm{k}_{C,i}^T \prod_{i&rsquo;=i+1}^{n_c} \left(I - \beta_{C,i&rsquo;} \bm{k}_{C,i&rsquo;} \bm{k}_{C,i&rsquo;}^T\right)\right)
\end{align*}
$$
Thus,
$$
\begin{align*}
S_C &amp;= S_{C-1}A^*_C + B^*_C\\
S_C &amp;= S_{C-1} \prod_{i=1}^{n_c} \left(I - \beta_{C,i} \bm{k}_{C,i} \bm{k}_{C,i}^T\right) + \sum_{i=1}^{n_c} \left(\beta_{C,i} \bm{v}_{C,i} \bm{k}_{C,i}^T \prod_{i&rsquo;=i+1}^{n_c} \left(I - \beta_{C,i&rsquo;} \bm{k}_{C,i&rsquo;} \bm{k}_{C,i&rsquo;}^T\right)\right)
\end{align*}
$$
</details></p>

<h3 id="chunk-wise-gated-deltanet">Chunk-wise Gated DeltaNet<a hidden class="anchor" aria-hidden="true" href="#chunk-wise-gated-deltanet">#</a></h3>


<p><details >
  <summary markdown="span">Show derivation of $S_C$</summary>
  $$
\begin{align*}
A_{c,i} &amp;= \alpha_{c,i}(I - \beta_{c,i} \bm{k}_{c,i} \bm{k}_{c,i}^T) &amp; B_{c,i} &amp;= \beta_{c,i} \bm{v}_{c,i} \bm{k}_{c,i}^T\\
A^*_C &amp;= \prod_{i=1}^{n_c} \alpha_{C,i} \left(I - \beta_{C,i} \bm{k}_{C,i} \bm{k}_{C,i}^T\right) \quad &amp; B^*_C &amp;= \sum_{i=1}^{n_c} \left(\beta_{C,i} \bm{v}_{C,i} \bm{k}_{C,i}^T \prod_{i&rsquo;=i+1}^{n_c} \alpha_{C,i&rsquo;} \left(I - \beta_{C,i&rsquo;} \bm{k}_{C,i&rsquo;} \bm{k}_{C,i&rsquo;}^T\right)\right)
\end{align*}
$$
Thus,
$$
\begin{align*}
S_C &amp;= S_{C-1}A^*_C + B^*_C\\
S_C &amp;= S_{C-1} \prod_{i=1}^{n_c} \alpha_{C,i} \left(I - \beta_{C,i} \bm{k}_{C,i} \bm{k}_{C,i}^T\right) + \sum_{i=1}^{n_c} \left(\beta_{C,i} \bm{v}_{C,i} \bm{k}_{C,i}^T \prod_{i&rsquo;=i+1}^{n_c} \alpha_{C,i&rsquo;} \left(I - \beta_{C,i&rsquo;} \bm{k}_{C,i&rsquo;} \bm{k}_{C,i&rsquo;}^T\right)\right)\\
S_C &amp;= S_{C-1} \left(\prod_{i=1}^{n_c} \alpha_{C,i} \right) \left(\prod_{i=1}^{n_c} \left(I - \beta_{C,i} \bm{k}_{C,i} \bm{k}_{C,i}^T\right)\right) + \sum_{i=1}^{n_c} \left(\left(\beta_{C,i} \prod_{i&rsquo;=i+1}^{n_c} \alpha_{C,i&rsquo;} \right) \bm{v}_{C,i} \bm{k}_{C,i}^T  \prod_{i&rsquo;=i+1}^{n_c} \left(I - \beta_{C,i&rsquo;} \bm{k}_{C,i&rsquo;} \bm{k}_{C,i&rsquo;}^T\right)\right)
\end{align*}
$$
</details></p>

<h3 id="chunk-wise-rwkv-7">Chunk-wise RWKV-7<a hidden class="anchor" aria-hidden="true" href="#chunk-wise-rwkv-7">#</a></h3>


<p><details >
  <summary markdown="span">Show derivation of $S_C$</summary>
  $$
\begin{align*}
A_{c,i} &amp;= \text{diag}\left(\bm{w}_{c,i}\right) - \bm{\hat{\kappa}}_{c,i}(\bm{a}_{c,i} \odot\bm{\hat{\kappa}}_{c,i}^T) &amp; B_{c,i} &amp;= \bm{v}_{c,i} \bm{k}_{c,i}^T\\
A^*_C &amp;= \prod_{i=1}^{n_c} \left(\text{diag}\left(\bm{w}_{C,i}\right) - \bm{\hat{\kappa}}_{C,i}(\bm{a}_{C,i} \odot\bm{\hat{\kappa}}_{C,i}^T)\right) \quad &amp; B^*_C &amp;= \sum_{i=1}^{n_c} \left(\bm{v}_{C,i} \bm{k}_{C,i}^T \prod_{i&rsquo;=i+1}^{n_c} \left(\text{diag}\left(\bm{w}_{C,i&rsquo;}\right) - \bm{\hat{\kappa}}_{C,i&rsquo;}(\bm{a}_{C,i&rsquo;} \odot\bm{\hat{\kappa}}_{C,i&rsquo;}^T)\right)\right)
\end{align*}
$$
Thus,
$$
\begin{align*}
S_C &amp;= S_{C-1}A^*_C + B^*_C\\
S_C &amp;= S_{C-1} \prod_{i=1}^{n_c} \left(\text{diag}\left(\bm{w}_{C,i}\right) - \bm{\hat{\kappa}}_{C,i}(\bm{a}_{C,i} \odot\bm{\hat{\kappa}}_{C,i}^T)\right) + \sum_{i=1}^{n_c} \left(\bm{v}_{C,i} \bm{k}_{C,i}^T \prod_{i&rsquo;=i+1}^{n_c} \left(\text{diag}\left(\bm{w}_{C,i&rsquo;}\right) - \bm{\hat{\kappa}}_{C,i&rsquo;}(\bm{a}_{C,i&rsquo;} \odot\bm{\hat{\kappa}}_{C,i&rsquo;}^T)\right)\right)
\end{align*}
$$
</details></p>

<h2 id="multi-step-online-gradient-descent-per-token-with-chunk-wise-parallelism">Multi-Step Online Gradient Descent per Token with Chunk-wise Parallelism<a hidden class="anchor" aria-hidden="true" href="#multi-step-online-gradient-descent-per-token-with-chunk-wise-parallelism">#</a></h2>
<p>Let&rsquo;s combine the two techniques we&rsquo;ve discussed so far: multi-step online gradient descent per token and chunk-wise parallelism.</p>
<h3 id="the-strategy">The strategy<a hidden class="anchor" aria-hidden="true" href="#the-strategy">#</a></h3>
<p>We can do this either way, but suppose we chunk the updates first then expand the each of the updates within the chunks into a product of $n_h$ updates. I.e., we have:</p>
<p>$$
\begin{bmatrix}
A_{(c-1)*n_c + i} &amp; 0 \\
B_{(c-1)*n_c + i} &amp; I
\end{bmatrix}
\xrightarrow{\text{reindex}}
\begin{bmatrix}
A_{c,i} &amp; 0 \\
B_{c,i} &amp; I
\end{bmatrix}
\xrightarrow{\text{expand}}
\begin{bmatrix}
A_{c,i,1} &amp; 0 \\
B_{c,i,1} &amp; I
\end{bmatrix}
\begin{bmatrix}
A_{c,i,2} &amp; 0 \\
B_{c,i,2} &amp; I
\end{bmatrix}
\cdots
\begin{bmatrix}
A_{c,i,n_h} &amp; 0 \\
B_{c,i,n_h} &amp; I
\end{bmatrix}
$$
where $A_{c,i,j}$ and $B_{c,i,j}$ are the update matrices for the $j$-th gradient descent step for the $i$-th token within the $c$-th chunk.</p>
<p>And from equations $(6)$, $(10)$, and $(11)$, we have:
$$
\begin{align*}
\begin{bmatrix}
A^*_{c} &amp; 0 \\
B^*_{c} &amp; I
\end{bmatrix}
&amp;= \prod_{i=1}^{n_c} \begin{bmatrix}
A&rsquo;_{c,i} &amp; 0 \\
B&rsquo;_{c,i} &amp; I
\end{bmatrix}
= \prod_{i=1}^{n_c} \prod_{j=1}^{n_h} \begin{bmatrix}
A_{c,i,j} &amp; 0 \\
B_{c,i,j} &amp; I
\end{bmatrix}\\
\begin{bmatrix}
A^*_{c} &amp; 0 \\
B^*_{c} &amp; I
\end{bmatrix}
&amp;= \begin{bmatrix}
\prod_{i=1}^{n_c} \prod_{j=1}^{n_h} A_{c,i,j} &amp; 0 \\
\sum_{i=1}^{n_c}\sum_{j=1}^{n_h} \left( B_{c,i,j} \left(\prod_{j&rsquo;=j+1}^{n_h} A_{c,i,j&rsquo;}\right) \left(\prod_{i&rsquo;=i+1}^{n_c} \prod_{j=1}^{n_h} A_{c,i,j}\right)\right) &amp; I
\end{bmatrix}
\end{align*}
$$
Thus,
$$
\begin{align*}
A^*_{c} &amp;= \prod_{i=1}^{n_c} \prod_{j=1}^{n_h} A_{c,i,j} \\
B^*_{c} &amp;= \sum_{i=1}^{n_c}\sum_{j=1}^{n_h} \left( B_{c,i,j} \left(\prod_{j&rsquo;=j+1}^{n_h} A_{c,i,j&rsquo;}\right) \left(\prod_{i&rsquo;=i+1}^{n_c} \prod_{j=1}^{n_h} A_{c,i,j}\right)\right)
\end{align*}
$$
which we can then plug into Equation $(13)$ to get the cross-chunk recurrence:</p>
<p>$$
\begin{align*}
S_C &amp;= S_{C-1}A^*_C + B^*_C\\
S_C &amp;= S_{C-1} \prod_{i=1}^{n_c} \prod_{j=1}^{n_h} A_{C,i,j} + \sum_{i=1}^{n_c}\sum_{j=1}^{n_h} \left( B_{C,i,j} \left(\prod_{j&rsquo;=j+1}^{n_h} A_{C,i,j&rsquo;}\right) \left(\prod_{i&rsquo;=i+1}^{n_c} \prod_{j=1}^{n_h} A_{C,i,j}\right)\right)
\end{align*}
$$</p>
<p>or, if we reindex this as $[\cdot]_{C,k} = [\cdot]_{C,\space \lceil k/n_h \rceil,\space (k-1) \% n_h + 1}$, we get:</p>
<p>$$
\begin{align*}
S_C &amp;= S_{C-1} \prod_{k=1}^{n_c n_h} A_{C,k} + \sum_{k=1}^{n_c n_h} \left( B_{C,k} \prod_{k&rsquo;=k+1}^{n_c n_h} A_{C,k&rsquo;}\right)
\end{align*}
$$</p>
<hr>
<p>As an exercise, try deriving the cross-chunk recurrence for MambaSum, DeltaProduct, Gated DeltaProduct, and RWKV-7P.</p>
<hr>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>And that&rsquo;s it!</p>
<p>Not only is the blocked matrix formulation of linear attention mechanisms intuitive, it also makes the connections between different algorithms and computational forms much more obvious. I&rsquo;d even go as far as to say that we now have the proper abstraction to do an evolutionary search for new linear attention mechanisms ;)</p>
<hr>
<p>In the next post, we&rsquo;ll talk about faster ways to calculate $A^*_{c}$ and $B^*_{c}$ for diagonal and diagonal-plus-low-rank $A^*_{c}$ using the WY Representations and the UT Transform. Stay tuned!</p>
<h2 id="acknowledgements">Acknowledgements<a hidden class="anchor" aria-hidden="true" href="#acknowledgements">#</a></h2>
<p>Big thanks to Songlin Yang, Julien Siems, and @Smerky, @BeeGass, @safelix, and @jacobbuckman for their feedback and discussions!</p>
<h2 id="how-to-cite">How to Cite<a hidden class="anchor" aria-hidden="true" href="#how-to-cite">#</a></h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bibtex" data-lang="bibtex"><span style="display:flex;"><span><span style="color:#0a0;text-decoration:underline">@misc</span>{cesista2025blockmatlinearattn,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">author</span> = <span style="color:#a50">{Franz Louis Cesista}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">title</span> = <span style="color:#a50">{Blocked Matrix Formulation of Linear Attention Mechanisms}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">year</span> = <span style="color:#a50">{2025}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">url</span> = <span style="color:#a50">{https://leloykun.github.io/ponder/blockmat-linear-attn/}</span>,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ol>
<li>Riccardo Grazzi, Julien Siems, Jörg K.H. Franke, Arber Zela, Frank Hutter, Massimiliano Pontil (2025). Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues. URL <a href="https://arxiv.org/abs/2411.12537" target="_blank">https://arxiv.org/abs/2411.12537</a></li>
<li>Julien Siems, Timur Carstensen, Arber Zela, Frank Hutter, Massimiliano Pontil, Riccardo Grazzi (2025). DeltaProduct: Increasing the Expressivity of DeltaNet Through Products of Householders. URL <a href="https://arxiv.org/abs/2502.10297" target="_blank">https://arxiv.org/abs/2502.10297</a></li>
<li>Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 5156–5165. PMLR, 2020b. URL <a href="http://proceedings.mlr.press/v119/katharopoulos20a.html" target="_blank">http://proceedings.mlr.press/v119/katharopoulos20a.html</a>.</li>
<li>Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In Proceedings of the 41st International Conference on MachineLearning, volume 235 of Proceedingsof Machine Learning Research, pp. 10041–10071. PMLR, 2024b. URL <a href="https://proceedings.mlr.press/v235/dao24a.html" target="_blank">https://proceedings.mlr.press/v235/dao24a.html</a>.</li>
<li>Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim (2025). Parallelizing Linear Transformers with the Delta Rule over Sequence Length. URL <a href="https://arxiv.org/abs/2406.06484" target="_blank">https://arxiv.org/abs/2406.06484</a></li>
<li>Songlin Yang, Jan Kautz, Ali Hatamizadeh (2025). Gated Delta Networks: Improving Mamba2 with Delta Rule. URL <a href="https://arxiv.org/abs/2412.06464" target="_blank">https://arxiv.org/abs/2412.06464</a></li>
<li>Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 9099–9117. PMLR, 2022b. URL <a href="https://proceedings.mlr.press/v162/hua22a.html" target="_blank">https://proceedings.mlr.press/v162/hua22a.html</a>.</li>
<li>Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. ArXiv preprint, abs/2307.08621, 2023. URL <a href="https://arxiv.org/abs/2307.08621" target="_blank">https://arxiv.org/abs/2307.08621</a>.</li>
<li>Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Haowen Hou, Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, Nathan Wilce, Johan S. Wind, Tianyi Wu, Daniel Wuttke, Christian Zhou-Zheng (2025). RWKV-7 &ldquo;Goose&rdquo; with Expressive Dynamic State Evolution. URL <a href="https://arxiv.org/abs/2503.14456" target="_blank">https://arxiv.org/abs/2503.14456</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://leloykun.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://leloykun.github.io/tags/linear-attention/">Linear Attention</a></li>
      <li><a href="https://leloykun.github.io/tags/test-time-regression/">Test-Time Regression</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Blocked Matrix Formulation of Linear Attention Mechanisms on x"
            href="https://x.com/intent/tweet/?text=Blocked%20Matrix%20Formulation%20of%20Linear%20Attention%20Mechanisms&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fblockmat-linear-attn%2f&amp;hashtags=MachineLearning%2cLinearAttention%2cTest-TimeRegression">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Blocked Matrix Formulation of Linear Attention Mechanisms on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fblockmat-linear-attn%2f&amp;title=Blocked%20Matrix%20Formulation%20of%20Linear%20Attention%20Mechanisms&amp;summary=Blocked%20Matrix%20Formulation%20of%20Linear%20Attention%20Mechanisms&amp;source=https%3a%2f%2fleloykun.github.io%2fponder%2fblockmat-linear-attn%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Blocked Matrix Formulation of Linear Attention Mechanisms on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fleloykun.github.io%2fponder%2fblockmat-linear-attn%2f&title=Blocked%20Matrix%20Formulation%20of%20Linear%20Attention%20Mechanisms">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Blocked Matrix Formulation of Linear Attention Mechanisms on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fleloykun.github.io%2fponder%2fblockmat-linear-attn%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Blocked Matrix Formulation of Linear Attention Mechanisms on whatsapp"
            href="https://api.whatsapp.com/send?text=Blocked%20Matrix%20Formulation%20of%20Linear%20Attention%20Mechanisms%20-%20https%3a%2f%2fleloykun.github.io%2fponder%2fblockmat-linear-attn%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Blocked Matrix Formulation of Linear Attention Mechanisms on telegram"
            href="https://telegram.me/share/url?text=Blocked%20Matrix%20Formulation%20of%20Linear%20Attention%20Mechanisms&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fblockmat-linear-attn%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Blocked Matrix Formulation of Linear Attention Mechanisms on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Blocked%20Matrix%20Formulation%20of%20Linear%20Attention%20Mechanisms&u=https%3a%2f%2fleloykun.github.io%2fponder%2fblockmat-linear-attn%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://leloykun.github.io/">Franz Louis Cesista</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
