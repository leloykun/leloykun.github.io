<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Sensitivity and Sharpness of n-Simplical Attention | Franz Louis Cesista</title>
<meta name="keywords" content="Machine Learning, Optimizers, Architecture-Optimizer Codesign">
<meta name="description" content="Towards a maximal update parameterization of n-simplical attention">
<meta name="author" content="Franz Louis Cesista">
<link rel="canonical" href="https://leloykun.github.io/ponder/lipschitz-n-simplical-transformer/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e50e51e1b615f62676c32fb60e15e4b5e3a80ade29f3c6f9c953c528f576fb9f.css" integrity="sha256-5Q5R4bYV9iZ2wy&#43;2DhXkteOoCt4p88b5yVPFKPV2&#43;58=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://leloykun.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://leloykun.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://leloykun.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://leloykun.github.io/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://leloykun.github.io/ponder/lipschitz-n-simplical-transformer/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Sensitivity and Sharpness of n-Simplical Attention" />
<meta property="og:description" content="Towards a maximal update parameterization of n-simplical attention" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://leloykun.github.io/ponder/lipschitz-n-simplical-transformer/" /><meta property="article:section" content="ponder" />
<meta property="article:published_time" content="2025-07-06T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-07-06T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Sensitivity and Sharpness of n-Simplical Attention"/>
<meta name="twitter:description" content="Towards a maximal update parameterization of n-simplical attention"/>
<meta name="twitter:site" content="@leloykun"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Ponder",
      "item": "https://leloykun.github.io/ponder/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Sensitivity and Sharpness of n-Simplical Attention",
      "item": "https://leloykun.github.io/ponder/lipschitz-n-simplical-transformer/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Sensitivity and Sharpness of n-Simplical Attention",
  "name": "Sensitivity and Sharpness of n-Simplical Attention",
  "description": "Towards a maximal update parameterization of n-simplical attention",
  "keywords": [
    "Machine Learning", "Optimizers", "Architecture-Optimizer Codesign"
  ],
  "articleBody": "Introduction A team from Meta have recently shown that 2-simplical attention improves the exponent in the scaling laws vs. vanilla attention (Roy et al., 2025; Clift et al., 2019, Vaswani et al., 2017). This means that while it may not be as good or even worse than vanilla attention flops-vs-loss-wise at smaller scales, the trade-off gets better as the model scales up. This would be useful in e.g. large-scale reasoning-LLM training runs where context lengths could blow up to millions, even billions of tokens. It is also very Bitter Lesson-pilled: compute exponentially scales over time and having a compute sponge which we can pour more compute into and get better results is great.\nAnd if we are to scale this up, we have to consider two questions:\nIf 2-simplical attention is better than (vanilla) 1-simplical attention at scale, then would $n$-simplical attention be better than 2-simplical attention for $n \\geq 3$? How do we guarantee that our activation and gradient norms are ‘stable’ during training as we scale up the model? In this blog post, we will focus on the latter, however we will consider $n$-simplical attention in general in our analyses.\nDefinition 1 (n-Simplical Attention): Let $q, k^{(1:n)}, v^{(1:n)} \\in \\mathbb{R}^{T \\times d}$ be the query, keys, and values, where $T$ is the sequence length and $d$ is the model width. The n-simplical attention function $\\texttt{F}$ is defined as follows, $$\\begin{aligned} \\texttt{F}(q, k^{(1:n)}, v^{(1:n)}) \u0026= {\\color{blue}s_2} \\texttt{softmax}\\left({\\color{blue}s_1} \\langle q, k^{(1)}, k^{(2)}, \\ldots, k^{(n)} \\rangle + M\\right) ( v^{(1)} \\circ v^{(2)} \\circ \\ldots \\circ v^{(n)} )\\\\ \u0026= {\\color{blue}s_2} \\texttt{softmax}\\left({\\color{blue}s_1} \\left\\langle q, \\left( \\prod_{t=1}^n \\circ k^{(t)} \\right) \\right\\rangle + M\\right) \\left( \\prod_{t=1}^n \\circ v^{(t)} \\right) \\end{aligned}$$ where $\\texttt{softmax}(\\cdot)$ is applied to all indices except the first.\nExamples:\nVanilla Attention (Vaswani et al., 2017), $$\\texttt{F}(q, k, v) = \\texttt{softmax}\\left(\\frac{1}{\\sqrt{d}} qk^T + M\\right) v$$ 2-Simplical Attention (Clift et al., 2019), $$\\texttt{F}(q, k^{(1)}, k^{(2)}, v^{(1)}, v^{(2)}) = \\texttt{softmax}\\left(\\frac{1}{\\sqrt{d}} \\langle q, k^{(1)}, k^{(2)} \\rangle + M\\right) ( v^{(1)} \\circ v^{(2)} )$$ Note that for both of these examples, $s_1 = 1/\\sqrt{d}$ and $s_2 = 1$.\nAnd more formally, what we mean by activation norms being “stable” is that tiny changes in the inputs should not cause unexpectedly large changes in the outputs. We call this property module sensitivity. Likewise, we want the gradients to not blow up either, i.e. tiny changes in the inputs should not cause unexpectedly large changes in the gradients. We call this property module sharpness. And following Large et al. (2024), we formalize module sensitivity and sharpness as follows,\nDefinition 2 (Sensitivity): Let $M$ be a module on $(\\mathcal{X}, \\mathcal{Y}, \\mathcal{W})$ where $\\mathcal{X}$ is the input space with norm $\\|\\cdot\\|_{\\mathcal{X}}$, $\\mathcal{Y}$ is the output space with norm $\\|\\cdot\\|_\\mathcal{Y}$, and $\\mathcal{W}$ is the parameter space. We define $M$ to be $\\sigma$-sensitive if, $$\\begin{equation} \\| \\nabla M(w, x) \\diamond \\Delta x \\|_{\\mathcal{Y}} \\leq \\sigma \\| \\Delta x \\|_{\\mathcal{X}} \\qquad\\forall w \\in \\mathcal{W}; x, \\Delta x \\in \\mathcal{X} \\end{equation}$$\nDefinition 3 (Sharpness): Let $M$ be a module on $(\\mathcal{X}, \\mathcal{Y}, \\mathcal{W})$ where $\\mathcal{X}$ is the input space with norm $\\|\\cdot\\|_{\\mathcal{X}}$, $\\mathcal{Y}$ is the output space with norm $\\|\\cdot\\|_\\mathcal{Y}$, and $\\mathcal{W}$ is the parameter space. We define $M$ to be $\\gamma$-sharp if, $$\\begin{equation} \\| \\tilde{\\Delta} x \\diamond \\nabla^2 M(w, x) \\diamond \\Delta x \\|_{\\mathcal{Y}} \\leq \\gamma \\| \\Delta x \\|_{\\mathcal{X}} \\| \\tilde{\\Delta} x \\|_{\\mathcal{X}} \\qquad\\forall w \\in \\mathcal{W}; x, \\Delta x, \\tilde{\\Delta} x \\in \\mathcal{X} \\end{equation}$$\nNote that if $\\mathcal{X}$ and $\\mathcal{Y}$ are normed vector spaces, then the sensitivity bounds the (forward) Lipschitz constant of the module, and the sharpness bounds the (backward) gradient Lipschitz constant. Having unit sensitivity means that a small change in the input can only cause at most as much change in the output. Likewise, having unit sharpness means that a small change in the input can only cause at most as much change in the gradient.\nIn this blog post, we will show that $n$-simplical attention is unit sensitive and $(1 + \\tilde{L}_{\\texttt{softmax}})$-sharp under the $\\infty RMS$ operator norm, where $\\tilde{L}_{\\texttt{softmax}}$ is the gradient Lipschitz constant of the softmax function.\nClaim 4 (Sensitivity and sharpness of n-Simplical Attention): Let $q, k^{(1:n)}, v^{(1:n)} \\in \\mathbb{R}^{T \\times d}$ be the query, keys, and values, where $T$ is the sequence length and $d$ is the model width. $n$-simplical attention parameterized as follows, $$\\begin{equation} \\texttt{F}(q, k^{(1:n)}, v^{(1:n)}) = {\\color{blue}\\frac{1}{d^{(n-1)/2}}} \\texttt{softmax}\\left({\\color{blue}\\frac{1}{d^{(n+1)/2}}} \\left\\langle q, \\left( \\prod_{t=1}^n \\circ k^{(t)} \\right) \\right\\rangle + M\\right) \\left( \\prod_{t=1}^n \\circ v^{(t)} \\right) \\end{equation}$$ is unit sensitive and $(1 + \\tilde{L}_{\\texttt{softmax}})$-sharp under the $\\infty RMS$ operator norm where $d$ is the model width and $\\tilde{L}_{\\texttt{softmax}}$ is the gradient Lipschitz constant of the softmax function.\nPreliminaries First, let’s rewrite n-Simplical Attention in Claim 4 above as follows,\n$$\\begin{align} S \u0026= s_1 \\left\\langle q, \\left( \\prod_{t=1}^n \\circ k^{(t)} \\right) \\right\\rangle \\qquad \u0026 s_1 \u0026= \\frac{1}{d^{(n+1)/2}} \\\\ A \u0026= \\texttt{softmax}\\left(S + M\\right) \\\\ W \u0026= s_2 \\prod_{t=1}^n \\circ v^{(t)} \\qquad \u0026 s_2 \u0026= \\frac{1}{d^{(n-1)/2}}\\\\ F \u0026= A W \\end{align}$$\nWe chose the scaling factor $s_2 = \\frac{1}{d^{(n-1)/2}}$ so that $\\| W \\|_{\\infty RMS} \\leq 1$ for unit RMS norm values. This follows directly from Lemma 6 below. As for the scaling factor $s_1 = \\frac{1}{d^{(n+1)/2}}$, we chose it so that the entries of $S$ are bounded by $1$ (see Lemma 7), making (masked) softmax 1-Lipschitz. This property is crucial for our proofs later on.\nProposition 5 (RMS norm of hadamard product of vectors): Let $x, y \\in \\mathbb{R}^d$ be vectors. Then the RMS norm of their hadamard product is bounded by the RMS norms of the individual vectors, $$\\begin{equation}\\| x \\circ y \\|_{RMS} \\leq \\sqrt{d} \\| x \\|_{RMS} \\| y \\|_{RMS} \\end{equation}$$\nShow proof of Proposition 5 Proof: $$\\begin{aligned} \\left\\| x \\circ y \\right\\|_{RMS}^2 \u0026= \\left\\| x \\circ y \\right\\|_{RMS}^2 \\\\ \u0026= \\left(\\frac{1}{\\sqrt{d}}\\right)^2\\left\\| x \\circ y \\right\\|_{2}^2 \\\\ \u0026= \\frac{1}{d} \\sum_{r=1}^d (x_r)^2 (y_r)^2 \\\\ \u0026\\leq \\frac{1}{d} \\left(\\sum_{r=1}^d (x_r)^4 \\right)^{1/2} \\left(\\sum_{r=1}^d (y_r)^4\\right)^{1/2} \u0026\\text{(from Cauchy-Schwarz)}\\\\ \u0026\\leq \\frac{1}{d} \\left(\\sum_{r=1}^d (x_r)^2 \\right) \\left(\\sum_{r=1}^d (y_r)^2\\right) \u0026\\text{(from Jensen’s Lemma)}\\\\ \u0026\\leq \\frac{1}{d} \\| x_r \\|_2^2 \\| y_r \\|_2^2 \\\\ \\left\\| x \\circ y \\right\\|_{RMS}^2 \u0026\\leq d \\| x \\|_{RMS}^2 \\| y \\|_{RMS}^2 \\\\ \\left\\| x \\circ y \\right\\|_{RMS} \u0026\\leq \\sqrt{d} \\| x \\|_{RMS}^2 \\| y \\|_{RMS}^2 \\quad\\blacksquare \\end{aligned}$$\nLemma 6 (RMS norm of hadamard product of unit RMS norm vectors): Let $x^{(1)}, x^{(2)}, \\ldots, x^{(n)} \\in \\mathbb{R}^d$ be vectors with $\\| x^{(t)} \\|_{RMS} \\leq 1$ for all $t$. Then, $$\\begin{equation}\\left\\| \\prod_{t=1}^n \\circ x^{(t)} \\right\\|_{RMS} \\leq d^{(n-1)/2}\\end{equation}$$\nThe proof follows directly from Proposition 5.\nLemma 7: For unit RMS norm query $q$ and keys $k^{(1:n)}$, the choice of scaling factor $s_1 = \\frac{1}{d^{(n+1)/2}}$ bounds the entries of $S$ by $1$.\nShow proof of Lemma 7 Proof: From Lemma 6, we have, $$\\| w(n) \\|_{\\infty RMS} := \\left\\| \\prod_{t=1}^n \\circ k^{(t)} \\right\\|_{\\infty RMS} \\leq d^{(n-1)/2}$$ Thus, $$\\begin{aligned} | \\langle q, w(n) \\rangle | \u0026\\leq \\| q \\|_2 \\| w(n) \\|_2 \\\\ \u0026\\leq (\\sqrt{d} \\cancel{\\| q \\|_{\\infty RMS}})( \\sqrt{d} \\| w(n) \\|_{\\infty RMS}) \\\\ \u0026\\leq d d^{(n-1)/2} \\\\ | \\langle q, w(n) \\rangle | \u0026= d^{(n+1)/2} \\end{aligned}$$ Thus the entries of $S$ are bounded by, $$| S_{i,J} | = \\frac{1}{d^{(n+1)/2}} | \\langle q_i, w(n)_J \\rangle | \\leq \\frac{1}{d^{(n+1)/2}} d^{(n+1)/2} = 1 \\quad\\blacksquare$$\nSensitivity of n-Simplical Attention We wish to show that the n-simplical attention is unit sensitive for unit RMS norm inputs $(q, k^{(1:n)}, v^{(1:n)}) \\in \\mathcal{X}$.\nClaim 8: Let $q, k^{(1:n)}, v^{(1:n)} \\in \\mathbb{R}^{T \\times d}$ be the query, keys, and values, where $T$ is the sequence length and $d$ is the model width. For $\\| q \\|_{\\infty RMS} = \\| k^{(t)} \\|_{\\infty RMS} = \\| v^{(t)} \\|_{\\infty RMS} = 1$ for all $t$, the n-simplical attention function $\\texttt{F}$ is unit sensitive under the $\\infty RMS$ operator norm. That is, for any perturbation $(\\Delta q, \\Delta k^{(1:n)}, \\Delta v^{(1:n)}) \\in \\mathcal{X}$, we have, $$\\begin{aligned} \\| \\nabla F \\diamond ( \\Delta q, \\Delta k^{(1:n)}, \\Delta v^{(1:n)} ) \\|_{\\infty RMS} \u0026\\leq \\| (\\Delta q, \\Delta k^{[1:n]}, \\Delta v^{[1:n]}) \\|_{\\infty RMS} \\\\ \u0026\\leq \\| \\Delta q \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\Delta k^{(t)} \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\Delta v^{(t)} \\|_{\\infty RMS}\\\\ \\end{aligned}$$\nTo prove this, let’s first take the derivative of $\\texttt{F}$ towards $(\\Delta q, \\Delta k^{(1:n)}, \\Delta v^{(1:n)})$,\n$$\\begin{align} \\nabla F \\diamond ( \\Delta q, \\Delta k^{(1:n)}, \\Delta v^{(1:n)} ) \u0026= (\\Delta A) W + A (\\Delta W) \\\\ \\| \\nabla F \\diamond ( \\Delta q, \\Delta k^{(1:n)}, \\Delta v^{(1:n)} ) \\|_{\\infty RMS} \u0026\\leq \\| \\Delta A \\|_{\\infty RMS} \\| W \\|_{\\infty RMS} + \\| A \\|_{\\infty RMS} \\| \\Delta W \\|_{\\infty RMS}\\\\ \\end{align}$$\nWe have already shown earlier that, $$\\| W \\|_{\\infty RMS} = \\| A \\|_{\\infty RMS} = 1$$ by construction. And so we only need to derive $\\| \\Delta A \\|_{\\infty RMS}$ and $\\| \\Delta W \\|_{\\infty RMS}$.\nAs for the $\\Delta A$ term, by the chain-rule, we have, $$\\begin{equation}\\| \\Delta A \\|_{\\infty RMS} \\leq \\| \\nabla \\texttt{softmax}(S)[\\Delta S] \\|_{\\infty -op} \\| \\Delta S \\|_{\\infty RMS}\\end{equation}$$ and since the softmax is 1-Lipschitz with respect to the $\\infty RMS$ norm with our parameterization, we have, $$\\| \\Delta A \\|_{\\infty RMS} \\leq \\| \\Delta S \\|_{\\infty RMS}$$\nBy the product rule, we have,\n$$\\begin{align} \\Delta S \u0026= \\frac{1}{d^{(n+1)/2}} \\left\\langle \\Delta q, \\left( \\prod_{t=1}^n \\circ k^{(t)} \\right) \\right\\rangle + \\frac{1}{d^{(n+1)/2}} \\sum_{t=1}^{n} \\left\\langle q, \\Delta k^{(t)} \\circ \\left( \\prod_{s=1,s\\neq t}^n \\circ k^{(s)} \\right) \\right\\rangle \\\\ \\Delta S \u0026= \\frac{1}{d^{(n+1)/2}} \\left\\langle \\Delta q, \\left( \\prod_{t=1}^n \\circ k^{(t)} \\right) \\right\\rangle + \\frac{1}{d^{(n+1)/2}} \\sum_{t=1}^{n} \\left\\langle \\Delta k^{(t)}, q \\circ \\left( \\prod_{s=1,s\\neq t}^n \\circ k^{(s)} \\right) \\right\\rangle \\nonumber\\\\ \\end{align}$$\nThus,\n$$\\begin{aligned} \\| \\Delta S \\|_{\\infty RMS} \u0026\\leq \\frac{1}{d^{(n+1)/2}} \\| \\Delta q \\|_{2} \\left \\| \\prod_{t=1}^n \\circ k^{(t)} \\right\\|_{2} + \\frac{d}{d^{(n+1)/2}} \\sum_{t=1}^{n} \\| \\Delta k^{(t)} \\|_{2} \\left\\| q \\circ \\prod_{s=1,s\\neq t}^n \\circ k^{(s)} \\right\\|_{2}\\\\ \u0026\\leq \\frac{d}{d^{(n+1)/2}} \\| \\Delta q \\|_{\\infty RMS} \\left \\| \\prod_{t=1}^n \\circ k^{(t)} \\right\\|_{\\infty RMS}\\\\ \u0026\\quad + \\frac{d}{d^{(n+1)/2}} \\sum_{t=1}^{n} \\| \\Delta k^{(t)} \\|_{\\infty RMS} \\left\\| q \\circ \\prod_{s=1,s\\neq t}^n \\circ k^{(s)} \\right\\|_{\\infty RMS}\\\\ \u0026\\leq \\cancel{\\frac{dd^{(n-1)/2}}{d^{(n+1)/2}}} \\left( \\| \\Delta q \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\Delta k^{(t)} \\|_{\\infty RMS} \\right)\\\\ \\| \\Delta S \\|_{\\infty RMS} \u0026\\leq \\| \\Delta q \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\Delta k^{(t)} \\|_{\\infty RMS} \\end{aligned}$$\nThus, $$\\begin{equation} \\| \\Delta A \\|_{\\infty RMS} \\leq \\| \\Delta q \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\Delta k^{(t)} \\|_{\\infty RMS} \\end{equation}$$\nAs for the $\\Delta W$ term, by the product rule, we have,\n$$\\begin{align} \\Delta W \u0026= \\frac{1}{d^{(n-1)/2}} \\sum_{t=1}^{n} \\left[ \\Delta v^{(t)} \\circ \\left( \\prod_{s=1,s\\neq t}^n \\circ v^{(s)} \\right)\\right] \\end{align}$$\nThus,\n$$\\begin{align} \\| \\Delta W \\|_{\\infty RMS} \u0026\\leq \\frac{1}{d^{(n-1)/2}} \\sum_{t=1}^{n} \\left\\| \\Delta v^{(t)} \\circ \\prod_{s=1,s\\neq t}^n \\circ v^{(s)} \\right\\|_{\\infty RMS}\\nonumber\\\\ \u0026\\leq \\frac{\\sqrt{d}}{d^{(n-1)/2}} \\sum_{t=1}^{n} \\| \\Delta v^{(t)} \\|_{\\infty RMS} \\left\\| \\prod_{s=1,s\\neq t}^n \\circ v^{(s)} \\right\\|_{\\infty RMS} \u0026\\text{(from Proposition 5)}\\nonumber\\\\ \u0026\\leq \\cancel{\\frac{d^{1/2}d^{(n-2)/2}}{d^{(n-1)/2}}} \\sum_{t=1}^{n} \\| \\Delta v^{(t)} \\|_{\\infty RMS}\u0026\\text{(from Lemma 6)}\\nonumber\\\\ \\| \\Delta W \\|_{\\infty RMS} \u0026= \\sum_{t=1}^{n} \\| \\Delta v^{(t)} \\|_{\\infty RMS} \\end{align}$$\nCombining Equations (11), (14), and (16) then gives us,\n$$\\begin{aligned} \\| \\nabla F \\diamond \\langle \\Delta q, \\Delta k^{(1:n)}, \\Delta v^{(1:n)} \\rangle \\|_{\\infty RMS} \u0026\\leq \\| \\Delta q \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\Delta k^{(t)} \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\Delta v^{(t)} \\|_{\\infty RMS}\\\\ \\| \\nabla F \\diamond \\langle \\Delta q, \\Delta k^{(1:n)}, \\Delta v^{(1:n)} \\rangle \\|_{\\infty RMS} \u0026\\leq \\| (q, k^{[1:n]}, v^{[1:n]}) \\|_{\\infty RMS} \\end{aligned}$$\nHence, n-simplical attention is unit sensitive under the $\\infty RMS$ operator norm as claimed.\nSharpness of n-Simplical Attention Next, we wish to show that the n-simplical attention is $(1+\\tilde{L}_{\\texttt{softmax}})$-sharp for unit RMS norm inputs $(q, k^{(1:n)}, v^{(1:n)}) \\in \\mathcal{X}$. More formally,\nClaim 9: Let $q, k^{(1:n)}, v^{(1:n)} \\in \\mathbb{R}^{T \\times d}$ be the query, keys, and values, where $T$ is the sequence length and $d$ is the model width. For $\\| q \\|_{\\infty RMS} = \\| k^{(t)} \\|_{\\infty RMS} = \\| v^{(t)} \\|_{\\infty RMS} = 1$ for all $t$, the n-simplical attention function $\\texttt{F}$ is unit sensitive under the $\\infty RMS$ operator norm. That is, for any pair of perturbations $(\\Delta q, \\Delta k^{(1:n)}, \\Delta v^{(1:n)}), (\\tilde{\\Delta} q, \\tilde{\\Delta} k^{(1:n)}, \\tilde{\\Delta} v^{(1:n)}) \\in \\mathcal{X}$, we have, $$\\begin{aligned} \u0026\\| (\\tilde{\\Delta} q, \\tilde{\\Delta} k^{(1:n)}, \\tilde{\\Delta} v^{(1:n)}) \\diamond \\nabla F \\diamond ( \\Delta q, \\Delta k^{(1:n)}, \\Delta v^{(1:n)} ) \\|_{\\infty RMS}\\\\ \u0026\\qquad\\qquad \\leq (1+\\tilde{L}_{\\texttt{softmax}})\\| (\\Delta q, \\Delta k^{[1:n]}, \\Delta v^{[1:n]}) \\|_{\\infty RMS} \\| (\\tilde{\\Delta} q, \\tilde{\\Delta} k^{[1:n]}, \\tilde{\\Delta} v^{[1:n]}) \\|_{\\infty RMS} \\\\ \u0026\\qquad\\qquad \\leq (1+\\tilde{L}_{\\texttt{softmax}})\\left(\\| \\Delta q \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\Delta k^{(t)} \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\tilde{\\Delta} v^{(t)} \\|_{\\infty RMS}\\right)\\\\ \u0026\\qquad\\qquad\\qquad\\qquad\\qquad\\quad \\times \\left(\\| \\tilde{\\Delta} q \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\tilde{\\Delta} k^{(t)} \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\tilde{\\Delta} v^{(t)} \\|_{\\infty RMS}\\right) \\end{aligned}$$\nTo prove this, let’s first take the derivative of Equation (10) towards $(\\tilde{\\Delta} q, \\tilde{\\Delta} k^{(1:n)}, \\tilde{\\Delta} v^{(1:n)})$,\n$$\\begin{align} \u0026\\langle \\tilde{\\Delta} q, \\tilde{\\Delta} k^{(1:n)}, \\tilde{\\Delta} v^{(1:n)} \\rangle \\diamond \\nabla^2 F \\diamond \\langle \\Delta q, \\Delta k^{(1:n)}, \\Delta v^{(1:n)} \\rangle\\nonumber\\\\ \u0026\\qquad\\qquad= (\\Delta^2 A) W + (\\tilde{\\Delta} A) (\\Delta W) + (\\Delta A) (\\tilde{\\Delta} W) + A (\\Delta^2 W) \\\\ \u0026\\| \\langle \\tilde{\\Delta} q, \\tilde{\\Delta} k^{(1:n)}, \\tilde{\\Delta} v^{(1:n)} \\rangle \\diamond \\nabla^2 F \\diamond \\langle \\Delta q, \\Delta k^{(1:n)}, \\Delta v^{(1:n)} \\rangle \\|_{\\infty RMS}\\nonumber\\\\ \u0026\\qquad\\qquad\\leq \\| \\Delta^2 A \\|_{\\infty RMS} \\cancel{\\| W \\|_{\\infty RMS}} + \\| \\tilde{\\Delta} A \\|_{\\infty RMS} \\| \\Delta W \\|_{\\infty RMS} \\nonumber\\\\ \u0026\\qquad\\qquad\\quad + \\| \\Delta A \\|_{\\infty RMS} \\| \\tilde{\\Delta} W \\|_{\\infty RMS} + \\cancel{\\| A \\|_{\\infty RMS}} \\| \\Delta^2 W \\|_{\\infty RMS} \\\\ \\end{align}$$\nWe have already derived $\\| \\Delta A \\|_{\\infty RMS}$ and $\\| \\Delta W \\|_{\\infty RMS}$ in the previous section. And for $\\| \\tilde{\\Delta} A \\|_{\\infty RMS}$ and $\\| \\tilde{\\Delta} W \\|_{\\infty RMS}$, it would suffice to replace $\\Delta$ with $\\tilde{\\Delta}$ in the previous derivations. Again, we also have $\\| W \\|_{\\infty RMS} = \\| A \\|_{\\infty RMS} = 1$ by construction. So, we only need to derive $\\| \\Delta^2 A \\|_{\\infty RMS}$ and $\\| \\Delta^2 W \\|_{\\infty RMS}$.\nFor the $\\Delta^2 A$ term, let’s take the derivative of Equation (12) towards $\\tilde{\\Delta}$,\n$$\\begin{aligned} \\| \\Delta^2 A \\|_{\\infty RMS} \u0026\\leq \\| \\nabla^2 \\texttt{softmax}(S)[\\Delta S, \\tilde{\\Delta} S] \\|_{\\infty -op} \\| \\Delta S \\|_{\\infty RMS} \\| \\tilde{\\Delta} S \\|_{\\infty RMS}\\\\ \u0026\\quad+ \\cancel{\\| \\nabla \\texttt{softmax}(S)[\\Delta S] \\|_{\\infty -op}} \\| \\Delta^2 S \\|_{\\infty RMS}\\\\ \\| \\Delta^2 A \\|_{\\infty RMS} \u0026\\leq \\tilde{L}_{\\texttt{softmax}} \\| \\Delta S \\|_{\\infty RMS} \\| \\tilde{\\Delta} S \\|_{\\infty RMS} + \\| \\Delta^2 S \\|_{\\infty RMS}\\\\ \\end{aligned}$$\nWe have already derived $\\| \\Delta S \\|_{\\infty RMS}$ in the previous section. And for $\\| \\tilde{\\Delta} S \\|_{\\infty RMS}$, it would suffice to replace $\\Delta$ with $\\tilde{\\Delta}$ in the previous derivation. So, we only need to derive $\\| \\Delta^2 S \\|_{\\infty RMS}$. Applying the product rule to Equation (13), we have,\n$$\\begin{align} \\Delta^2 S \u0026= \\frac{1}{d^{(n+1)/2}} \\sum_{t=1}^n\\left\\langle \\Delta q, \\tilde{\\Delta} k^{(t)} \\circ \\left( \\prod_{s=1,s\\neq t}^n \\circ k^{(s)} \\right) \\right\\rangle \\nonumber\\\\ \u0026\\quad + \\frac{1}{d^{(n+1)/2}} \\sum_{t=1}^{n} \\left\\langle \\tilde{\\Delta} q, \\Delta k^{(t)} \\circ \\left( \\prod_{s=1,s\\neq t}^n \\circ k^{(s)} \\right) \\right\\rangle \\nonumber\\\\ \u0026\\quad + \\frac{1}{d^{(n+1)/2}} \\sum_{1 \\leq t \u003c s \\leq n} \\left\\langle q, \\Delta k^{(t)} \\circ \\tilde{\\Delta} k^{(s)} \\circ \\left( \\prod_{r=1,r\\neq t,r\\neq s}^n \\circ k^{(r)} \\right) \\right\\rangle \\end{align}$$\nThus,\n$$\\begin{aligned} \\| \\Delta^2 S \\|_{\\infty RMS} \u0026\\leq \\frac{1}{d^{(n+1)/2}} \\| \\Delta q \\|_{2} \\sum_{t=1}^n \\left\\| \\tilde{\\Delta} k^{(t)} \\circ \\prod_{s=1,s\\neq t}^n \\circ k^{(s)} \\right\\|_{2} \\\\ \u0026\\quad + \\frac{1}{d^{(n+1)/2}} \\| \\tilde{\\Delta} q \\|_{2} \\sum_{t=1}^{n} \\left\\| \\Delta k^{(t)} \\circ \\prod_{s=1,s\\neq t}^n \\circ k^{(s)} \\right\\|_{2} \\\\ \u0026\\quad + \\frac{1}{d^{(n+1)/2}} \\| q \\|_{2} \\sum_{1 \\leq t \u003c s \\leq n} \\left\\| \\Delta k^{(t)} \\circ \\tilde{\\Delta} k^{(s)} \\circ \\prod_{r=1,r\\neq t,r\\neq s}^n \\circ k^{(r)} \\right\\|_{2} \\\\ \u0026\\leq \\frac{d^{3/2}}{d^{(n+1)/2}} \\| \\Delta q \\|_{\\infty RMS} \\sum_{t=1}^n \\| \\tilde{\\Delta} k^{(t)} \\|_{\\infty RMS} \\left\\| \\prod_{s=1,s\\neq t}^n \\circ k^{(s)} \\right\\|_{\\infty RMS} \\\\ \u0026\\quad + \\frac{d^{3/2}}{d^{(n+1)/2}} \\| \\tilde{\\Delta} q \\|_{\\infty RMS} \\sum_{t=1}^{n} \\| \\Delta k^{(t)} \\|_{\\infty RMS} \\left\\| \\prod_{s=1,s\\neq t}^n \\circ k^{(s)} \\right\\|_{\\infty RMS} \\\\ \u0026\\quad + \\frac{d^{2}}{d^{(n+1)/2}} \\cancel{\\| q \\|_{\\infty RMS}} \\sum_{1 \\leq t \u003c s \\leq n} \\| \\Delta k^{(t)} \\|_{\\infty RMS} \\| \\tilde{\\Delta} k^{(s)} \\|_{\\infty RMS} \\left\\| \\prod_{r=1,r\\neq t,r\\neq s}^n \\circ k^{(r)} \\right\\|_{\\infty RMS} \\\\ \u0026\\leq \\cancel{\\frac{d^{3/2}d^{(n-2)/2}}{d^{(n+1)/2}}} \\| \\Delta q \\|_{\\infty RMS} \\sum_{t=1}^n \\| \\tilde{\\Delta} k^{(t)} \\|_{\\infty RMS} \\\\ \u0026\\quad + \\cancel{\\frac{d^{3/2}d^{(n-2)/2}}{d^{(n+1)/2}}} \\| \\tilde{\\Delta} q \\|_{\\infty RMS} \\sum_{t=1}^{n} \\| \\Delta k^{(t)} \\|_{\\infty RMS} \\\\ \u0026\\quad + \\cancel{\\frac{d^{2}d^{(n-3)/2}}{d^{(n+1)/2}}} \\sum_{1 \\leq t \u003c s \\leq n} \\| \\Delta k^{(t)} \\|_{\\infty RMS} \\| \\tilde{\\Delta} k^{(s)} \\|_{\\infty RMS} \\\\ \\| \\Delta^2 S \\|_{\\infty RMS} \u0026\\leq \\left( \\| \\Delta q \\|_{\\infty RMS} + \\sum_{t=1}^n \\| \\Delta k^{(t)} \\|_{\\infty RMS} \\right) \\left( \\| \\tilde{\\Delta} q \\|_{\\infty RMS} + \\sum_{t=1}^n \\| \\tilde{\\Delta} k^{(t)} \\|_{\\infty RMS} \\right) \\end{aligned}$$\nThus,\n$$\\begin{align} \\| \\Delta^2 A \\|_{\\infty RMS} \u0026\\leq (1+\\tilde{L}_{\\texttt{softmax}}) \\left( \\| \\Delta q \\|_{\\infty RMS} + \\sum_{t=1}^n \\| \\Delta k^{(t)} \\|_{\\infty RMS} \\right) \\left( \\| \\tilde{\\Delta} q \\|_{\\infty RMS} + \\sum_{t=1}^n \\| \\tilde{\\Delta} k^{(t)} \\|_{\\infty RMS} \\right) \\end{align}$$\nFor the $\\Delta^2 W$ term, let’s take the derivative of Equation (15) towards $\\tilde{\\Delta}$,\n$$\\begin{align} \\Delta^2 W \u0026= \\frac{1}{d^{(n-1)/2}} \\sum_{1 \\leq t \u003c s \\leq n} \\left[ \\Delta v^{(t)} \\circ \\tilde{\\Delta} v^{(s)} \\circ \\left( \\prod_{r=1,r\\neq t,r\\neq s}^n \\circ v^{(r)} \\right)\\right] \\end{align}$$\n$$\\begin{align} \\| \\Delta^2 W \\|_{\\infty RMS} \u0026\\leq \\frac{1}{d^{(n-1)/2}} \\sum_{1 \\leq t \u003c s \\leq n} \\left\\| \\Delta v^{(t)} \\circ \\tilde{\\Delta} v^{(s)} \\circ \\prod_{r=1,r\\neq t,r\\neq s}^n \\circ v^{(r)} \\right\\|_{\\infty RMS} \\nonumber\\\\ \u0026\\leq \\frac{d}{d^{(n-1)/2}} \\sum_{1 \\leq t \u003c s \\leq n} \\| \\Delta v^{(t)} \\|_{\\infty RMS} \\| \\tilde{\\Delta} v^{(s)} \\|_{\\infty RMS} \\left\\| \\prod_{r=1,r\\neq t,r\\neq s}^n \\circ v^{(r)} \\right\\|_{\\infty RMS} \\nonumber\\\\ \u0026\\leq \\cancel{\\frac{dd^{(n-3)/2}}{d^{(n-1)/2}}} \\sum_{1 \\leq t \u003c s \\leq n} \\| \\Delta v^{(t)} \\|_{\\infty RMS} \\| \\tilde{\\Delta} v^{(s)} \\|_{\\infty RMS} \\nonumber\\\\ \\| \\Delta^2 W \\|_{\\infty RMS} \u0026\\leq \\left( \\sum_{t=1}^n \\| \\Delta v^{(t)} \\|_{\\infty RMS} \\right) \\left( \\sum_{t=1}^n \\| \\tilde{\\Delta} v^{(t)} \\|_{\\infty RMS} \\right) \\end{align}$$\nCombining Equations (18), (20), and (22) then gives us,\n$$\\begin{aligned} \u0026\\| \\langle \\tilde{\\Delta} q, \\tilde{\\Delta} k^{(1:n)}, \\tilde{\\Delta} v^{(1:n)} \\rangle \\diamond \\nabla^2 F \\diamond \\langle \\Delta q, \\Delta k^{(1:n)}, \\Delta v^{(1:n)} \\rangle \\|_{\\infty RMS} \\\\ \u0026\\quad \\leq (1 + \\tilde{L}_{\\texttt{softmax}}) \\left( \\| \\Delta q \\|_{\\infty RMS} + \\sum_{t=1}^n \\| \\Delta k^{(t)} \\|_{\\infty RMS} \\right) \\left( \\| \\tilde{\\Delta} q \\|_{\\infty RMS} + \\sum_{t=1}^n \\| \\tilde{\\Delta} k^{(t)} \\|_{\\infty RMS} \\right)\\\\ \u0026\\qquad + \\left( \\| \\tilde{\\Delta} q \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\tilde{\\Delta} k^{(t)} \\|_{\\infty RMS} \\right) \\left( \\sum_{t=1}^{n} \\| \\Delta v^{(t)} \\|_{\\infty RMS} \\right) \\\\ \u0026\\qquad + \\left( \\| \\Delta q \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\Delta k^{(t)} \\|_{\\infty RMS} \\right) \\left( \\sum_{t=1}^{n} \\| \\tilde{\\Delta} v^{(t)} \\|_{\\infty RMS} \\right) \\\\ \u0026\\qquad + \\left( \\sum_{t=1}^n \\| \\Delta v^{(t)} \\|_{\\infty RMS} \\right) \\left( \\sum_{t=1}^n \\| \\tilde{\\Delta} v^{(t)} \\|_{\\infty RMS} \\right) \\\\ \u0026\\quad \\leq (1 + \\tilde{L}_{\\texttt{softmax}}) \\left( \\| \\Delta q \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\Delta k^{(t)} \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\Delta v^{(t)} \\|_{\\infty RMS} \\right) \\\\ \u0026\\qquad\\qquad\\qquad\\qquad \\left( \\| \\tilde{\\Delta} q \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\tilde{\\Delta} k^{(t)} \\|_{\\infty RMS} + \\sum_{t=1}^{n} \\| \\tilde{\\Delta} v^{(t)} \\|_{\\infty RMS} \\right) \\end{aligned}$$\nHence, n-simplical attention is $(1+\\tilde{L}_{\\texttt{softmax}})$-sharp under the $\\infty RMS$ operator norm as claimed.\nDiscussion Here we have devised a parametrization that allows us to have width-independent sensitivity and sharpness bounds for n-simplical attention. We hope that this will allow us to construct a maximum update parametrization of some sort for such modules and networks containing them.\nNote however that for $n = 1$, we have to set the scaling factor $s_1 = \\frac{1}{d^{(1+1)/2}} = \\frac{1}{d}$, which is the same scaling factor suggested by Large et al. (2024), but is different from the more standard $s_1 = \\frac{1}{\\sqrt{d}}$. Likewise, for 2-simplical attention, we have to set the scaling factor $s_1 = \\frac{1}{d^{(2+1)/2}} = \\frac{1}{d^{3/2}}$, which is different from the $s_1 = \\frac{1}{\\sqrt{d}}$ used by Roy et al. (2025). Additionally, we also have to set $s_2 = \\frac{1}{d^{(2-1)/2}} = \\frac{1}{\\sqrt{d}}$ for the outer scale in 2-simplical attention, which, for larger dimensions, scales down the outputs significantly. Empirically, such parametrization leads to worse performance early in training, but guarantees stable training, especially at the tail end of training where the queries, keys, and values are more often aligned than not.\nThe main benefit of having low (and width-independent) sensitivity and sharpness really is that it allows us to have larger update step sizes without worrying about suddenly exploding or vanishing activations and gradients. Additionally, bounding the sensitivity allows us to control how much the gradients change as they pass through the module via backpropagation–the smaller the sensitivity, the smaller the change in the gradients. And bounding the sharpness allows us to have more trust in the momentum term more knowing that gradient spikes would rarely happen, if at all. These gradient spikes notoriously ‘break’ the momentum term at larger traning runs, especially near the end of training.\nLastly, this could also be useful in distributed training setups where gradient all-reduces are expensive and thus sparsifying the gradients before sending them over the network is a must (Douillard et al., 2024; Thérien et al., 2025). Problem arises when the gradients have outliers, requiring us to use more expensive quantization schemes to avoid losing information. But having control over the gradient norms should allow us to eliminate such outliers and get low-precision (and thus low-communication) training basically “for free”.\nHow to Cite @misc{cesista2025sensitivitysharpnessnsimplicalattention, author = {Franz Louis Cesista}, title = {\"Sensitivity and Sharpness of n-Simplical Attention\"}, year = {2025}, url = {https://leloykun.github.io/ponder/lipschitz-n-simplical-transformer/}, } References Aurko Roy, Timothy Chou, Sai Surya Duvvuri, Sijia Chen, Jiecao Yu, Xiaodong Wang, Manzil Zaheer, Rohan Anil (2025). Fast and Simplex: 2-Simplicial Attention in Triton. URL https://arxiv.org/abs/2507.02754v1 James Clift, Dmitry Doryn, Daniel Murfet, James Wallbridge (2019). Logic and the -Simplicial Transformer. URL https://arxiv.org/abs/1909.00668 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … \u0026 Polosukhin, I. (2017). Attention is all you need. URL https://arxiv.org/abs/1706.03762 Tim Large, Yang Liu, Minyoung Huh, Hyojin Bahng, Phillip Isola, Jeremy Bernstein (2024). Scalable Optimization in the Modular Norm. URL https://arxiv.org/abs/2405.14813 Benjamin Thérien, Xiaolong Huang, Irina Rish, Eugene Belilovsky (2025). MuLoCo: Muon is a practical inner optimizer for DiLoCo. URL https://arxiv.org/abs/2505.23725 Arthur Douillard, Qixuan Feng, Andrei A. Rusu, Rachita Chhaparia, Yani Donchev, Adhiguna Kuncoro, Marc’Aurelio Ranzato, Arthur Szlam, Jiajun Shen (2024). DiLoCo: Distributed Low-Communication Training of Language Models. URL https://arxiv.org/abs/2311.08105 ",
  "wordCount" : "3578",
  "inLanguage": "en",
  "datePublished": "2025-07-06T00:00:00Z",
  "dateModified": "2025-07-06T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Franz Louis Cesista"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://leloykun.github.io/ponder/lipschitz-n-simplical-transformer/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Franz Louis Cesista",
    "logo": {
      "@type": "ImageObject",
      "url": "https://leloykun.github.io/favicon.ico"
    }
  }
}
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-BWCGBRX8G1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BWCGBRX8G1');
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"
  integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"
  integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js"
  integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
        { left: "\\begin{equation}", right: "\\end{equation}", display: true },
        { left: "\\begin{equation*}", right: "\\end{equation*}", display: true },
        { left: "\\begin{align}", right: "\\end{align}", display: true },
        { left: "\\begin{align*}", right: "\\end{align*}", display: true },
        { left: "\\begin{alignat}", right: "\\end{alignat}", display: true },
        { left: "\\begin{gather}", right: "\\end{gather}", display: true },
        { left: "\\begin{CD}", right: "\\end{CD}", display: true },
      ],
      throwOnError: false,
      trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
      macros: {
        "\\eqref": "\\href{###1}{(\\text{#1})}",
        "\\ref": "\\href{###1}{\\text{#1}}",
        "\\label": "\\htmlId{#1}{}"
      }
    });
  });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://leloykun.github.io/" accesskey="h" title="Franz Louis Cesista">
                <img src="https://leloykun.github.io/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Franz Louis Cesista</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://leloykun.github.io/cv.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/personal-projects/" title="Personal Projects">
                    <span>Personal Projects</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/ponder/" title="Ponder (Posts)">
                    <span>Ponder (Posts)</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Sensitivity and Sharpness of n-Simplical Attention
    </h1>
    <div class="post-meta"><span title='2025-07-06 00:00:00 +0000 UTC'>July 6, 2025</span>&nbsp;&middot;&nbsp;Franz Louis Cesista

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#preliminaries">Preliminaries</a></li>
    <li><a href="#sensitivity-of-n-simplical-attention">Sensitivity of n-Simplical Attention</a></li>
    <li><a href="#sharpness-of-n-simplical-attention">Sharpness of n-Simplical Attention</a></li>
    <li><a href="#discussion">Discussion</a></li>
    <li><a href="#how-to-cite">How to Cite</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>A team from Meta have recently shown that 2-simplical attention improves the exponent in the scaling laws vs. vanilla attention (Roy et al., 2025; Clift et al., 2019, Vaswani et al., 2017). This means that while it may not be as good or even worse than vanilla attention flops-vs-loss-wise at smaller scales, the trade-off gets better as the model scales up. This would be useful in e.g. large-scale reasoning-LLM training runs where context lengths could blow up to millions, even billions of tokens. It is also very Bitter Lesson-pilled: compute exponentially scales over time and having a compute sponge which we can pour more compute into and get better results is great.</p>
<p>And if we are to scale this up, we have to consider two questions:</p>
<ol>
<li>If 2-simplical attention is better than (vanilla) 1-simplical attention at scale, then would $n$-simplical attention be better than 2-simplical attention for $n \geq 3$?</li>
<li>How do we guarantee that our activation and gradient norms are &lsquo;stable&rsquo; during training as we scale up the model?</li>
</ol>
<p>In this blog post, we will focus on the latter, however we will consider $n$-simplical attention in general in our analyses.</p>
<blockquote>
<p><strong>Definition 1 (n-Simplical Attention):</strong> Let $q, k^{(1:n)}, v^{(1:n)} \in \mathbb{R}^{T \times d}$ be the query, keys, and values, where $T$ is the sequence length and $d$ is the model width. The n-simplical attention function $\texttt{F}$ is defined as follows,
$$\begin{aligned}
\texttt{F}(q, k^{(1:n)}, v^{(1:n)})
&amp;= {\color{blue}s_2} \texttt{softmax}\left({\color{blue}s_1} \langle q, k^{(1)}, k^{(2)}, \ldots, k^{(n)} \rangle + M\right) ( v^{(1)} \circ v^{(2)} \circ \ldots \circ v^{(n)} )\\
&amp;= {\color{blue}s_2} \texttt{softmax}\left({\color{blue}s_1} \left\langle q, \left( \prod_{t=1}^n \circ k^{(t)} \right) \right\rangle + M\right) \left( \prod_{t=1}^n \circ v^{(t)} \right)
\end{aligned}$$
where $\texttt{softmax}(\cdot)$ is applied to all indices except the first.</p>
<p>Examples:</p>
<ol>
<li>Vanilla Attention (Vaswani et al., 2017), $$\texttt{F}(q, k, v) = \texttt{softmax}\left(\frac{1}{\sqrt{d}} qk^T + M\right) v$$</li>
<li>2-Simplical Attention (Clift et al., 2019), $$\texttt{F}(q, k^{(1)}, k^{(2)}, v^{(1)}, v^{(2)}) = \texttt{softmax}\left(\frac{1}{\sqrt{d}} \langle q, k^{(1)}, k^{(2)} \rangle + M\right) ( v^{(1)} \circ v^{(2)} )$$</li>
</ol>
<p>Note that for both of these examples, $s_1 = 1/\sqrt{d}$ and $s_2 = 1$.</p>
</blockquote>
<p>And more formally, what we mean by activation norms being &ldquo;stable&rdquo; is that tiny changes in the inputs should not cause unexpectedly large changes in the outputs. We call this property <em>module sensitivity</em>. Likewise, we want the gradients to not blow up either, i.e. tiny changes in the inputs should not cause unexpectedly large changes in the gradients. We call this property <em>module sharpness</em>. And following Large et al. (2024), we formalize module sensitivity and sharpness as follows,</p>
<blockquote>
<p><strong>Definition 2 (Sensitivity):</strong> Let $M$ be a module on $(\mathcal{X}, \mathcal{Y}, \mathcal{W})$ where $\mathcal{X}$ is the input space with norm $\|\cdot\|_{\mathcal{X}}$, $\mathcal{Y}$ is the output space with norm $\|\cdot\|_\mathcal{Y}$, and $\mathcal{W}$ is the parameter space. We define $M$ to be $\sigma$-sensitive if,
$$\begin{equation}
\| \nabla M(w, x) \diamond \Delta x \|_{\mathcal{Y}} \leq \sigma \| \Delta x \|_{\mathcal{X}} \qquad\forall w \in \mathcal{W}; x, \Delta x \in \mathcal{X}
\end{equation}$$</p>
</blockquote>
<blockquote>
<p><strong>Definition 3 (Sharpness):</strong> Let $M$ be a module on $(\mathcal{X}, \mathcal{Y}, \mathcal{W})$ where $\mathcal{X}$ is the input space with norm $\|\cdot\|_{\mathcal{X}}$, $\mathcal{Y}$ is the output space with norm $\|\cdot\|_\mathcal{Y}$, and $\mathcal{W}$ is the parameter space. We define $M$ to be $\gamma$-sharp if,
$$\begin{equation}
\| \tilde{\Delta} x \diamond \nabla^2 M(w, x) \diamond \Delta x \|_{\mathcal{Y}} \leq \gamma \| \Delta x \|_{\mathcal{X}} \| \tilde{\Delta} x \|_{\mathcal{X}} \qquad\forall w \in \mathcal{W}; x, \Delta x, \tilde{\Delta} x \in \mathcal{X}
\end{equation}$$</p>
</blockquote>
<p>Note that if $\mathcal{X}$ and $\mathcal{Y}$ are normed vector spaces, then the sensitivity bounds the (forward) Lipschitz constant of the module, and the sharpness bounds the (backward) <em>gradient</em> Lipschitz constant. Having unit sensitivity means that a small change in the input can only cause at most as much change in the output. Likewise, having unit sharpness means that a small change in the input can only cause at most as much change in the gradient.</p>
<p>In this blog post, we will show that $n$-simplical attention is unit sensitive and $(1 + \tilde{L}_{\texttt{softmax}})$-sharp under the $\infty RMS$ operator norm, where $\tilde{L}_{\texttt{softmax}}$ is the <em>gradient</em> Lipschitz constant of the softmax function.</p>
<blockquote>
<p><strong>Claim 4 (Sensitivity and sharpness of n-Simplical Attention):</strong> Let $q, k^{(1:n)}, v^{(1:n)} \in \mathbb{R}^{T \times d}$ be the query, keys, and values, where $T$ is the sequence length and $d$ is the model width. $n$-simplical attention parameterized as follows,
$$\begin{equation}
\texttt{F}(q, k^{(1:n)}, v^{(1:n)}) = {\color{blue}\frac{1}{d^{(n-1)/2}}} \texttt{softmax}\left({\color{blue}\frac{1}{d^{(n+1)/2}}} \left\langle q, \left( \prod_{t=1}^n \circ k^{(t)} \right) \right\rangle + M\right) \left( \prod_{t=1}^n \circ v^{(t)} \right)
\end{equation}$$
is unit sensitive and $(1 + \tilde{L}_{\texttt{softmax}})$-sharp under the $\infty RMS$ operator norm where $d$ is the model width and $\tilde{L}_{\texttt{softmax}}$ is the <em>gradient</em> Lipschitz constant of the softmax function.</p>
</blockquote>
<h2 id="preliminaries">Preliminaries<a hidden class="anchor" aria-hidden="true" href="#preliminaries">#</a></h2>
<p>First, let&rsquo;s rewrite n-Simplical Attention in Claim 4 above as follows,</p>
<p>$$\begin{align}
S &amp;= s_1 \left\langle q, \left( \prod_{t=1}^n \circ k^{(t)} \right) \right\rangle \qquad &amp; s_1 &amp;= \frac{1}{d^{(n+1)/2}} \\
A &amp;= \texttt{softmax}\left(S + M\right) \\
W &amp;= s_2 \prod_{t=1}^n \circ v^{(t)} \qquad &amp; s_2 &amp;= \frac{1}{d^{(n-1)/2}}\\
F &amp;= A W
\end{align}$$</p>
<p>We chose the scaling factor $s_2 = \frac{1}{d^{(n-1)/2}}$ so that $\| W \|_{\infty RMS} \leq 1$ for unit RMS norm values. This follows directly from Lemma 6 below. As for the scaling factor $s_1 = \frac{1}{d^{(n+1)/2}}$, we chose it so that the entries of $S$ are bounded by $1$ (see Lemma 7), making (masked) softmax 1-Lipschitz. This property is crucial for our proofs later on.</p>
<blockquote>
<p><strong>Proposition 5 (RMS norm of hadamard product of vectors):</strong> Let $x, y \in \mathbb{R}^d$ be vectors. Then the RMS norm of their hadamard product is bounded by the RMS norms of the individual vectors,
$$\begin{equation}\| x \circ y \|_{RMS} \leq \sqrt{d} \| x \|_{RMS} \| y \|_{RMS} \end{equation}$$</p>
</blockquote>


<p><details >
  <summary markdown="span">Show <strong>proof of Proposition 5</strong></summary>
  <blockquote>
<p><strong>Proof:</strong>
$$\begin{aligned}
\left\| x \circ y \right\|_{RMS}^2
&amp;= \left\| x \circ y \right\|_{RMS}^2 \\
&amp;= \left(\frac{1}{\sqrt{d}}\right)^2\left\| x \circ y \right\|_{2}^2 \\
&amp;= \frac{1}{d} \sum_{r=1}^d (x_r)^2 (y_r)^2 \\
&amp;\leq \frac{1}{d} \left(\sum_{r=1}^d (x_r)^4 \right)^{1/2} \left(\sum_{r=1}^d (y_r)^4\right)^{1/2} &amp;\text{(from Cauchy-Schwarz)}\\
&amp;\leq \frac{1}{d} \left(\sum_{r=1}^d (x_r)^2 \right) \left(\sum_{r=1}^d (y_r)^2\right)  &amp;\text{(from Jensen&rsquo;s Lemma)}\\
&amp;\leq \frac{1}{d} \| x_r \|_2^2 \| y_r \|_2^2 \\
\left\| x \circ y \right\|_{RMS}^2
&amp;\leq d \| x \|_{RMS}^2 \| y \|_{RMS}^2 \\
\left\| x \circ y \right\|_{RMS}
&amp;\leq \sqrt{d} \| x \|_{RMS}^2 \| y \|_{RMS}^2 \quad\blacksquare
\end{aligned}$$</p>
</blockquote>
</details></p>

<blockquote>
<p><strong>Lemma 6 (RMS norm of hadamard product of <em>unit RMS norm</em> vectors):</strong> Let $x^{(1)}, x^{(2)}, \ldots, x^{(n)} \in \mathbb{R}^d$ be vectors with $\| x^{(t)} \|_{RMS} \leq 1$ for all $t$. Then,
$$\begin{equation}\left\| \prod_{t=1}^n \circ x^{(t)} \right\|_{RMS} \leq d^{(n-1)/2}\end{equation}$$</p>
</blockquote>
<p>The proof follows directly from Proposition 5.</p>
<blockquote>
<p><strong>Lemma 7:</strong> For unit RMS norm query $q$ and keys $k^{(1:n)}$, the choice of scaling factor $s_1 = \frac{1}{d^{(n+1)/2}}$ bounds the entries of $S$ by $1$.</p>
</blockquote>


<p><details >
  <summary markdown="span">Show <strong>proof of Lemma 7</strong></summary>
  <blockquote>
<p><strong>Proof:</strong> From Lemma 6, we have,
$$\| w(n) \|_{\infty RMS} := \left\| \prod_{t=1}^n \circ k^{(t)} \right\|_{\infty RMS} \leq d^{(n-1)/2}$$
Thus,
$$\begin{aligned}
| \langle q, w(n) \rangle |
&amp;\leq \| q \|_2 \| w(n) \|_2  \\
&amp;\leq (\sqrt{d} \cancel{\| q \|_{\infty RMS}})( \sqrt{d} \| w(n) \|_{\infty RMS}) \\
&amp;\leq d d^{(n-1)/2} \\
| \langle q, w(n) \rangle |
&amp;= d^{(n+1)/2}
\end{aligned}$$
Thus the entries of $S$ are bounded by,
$$| S_{i,J} | = \frac{1}{d^{(n+1)/2}} | \langle q_i, w(n)_J \rangle | \leq \frac{1}{d^{(n+1)/2}} d^{(n+1)/2} = 1 \quad\blacksquare$$</p>
</blockquote>
</details></p>

<h2 id="sensitivity-of-n-simplical-attention">Sensitivity of n-Simplical Attention<a hidden class="anchor" aria-hidden="true" href="#sensitivity-of-n-simplical-attention">#</a></h2>
<p>We wish to show that the n-simplical attention is unit sensitive for unit RMS norm inputs $(q, k^{(1:n)}, v^{(1:n)}) \in \mathcal{X}$.</p>
<blockquote>
<p><strong>Claim 8:</strong> Let $q, k^{(1:n)}, v^{(1:n)} \in \mathbb{R}^{T \times d}$ be the query, keys, and values, where $T$ is the sequence length and $d$ is the model width. For $\| q \|_{\infty RMS} = \| k^{(t)} \|_{\infty RMS} = \| v^{(t)} \|_{\infty RMS} = 1$ for all $t$, the n-simplical attention function $\texttt{F}$ is unit sensitive under the $\infty RMS$ operator norm. That is, for any perturbation $(\Delta q, \Delta k^{(1:n)}, \Delta v^{(1:n)}) \in \mathcal{X}$, we have,
$$\begin{aligned}
\| \nabla F \diamond ( \Delta q, \Delta k^{(1:n)}, \Delta v^{(1:n)} ) \|_{\infty RMS}
&amp;\leq \| (\Delta q, \Delta k^{[1:n]}, \Delta v^{[1:n]}) \|_{\infty RMS} \\
&amp;\leq \| \Delta q \|_{\infty RMS} + \sum_{t=1}^{n} \| \Delta k^{(t)} \|_{\infty RMS} + \sum_{t=1}^{n} \| \Delta v^{(t)} \|_{\infty RMS}\\
\end{aligned}$$</p>
</blockquote>
<p>To prove this, let&rsquo;s first take the derivative of $\texttt{F}$ towards $(\Delta q, \Delta k^{(1:n)}, \Delta v^{(1:n)})$,</p>
<p>$$\begin{align}
\nabla F \diamond ( \Delta q, \Delta k^{(1:n)}, \Delta v^{(1:n)} ) &amp;= (\Delta A) W + A (\Delta W) \\
\| \nabla F \diamond ( \Delta q, \Delta k^{(1:n)}, \Delta v^{(1:n)} ) \|_{\infty RMS}
&amp;\leq \| \Delta A \|_{\infty RMS} \| W \|_{\infty RMS} + \| A \|_{\infty RMS} \| \Delta W \|_{\infty RMS}\\
\end{align}$$</p>
<p>We have already shown earlier that,
$$\| W \|_{\infty RMS} = \| A \|_{\infty RMS} = 1$$
by construction. And so we only need to derive $\| \Delta A \|_{\infty RMS}$ and $\| \Delta W \|_{\infty RMS}$.</p>
<hr>
<p>As for the $\Delta A$ term, by the chain-rule, we have,
$$\begin{equation}\| \Delta A \|_{\infty RMS} \leq \| \nabla \texttt{softmax}(S)[\Delta S] \|_{\infty -op} \| \Delta S \|_{\infty RMS}\end{equation}$$
and since the softmax is 1-Lipschitz with respect to the $\infty RMS$ norm with our parameterization, we have,
$$\| \Delta A \|_{\infty RMS} \leq \| \Delta S \|_{\infty RMS}$$</p>
<p>By the product rule, we have,</p>
<p>$$\begin{align}
\Delta S
&amp;= \frac{1}{d^{(n+1)/2}} \left\langle \Delta q, \left( \prod_{t=1}^n \circ k^{(t)} \right) \right\rangle + \frac{1}{d^{(n+1)/2}}  \sum_{t=1}^{n} \left\langle q, \Delta k^{(t)} \circ \left( \prod_{s=1,s\neq t}^n \circ k^{(s)} \right) \right\rangle \\
\Delta S
&amp;= \frac{1}{d^{(n+1)/2}} \left\langle \Delta q, \left( \prod_{t=1}^n \circ k^{(t)} \right) \right\rangle + \frac{1}{d^{(n+1)/2}}  \sum_{t=1}^{n} \left\langle \Delta k^{(t)}, q \circ \left( \prod_{s=1,s\neq t}^n \circ k^{(s)} \right) \right\rangle \nonumber\\
\end{align}$$</p>
<p>Thus,</p>
<p>$$\begin{aligned}
\| \Delta S \|_{\infty RMS}
&amp;\leq \frac{1}{d^{(n+1)/2}} \| \Delta q \|_{2} \left \| \prod_{t=1}^n \circ k^{(t)} \right\|_{2} + \frac{d}{d^{(n+1)/2}} \sum_{t=1}^{n} \| \Delta k^{(t)} \|_{2} \left\| q \circ \prod_{s=1,s\neq t}^n \circ k^{(s)} \right\|_{2}\\
&amp;\leq \frac{d}{d^{(n+1)/2}} \| \Delta q \|_{\infty RMS} \left \| \prod_{t=1}^n \circ k^{(t)} \right\|_{\infty RMS}\\
&amp;\quad + \frac{d}{d^{(n+1)/2}} \sum_{t=1}^{n} \| \Delta k^{(t)} \|_{\infty RMS} \left\| q \circ \prod_{s=1,s\neq t}^n \circ k^{(s)} \right\|_{\infty RMS}\\
&amp;\leq \cancel{\frac{dd^{(n-1)/2}}{d^{(n+1)/2}}} \left( \| \Delta q \|_{\infty RMS} + \sum_{t=1}^{n} \| \Delta k^{(t)} \|_{\infty RMS} \right)\\
\| \Delta S \|_{\infty RMS} &amp;\leq  \| \Delta q \|_{\infty RMS} + \sum_{t=1}^{n} \| \Delta k^{(t)} \|_{\infty RMS}
\end{aligned}$$</p>
<p>Thus, $$\begin{equation}
\| \Delta A \|_{\infty RMS} \leq \| \Delta q \|_{\infty RMS} + \sum_{t=1}^{n} \| \Delta k^{(t)} \|_{\infty RMS}
\end{equation}$$</p>
<hr>
<p>As for the $\Delta W$ term, by the product rule, we have,</p>
<p>$$\begin{align}
\Delta W
&amp;= \frac{1}{d^{(n-1)/2}}  \sum_{t=1}^{n} \left[ \Delta v^{(t)} \circ \left( \prod_{s=1,s\neq t}^n \circ v^{(s)} \right)\right]
\end{align}$$</p>
<p>Thus,</p>
<p>$$\begin{align}
\| \Delta W \|_{\infty RMS}
&amp;\leq \frac{1}{d^{(n-1)/2}}  \sum_{t=1}^{n} \left\| \Delta v^{(t)} \circ \prod_{s=1,s\neq t}^n \circ v^{(s)} \right\|_{\infty RMS}\nonumber\\
&amp;\leq \frac{\sqrt{d}}{d^{(n-1)/2}}  \sum_{t=1}^{n} \| \Delta v^{(t)} \|_{\infty RMS} \left\| \prod_{s=1,s\neq t}^n \circ v^{(s)} \right\|_{\infty RMS} &amp;\text{(from Proposition 5)}\nonumber\\
&amp;\leq \cancel{\frac{d^{1/2}d^{(n-2)/2}}{d^{(n-1)/2}}}  \sum_{t=1}^{n} \| \Delta v^{(t)} \|_{\infty RMS}&amp;\text{(from Lemma 6)}\nonumber\\
\| \Delta W \|_{\infty RMS} &amp;= \sum_{t=1}^{n} \| \Delta v^{(t)} \|_{\infty RMS}
\end{align}$$</p>
<hr>
<p>Combining Equations (11), (14), and (16) then gives us,</p>
<p>$$\begin{aligned}
\| \nabla F \diamond \langle \Delta q, \Delta k^{(1:n)}, \Delta v^{(1:n)} \rangle \|_{\infty RMS}
&amp;\leq \| \Delta q \|_{\infty RMS} + \sum_{t=1}^{n} \| \Delta k^{(t)} \|_{\infty RMS} + \sum_{t=1}^{n} \| \Delta v^{(t)} \|_{\infty RMS}\\
\| \nabla F \diamond \langle \Delta q, \Delta k^{(1:n)}, \Delta v^{(1:n)} \rangle \|_{\infty RMS}
&amp;\leq \| (q, k^{[1:n]}, v^{[1:n]}) \|_{\infty RMS}
\end{aligned}$$</p>
<p>Hence, n-simplical attention is unit sensitive under the $\infty RMS$ operator norm as claimed.</p>
<h2 id="sharpness-of-n-simplical-attention">Sharpness of n-Simplical Attention<a hidden class="anchor" aria-hidden="true" href="#sharpness-of-n-simplical-attention">#</a></h2>
<p>Next, we wish to show that the n-simplical attention is $(1+\tilde{L}_{\texttt{softmax}})$-sharp for unit RMS norm inputs $(q, k^{(1:n)}, v^{(1:n)}) \in \mathcal{X}$. More formally,</p>
<blockquote>
<p><strong>Claim 9:</strong> Let $q, k^{(1:n)}, v^{(1:n)} \in \mathbb{R}^{T \times d}$ be the query, keys, and values, where $T$ is the sequence length and $d$ is the model width. For $\| q \|_{\infty RMS} = \| k^{(t)} \|_{\infty RMS} = \| v^{(t)} \|_{\infty RMS} = 1$ for all $t$, the n-simplical attention function $\texttt{F}$ is unit sensitive under the $\infty RMS$ operator norm. That is, for any pair of perturbations $(\Delta q, \Delta k^{(1:n)}, \Delta v^{(1:n)}), (\tilde{\Delta} q, \tilde{\Delta} k^{(1:n)}, \tilde{\Delta} v^{(1:n)}) \in \mathcal{X}$, we have,
$$\begin{aligned}
&amp;\| (\tilde{\Delta} q, \tilde{\Delta} k^{(1:n)}, \tilde{\Delta} v^{(1:n)}) \diamond \nabla F \diamond ( \Delta q, \Delta k^{(1:n)}, \Delta v^{(1:n)} ) \|_{\infty RMS}\\
&amp;\qquad\qquad \leq (1+\tilde{L}_{\texttt{softmax}})\| (\Delta q, \Delta k^{[1:n]}, \Delta v^{[1:n]}) \|_{\infty RMS} \| (\tilde{\Delta} q, \tilde{\Delta} k^{[1:n]}, \tilde{\Delta} v^{[1:n]}) \|_{\infty RMS} \\
&amp;\qquad\qquad \leq (1+\tilde{L}_{\texttt{softmax}})\left(\| \Delta q \|_{\infty RMS} + \sum_{t=1}^{n} \| \Delta k^{(t)} \|_{\infty RMS} + \sum_{t=1}^{n} \| \tilde{\Delta} v^{(t)} \|_{\infty RMS}\right)\\
&amp;\qquad\qquad\qquad\qquad\qquad\quad \times \left(\| \tilde{\Delta} q \|_{\infty RMS} + \sum_{t=1}^{n} \| \tilde{\Delta} k^{(t)} \|_{\infty RMS} + \sum_{t=1}^{n} \| \tilde{\Delta} v^{(t)} \|_{\infty RMS}\right)
\end{aligned}$$</p>
</blockquote>
<p>To prove this, let&rsquo;s first take the derivative of Equation (10) towards $(\tilde{\Delta} q, \tilde{\Delta} k^{(1:n)}, \tilde{\Delta} v^{(1:n)})$,</p>
<p>$$\begin{align}
&amp;\langle \tilde{\Delta} q, \tilde{\Delta} k^{(1:n)}, \tilde{\Delta} v^{(1:n)} \rangle \diamond \nabla^2 F \diamond \langle \Delta q, \Delta k^{(1:n)}, \Delta v^{(1:n)} \rangle\nonumber\\
&amp;\qquad\qquad= (\Delta^2 A) W + (\tilde{\Delta} A) (\Delta W) + (\Delta A) (\tilde{\Delta} W) + A (\Delta^2 W) \\
&amp;\| \langle \tilde{\Delta} q, \tilde{\Delta} k^{(1:n)}, \tilde{\Delta} v^{(1:n)} \rangle \diamond \nabla^2 F \diamond \langle \Delta q, \Delta k^{(1:n)}, \Delta v^{(1:n)} \rangle \|_{\infty RMS}\nonumber\\
&amp;\qquad\qquad\leq \| \Delta^2 A \|_{\infty RMS} \cancel{\| W \|_{\infty RMS}} + \| \tilde{\Delta} A \|_{\infty RMS} \| \Delta W \|_{\infty RMS} \nonumber\\
&amp;\qquad\qquad\quad + \| \Delta A \|_{\infty RMS} \| \tilde{\Delta} W \|_{\infty RMS} + \cancel{\| A \|_{\infty RMS}} \| \Delta^2 W \|_{\infty RMS} \\
\end{align}$$</p>
<p>We have already derived $\| \Delta A \|_{\infty RMS}$ and $\| \Delta W \|_{\infty RMS}$ in the previous section. And for $\| \tilde{\Delta} A \|_{\infty RMS}$ and $\| \tilde{\Delta} W \|_{\infty RMS}$, it would suffice to replace $\Delta$ with $\tilde{\Delta}$ in the previous derivations. Again, we also have $\| W \|_{\infty RMS} = \| A \|_{\infty RMS} = 1$ by construction. So, we only need to derive $\| \Delta^2 A \|_{\infty RMS}$ and $\| \Delta^2 W \|_{\infty RMS}$.</p>
<hr>
<p>For the $\Delta^2 A$ term, let&rsquo;s take the derivative of Equation (12) towards $\tilde{\Delta}$,</p>
<p>$$\begin{aligned}
\| \Delta^2 A \|_{\infty RMS}
&amp;\leq \| \nabla^2 \texttt{softmax}(S)[\Delta S, \tilde{\Delta} S] \|_{\infty -op} \| \Delta S \|_{\infty RMS} \| \tilde{\Delta} S \|_{\infty RMS}\\
&amp;\quad+ \cancel{\| \nabla \texttt{softmax}(S)[\Delta S] \|_{\infty -op}} \| \Delta^2 S \|_{\infty RMS}\\
\| \Delta^2 A \|_{\infty RMS}
&amp;\leq \tilde{L}_{\texttt{softmax}} \| \Delta S \|_{\infty RMS} \| \tilde{\Delta} S \|_{\infty RMS} + \| \Delta^2 S \|_{\infty RMS}\\
\end{aligned}$$</p>
<p>We have already derived $\| \Delta S \|_{\infty RMS}$ in the previous section. And for $\| \tilde{\Delta} S \|_{\infty RMS}$, it would suffice to replace $\Delta$ with $\tilde{\Delta}$ in the previous derivation. So, we only need to derive $\| \Delta^2 S \|_{\infty RMS}$. Applying the product rule to Equation (13), we have,</p>
<p>$$\begin{align}
\Delta^2 S
&amp;= \frac{1}{d^{(n+1)/2}} \sum_{t=1}^n\left\langle \Delta q, \tilde{\Delta} k^{(t)} \circ \left( \prod_{s=1,s\neq t}^n \circ k^{(s)} \right) \right\rangle \nonumber\\
&amp;\quad + \frac{1}{d^{(n+1)/2}}  \sum_{t=1}^{n} \left\langle \tilde{\Delta} q, \Delta k^{(t)} \circ \left( \prod_{s=1,s\neq t}^n \circ k^{(s)} \right) \right\rangle \nonumber\\
&amp;\quad + \frac{1}{d^{(n+1)/2}}  \sum_{1 \leq t &lt; s \leq n} \left\langle q, \Delta k^{(t)} \circ \tilde{\Delta} k^{(s)} \circ \left( \prod_{r=1,r\neq t,r\neq s}^n \circ k^{(r)} \right) \right\rangle
\end{align}$$</p>
<p>Thus,</p>
<p>$$\begin{aligned}
\| \Delta^2 S \|_{\infty RMS}
&amp;\leq \frac{1}{d^{(n+1)/2}} \| \Delta q \|_{2} \sum_{t=1}^n \left\| \tilde{\Delta} k^{(t)} \circ \prod_{s=1,s\neq t}^n \circ k^{(s)} \right\|_{2} \\
&amp;\quad + \frac{1}{d^{(n+1)/2}} \| \tilde{\Delta} q \|_{2} \sum_{t=1}^{n} \left\| \Delta k^{(t)} \circ \prod_{s=1,s\neq t}^n \circ k^{(s)} \right\|_{2} \\
&amp;\quad + \frac{1}{d^{(n+1)/2}} \| q \|_{2} \sum_{1 \leq t &lt; s \leq n} \left\| \Delta k^{(t)} \circ \tilde{\Delta} k^{(s)} \circ \prod_{r=1,r\neq t,r\neq s}^n \circ k^{(r)} \right\|_{2} \\
&amp;\leq \frac{d^{3/2}}{d^{(n+1)/2}} \| \Delta q \|_{\infty RMS} \sum_{t=1}^n \| \tilde{\Delta} k^{(t)} \|_{\infty RMS} \left\| \prod_{s=1,s\neq t}^n \circ k^{(s)} \right\|_{\infty RMS} \\
&amp;\quad + \frac{d^{3/2}}{d^{(n+1)/2}} \| \tilde{\Delta} q \|_{\infty RMS} \sum_{t=1}^{n} \| \Delta k^{(t)} \|_{\infty RMS} \left\| \prod_{s=1,s\neq t}^n \circ k^{(s)} \right\|_{\infty RMS} \\
&amp;\quad + \frac{d^{2}}{d^{(n+1)/2}} \cancel{\| q \|_{\infty RMS}} \sum_{1 \leq t &lt; s \leq n} \| \Delta k^{(t)} \|_{\infty RMS} \| \tilde{\Delta} k^{(s)} \|_{\infty RMS} \left\| \prod_{r=1,r\neq t,r\neq s}^n \circ k^{(r)} \right\|_{\infty RMS} \\
&amp;\leq \cancel{\frac{d^{3/2}d^{(n-2)/2}}{d^{(n+1)/2}}} \| \Delta q \|_{\infty RMS} \sum_{t=1}^n \| \tilde{\Delta} k^{(t)} \|_{\infty RMS} \\
&amp;\quad + \cancel{\frac{d^{3/2}d^{(n-2)/2}}{d^{(n+1)/2}}} \| \tilde{\Delta} q \|_{\infty RMS} \sum_{t=1}^{n} \| \Delta k^{(t)} \|_{\infty RMS} \\
&amp;\quad + \cancel{\frac{d^{2}d^{(n-3)/2}}{d^{(n+1)/2}}} \sum_{1 \leq t &lt; s \leq n} \| \Delta k^{(t)} \|_{\infty RMS} \| \tilde{\Delta} k^{(s)} \|_{\infty RMS} \\
\| \Delta^2 S \|_{\infty RMS}
&amp;\leq \left( \| \Delta q \|_{\infty RMS} + \sum_{t=1}^n \| \Delta k^{(t)} \|_{\infty RMS} \right) \left( \| \tilde{\Delta} q \|_{\infty RMS} + \sum_{t=1}^n \| \tilde{\Delta} k^{(t)} \|_{\infty RMS} \right)
\end{aligned}$$</p>
<p>Thus,</p>
<p>$$\begin{align}
\| \Delta^2 A \|_{\infty RMS} &amp;\leq (1+\tilde{L}_{\texttt{softmax}}) \left( \| \Delta q \|_{\infty RMS} + \sum_{t=1}^n \| \Delta k^{(t)} \|_{\infty RMS} \right) \left( \| \tilde{\Delta} q \|_{\infty RMS} + \sum_{t=1}^n \| \tilde{\Delta} k^{(t)} \|_{\infty RMS} \right)
\end{align}$$</p>
<hr>
<p>For the $\Delta^2 W$ term, let&rsquo;s take the derivative of Equation (15) towards $\tilde{\Delta}$,</p>
<p>$$\begin{align}
\Delta^2 W
&amp;= \frac{1}{d^{(n-1)/2}}  \sum_{1 \leq t &lt; s \leq n} \left[ \Delta v^{(t)} \circ \tilde{\Delta} v^{(s)} \circ \left( \prod_{r=1,r\neq t,r\neq s}^n \circ v^{(r)} \right)\right]
\end{align}$$</p>
<p>$$\begin{align}
\| \Delta^2 W \|_{\infty RMS}
&amp;\leq \frac{1}{d^{(n-1)/2}}  \sum_{1 \leq t &lt; s \leq n} \left\| \Delta v^{(t)} \circ \tilde{\Delta} v^{(s)} \circ \prod_{r=1,r\neq t,r\neq s}^n \circ v^{(r)} \right\|_{\infty RMS} \nonumber\\
&amp;\leq \frac{d}{d^{(n-1)/2}}  \sum_{1 \leq t &lt; s \leq n} \| \Delta v^{(t)} \|_{\infty RMS} \| \tilde{\Delta} v^{(s)} \|_{\infty RMS} \left\| \prod_{r=1,r\neq t,r\neq s}^n \circ v^{(r)} \right\|_{\infty RMS} \nonumber\\
&amp;\leq \cancel{\frac{dd^{(n-3)/2}}{d^{(n-1)/2}}} \sum_{1 \leq t &lt; s \leq n} \| \Delta v^{(t)} \|_{\infty RMS} \| \tilde{\Delta} v^{(s)} \|_{\infty RMS} \nonumber\\
\| \Delta^2 W \|_{\infty RMS}
&amp;\leq \left( \sum_{t=1}^n \| \Delta v^{(t)} \|_{\infty RMS} \right) \left( \sum_{t=1}^n \| \tilde{\Delta} v^{(t)} \|_{\infty RMS} \right)
\end{align}$$</p>
<hr>
<p>Combining Equations (18), (20), and (22) then gives us,</p>
<p>$$\begin{aligned}
&amp;\| \langle \tilde{\Delta} q, \tilde{\Delta} k^{(1:n)}, \tilde{\Delta} v^{(1:n)} \rangle \diamond \nabla^2 F \diamond \langle \Delta q, \Delta k^{(1:n)}, \Delta v^{(1:n)} \rangle \|_{\infty RMS} \\
&amp;\quad \leq (1 + \tilde{L}_{\texttt{softmax}}) \left( \| \Delta q \|_{\infty RMS} + \sum_{t=1}^n \| \Delta k^{(t)} \|_{\infty RMS} \right) \left( \| \tilde{\Delta} q \|_{\infty RMS} + \sum_{t=1}^n \| \tilde{\Delta} k^{(t)} \|_{\infty RMS} \right)\\
&amp;\qquad + \left( \| \tilde{\Delta} q \|_{\infty RMS} + \sum_{t=1}^{n} \| \tilde{\Delta} k^{(t)} \|_{\infty RMS} \right) \left( \sum_{t=1}^{n} \| \Delta v^{(t)} \|_{\infty RMS} \right) \\
&amp;\qquad + \left( \| \Delta q \|_{\infty RMS} + \sum_{t=1}^{n} \| \Delta k^{(t)} \|_{\infty RMS} \right) \left( \sum_{t=1}^{n} \| \tilde{\Delta} v^{(t)} \|_{\infty RMS} \right) \\
&amp;\qquad + \left( \sum_{t=1}^n \| \Delta v^{(t)} \|_{\infty RMS} \right) \left( \sum_{t=1}^n \| \tilde{\Delta} v^{(t)} \|_{\infty RMS} \right) \\
&amp;\quad \leq (1 + \tilde{L}_{\texttt{softmax}}) \left( \| \Delta q \|_{\infty RMS} + \sum_{t=1}^{n} \| \Delta k^{(t)} \|_{\infty RMS} + \sum_{t=1}^{n} \| \Delta v^{(t)} \|_{\infty RMS} \right) \\
&amp;\qquad\qquad\qquad\qquad \left( \| \tilde{\Delta} q \|_{\infty RMS} + \sum_{t=1}^{n} \| \tilde{\Delta} k^{(t)} \|_{\infty RMS} + \sum_{t=1}^{n} \| \tilde{\Delta} v^{(t)} \|_{\infty RMS} \right)
\end{aligned}$$</p>
<p>Hence, n-simplical attention is $(1+\tilde{L}_{\texttt{softmax}})$-sharp under the $\infty RMS$ operator norm as claimed.</p>
<h2 id="discussion">Discussion<a hidden class="anchor" aria-hidden="true" href="#discussion">#</a></h2>
<p>Here we have devised a parametrization that allows us to have width-independent sensitivity and sharpness bounds for n-simplical attention. We hope that this will allow us to construct a maximum update parametrization of some sort for such modules and networks containing them.</p>
<p>Note however that for $n = 1$, we have to set the scaling factor $s_1 = \frac{1}{d^{(1+1)/2}} = \frac{1}{d}$, which is the same scaling factor suggested by Large et al. (2024), but is different from the more standard $s_1 = \frac{1}{\sqrt{d}}$. Likewise, for 2-simplical attention, we have to set the scaling factor $s_1 = \frac{1}{d^{(2+1)/2}} = \frac{1}{d^{3/2}}$, which is different from the $s_1 = \frac{1}{\sqrt{d}}$ used by Roy et al. (2025). Additionally, we also have to set $s_2 = \frac{1}{d^{(2-1)/2}} = \frac{1}{\sqrt{d}}$ for the outer scale in 2-simplical attention, which, for larger dimensions, scales down the outputs significantly. Empirically, such parametrization leads to worse performance early in training, but guarantees stable training, especially at the tail end of training where the queries, keys, and values are more often aligned than not.</p>
<p>The main benefit of having low (and width-independent) sensitivity and sharpness really is that it allows us to have larger update step sizes without worrying about suddenly exploding or vanishing activations and gradients. Additionally, bounding the sensitivity allows us to control how much the gradients change as they pass through the module via backpropagation&ndash;the smaller the sensitivity, the smaller the change in the gradients. And bounding the sharpness allows us to have more trust in the momentum term more knowing that gradient spikes would rarely happen, if at all. These gradient spikes notoriously &lsquo;break&rsquo; the momentum term at larger traning runs, especially near the end of training.</p>
<p>Lastly, this could also be useful in distributed training setups where gradient all-reduces are expensive and thus sparsifying the gradients before sending them over the network is a must (Douillard et al., 2024; Thérien et al., 2025). Problem arises when the gradients have outliers, requiring us to use more expensive quantization schemes to avoid losing information. But having control over the gradient norms should allow us to eliminate such outliers and get low-precision (and thus low-communication) training basically &ldquo;for free&rdquo;.</p>
<h2 id="how-to-cite">How to Cite<a hidden class="anchor" aria-hidden="true" href="#how-to-cite">#</a></h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bibtex" data-lang="bibtex"><span style="display:flex;"><span><span style="color:#0a0;text-decoration:underline">@misc</span>{cesista2025sensitivitysharpnessnsimplicalattention,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">author</span> = <span style="color:#a50">{Franz Louis Cesista}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">title</span> = <span style="color:#a50">{&#34;Sensitivity and Sharpness of n-Simplical Attention&#34;}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">year</span> = <span style="color:#a50">{2025}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">url</span> = <span style="color:#a50">{https://leloykun.github.io/ponder/lipschitz-n-simplical-transformer/}</span>,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ol>
<li>Aurko Roy, Timothy Chou, Sai Surya Duvvuri, Sijia Chen, Jiecao Yu, Xiaodong Wang, Manzil Zaheer, Rohan Anil (2025). Fast and Simplex: 2-Simplicial Attention in Triton. URL <a href="https://arxiv.org/abs/2507.02754v1" target="_blank">https://arxiv.org/abs/2507.02754v1</a></li>
<li>James Clift, Dmitry Doryn, Daniel Murfet, James Wallbridge (2019). Logic and the -Simplicial Transformer. URL <a href="https://arxiv.org/abs/1909.00668" target="_blank">https://arxiv.org/abs/1909.00668</a></li>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., &hellip; &amp; Polosukhin, I. (2017). Attention is all you need. URL <a href="https://arxiv.org/abs/1706.03762" target="_blank">https://arxiv.org/abs/1706.03762</a></li>
<li>Tim Large, Yang Liu, Minyoung Huh, Hyojin Bahng, Phillip Isola, Jeremy Bernstein (2024). Scalable Optimization in the Modular Norm. URL <a href="https://arxiv.org/abs/2405.14813" target="_blank">https://arxiv.org/abs/2405.14813</a></li>
<li>Benjamin Thérien, Xiaolong Huang, Irina Rish, Eugene Belilovsky (2025). MuLoCo: Muon is a practical inner optimizer for DiLoCo. URL <a href="https://arxiv.org/abs/2505.23725" target="_blank">https://arxiv.org/abs/2505.23725</a></li>
<li>Arthur Douillard, Qixuan Feng, Andrei A. Rusu, Rachita Chhaparia, Yani Donchev, Adhiguna Kuncoro, Marc&rsquo;Aurelio Ranzato, Arthur Szlam, Jiajun Shen (2024). DiLoCo: Distributed Low-Communication Training of Language Models. URL <a href="https://arxiv.org/abs/2311.08105" target="_blank">https://arxiv.org/abs/2311.08105</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://leloykun.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://leloykun.github.io/tags/optimizers/">Optimizers</a></li>
      <li><a href="https://leloykun.github.io/tags/architecture-optimizer-codesign/">Architecture-Optimizer Codesign</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Sensitivity and Sharpness of n-Simplical Attention on x"
            href="https://x.com/intent/tweet/?text=Sensitivity%20and%20Sharpness%20of%20n-Simplical%20Attention&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2flipschitz-n-simplical-transformer%2f&amp;hashtags=MachineLearning%2cOptimizers%2cArchitecture-OptimizerCodesign">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Sensitivity and Sharpness of n-Simplical Attention on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2flipschitz-n-simplical-transformer%2f&amp;title=Sensitivity%20and%20Sharpness%20of%20n-Simplical%20Attention&amp;summary=Sensitivity%20and%20Sharpness%20of%20n-Simplical%20Attention&amp;source=https%3a%2f%2fleloykun.github.io%2fponder%2flipschitz-n-simplical-transformer%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Sensitivity and Sharpness of n-Simplical Attention on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fleloykun.github.io%2fponder%2flipschitz-n-simplical-transformer%2f&title=Sensitivity%20and%20Sharpness%20of%20n-Simplical%20Attention">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Sensitivity and Sharpness of n-Simplical Attention on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fleloykun.github.io%2fponder%2flipschitz-n-simplical-transformer%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Sensitivity and Sharpness of n-Simplical Attention on whatsapp"
            href="https://api.whatsapp.com/send?text=Sensitivity%20and%20Sharpness%20of%20n-Simplical%20Attention%20-%20https%3a%2f%2fleloykun.github.io%2fponder%2flipschitz-n-simplical-transformer%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Sensitivity and Sharpness of n-Simplical Attention on telegram"
            href="https://telegram.me/share/url?text=Sensitivity%20and%20Sharpness%20of%20n-Simplical%20Attention&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2flipschitz-n-simplical-transformer%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Sensitivity and Sharpness of n-Simplical Attention on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Sensitivity%20and%20Sharpness%20of%20n-Simplical%20Attention&u=https%3a%2f%2fleloykun.github.io%2fponder%2flipschitz-n-simplical-transformer%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://leloykun.github.io/">Franz Louis Cesista</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
