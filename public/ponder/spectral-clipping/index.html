<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping via Newton-Schulz Iteration | Franz Louis Cesista</title>
<meta name="keywords" content="Machine Learning, Optimizers, Architecture-Optimizer Codesign">
<meta name="description" content="A small step towards hardware-architecture-optimizer codesign in deep learning.">
<meta name="author" content="Franz Louis Cesista">
<link rel="canonical" href="https://leloykun.github.io/ponder/spectral-clipping/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e50e51e1b615f62676c32fb60e15e4b5e3a80ade29f3c6f9c953c528f576fb9f.css" integrity="sha256-5Q5R4bYV9iZ2wy&#43;2DhXkteOoCt4p88b5yVPFKPV2&#43;58=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://leloykun.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://leloykun.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://leloykun.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://leloykun.github.io/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://leloykun.github.io/ponder/spectral-clipping/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping via Newton-Schulz Iteration" />
<meta property="og:description" content="A small step towards hardware-architecture-optimizer codesign in deep learning." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://leloykun.github.io/ponder/spectral-clipping/" />
<meta property="og:image" content="https://leloykun.github.io/ponder/spectral-clipping/clip_lifting_trap_fix.png" /><meta property="article:section" content="ponder" />
<meta property="article:published_time" content="2025-06-05T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-06-05T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://leloykun.github.io/ponder/spectral-clipping/clip_lifting_trap_fix.png" />
<meta name="twitter:title" content="Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping via Newton-Schulz Iteration"/>
<meta name="twitter:description" content="A small step towards hardware-architecture-optimizer codesign in deep learning."/>
<meta name="twitter:site" content="@leloykun"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Ponder",
      "item": "https://leloykun.github.io/ponder/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping via Newton-Schulz Iteration",
      "item": "https://leloykun.github.io/ponder/spectral-clipping/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping via Newton-Schulz Iteration",
  "name": "Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping via Newton-Schulz Iteration",
  "description": "A small step towards hardware-architecture-optimizer codesign in deep learning.",
  "keywords": [
    "Machine Learning", "Optimizers", "Architecture-Optimizer Codesign"
  ],
  "articleBody": "1. Introduction Here I’ll discuss a fast, numerically stable, and (auto-)differentiable way to perform spectral clipping, i.e., clipping the singular values of a matrix to a certain range. This is useful in deep learning because it allows us to control the ‘growth’ of our weights and weight updates, enabling faster and stabler feature learning (Yang et al., 2024; Large et al., 2024). As discussed in a previous post,\nIf we want the Euclidean norm of our features and feature updates to ‘grow’ with the model size, then the Spectral norm of our weights and weight updates must also ‘grow’ with the model size.\nThere are multiple ways to control the spectral norm of our (matrix-structured) weights and weight updates. One is to “pull” all of the singular values to some target value chosen a priori via the matrix sign function $\\texttt{misgn}$. This is what the Muon optimizer already does, but only on the weight updates: it takes the raw gradient and tries to “pull” its as many of its singular values to $\\sqrt{\\frac{d_{out}}{d_{in}}}$. This guarantees that the update step merely changes the activation RMS-norm of that layer by at most $1$ unit. We could also apply this process to the weights after every update step to guarantee that the weight norms would not blow up, but constraining the weight space to the Stiefel manifold is too strong of a constraint. We discuss more of this in our upcoming Neurips preprint. For now, we will focus on Spectral Clipping:\nDefinition 1 (Spectral Clipping). Let $W \\in \\mathbb{R}^{m \\times n}$ and $W = U \\Sigma V^T$ be its singular value decomposition where $\\Sigma = (\\sigma_1, \\ldots, \\sigma_{min(m,n)})$ are the singular values of $W$. Then we define Spectral Clipping as the following matrix function $\\texttt{spectral\\_clip}_{[\\sigma_{min}, \\sigma_{max}]}: \\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{m \\times n}$, $$\\begin{equation}\\texttt{spectral\\_clip}_{[\\sigma_{min}, \\sigma_{max}]}(W) = U \\texttt{clip}_{[\\sigma_{min}, \\sigma_{max}]}(\\Sigma) V^T\\label{1}\\end{equation}$$ where $\\sigma_{min}, \\sigma_{max} \\in [0, \\infty)$ are hyperparameters that control the minimum and maximum attainable singular values of the resulting matrix and $\\texttt{clip}_{[\\alpha, \\beta]}: \\mathbb{R} \\to \\mathbb{R}$ is applied element-wise on the singular values of $W$,\n$$\\begin{equation}\\texttt{clip}_{[\\alpha, \\beta]}(x) = \\begin{cases} \\alpha \u0026 \\texttt{if } x \u003c \\alpha \\\\ x \u0026 \\texttt{if } \\alpha \\leq x \\leq \\beta \\\\ \\beta \u0026 \\texttt{if } \\beta \u003c x \\end{cases}\\end{equation}$$ where $\\alpha, \\beta \\in \\mathbb{R} \\cup \\{-\\infty, \\infty\\}$ and $\\alpha \\leq \\beta$.\nNote that since the singular values of a matrix are guaranteed to be non-negative, $\\texttt{clip}$ above does not need to be bidirectional. And setting $\\alpha \\leq 0$ and/or $\\beta = \\infty$ massively simpifies our (matrix) function, resulting in efficiency gains,\n$\\texttt{clip}_{[\\leq 0, \\beta]}(x) = \\min(x, \\beta)$; and $\\texttt{clip}_{[\\alpha, \\infty]}(x) = \\max(x, \\alpha)$ which is simply the (shifted-)$\\texttt{ReLU}$. In practice, the former would suffice for constraining the weights of neural networks. However, we will keep both parameters $\\alpha, \\beta$ in this work for generality and in case one would need to constrain the weights to always be full rank to prevent the activations from collapsing in dimension.\nPotential application to test-time training (TTT) As discussed in previous posts, (linear) attention mechanisms implicitly or explicitly perform test-time training (TTT) by learning to adapt the attention state as the model ingests more and more context without updating the model parameters. The core idea behind this is that we can hardcode a subnetwork and its optimizer into the model architecture itself and if this subnetwork-optimizer pair is end-to-end (auto-)differentiable, then in theory this should allow the model to learn methods on how to learn from the context it ingests which it can then use at test-time.\nRecent work in this direction focuses on optimizing speed, stability, and expressiveness of such architectures (Yang et al., 2025; Grazzi et al., 2025). Hence the design choices in this post. In theory, we could use $\\texttt{spectral\\_clip}$ we construct here as an inner optimizer in a (linear) attention mechanism. In fact the team behind Atlas (Behrouz et al., 2025) has recently shown that the Muon optimizer (Jordan et al., 2024) can indeed be incorporated into an attention mechanism and that doing so not only improves performance but also reduces accuracy drop at longer context lengths. And as previously discussed by Su (2025), $$\\lim_{k \\to \\infty}\\texttt{spectral\\_clip}(kG) = \\texttt{misgn}(G)$$ for some scalar $k \\in \\mathbb{R}$. Thus, we could simply swap in Muon’s orthogonalization step with spectral clipping with minimal changes to the architecture. Alternatively, we could also apply it after applying Muon optimizer’s update step to control the growth of the attention state and prevent it from blowing up. Think of this as a more theoretically-grounded way of constraining the weights vs. weight decay.\n2. Towards hardware-architecture-optimizer codesign In deep learning, we not only have to be mindful of architecture-optimizer codesign but also hardware-software codesign. That is, architectural and optimizer choices and how we implement them have to be hardware-aware so that we can squeeze as much performance as we can from our GPUs/TPUs.\nFor example, the naive way to compute Spectral Clipping is to directly compute the SVD, clip the singular values we get from it, then reconstruct the matrix using the clipped singular values. A JAX implementation would look like this:\ndef naive_spectral_clip(W: jax.Array, sigma_min: float=-1., sigma_max: float=1.): U, S, Vt = jnp.linalg.svd(W, full_matrices=False) S_clipped = jnp.clip(S, min=sigma_min, max=sigma_max) return U @ jnp.diag(S_clipped) @ Vt However, this is not recommended because computing the SVD directly (1) does not take advantage of the GPUs’ tensor cores and (2) requires higher numerical precision, typically 32-bit float types. These not only slow things down but also increase precious memory usage, making it hard to scale to larger models.\nIdeally, we want to only use operations that (1) have fast implementations on GPUs/TPUs and (2) are stable under lower numerical precision, e.g., 16-bit, 8-bit, even 4-bit float types. So, elementwise operations like matrix addition and scalar multiplication, matrix multiplication, matrix-vector products, among others are preferred, but not operations like matrix inversions or SVD decomposition, etc. With the proper coefficients, (semi-)orthogonalization via Newton-Schulz iteration for computing the matrix sign function has also been shown to be fast and numerically stable under lower precision (Jordan et al., 2024), thus we can use that here.\nFinding a suitable surrogate function for $\\texttt{clip}$ This is the fun part.\nSo, how do we compute spectral clipping while only using simple, but fast \u0026 numerically stable operations? First, let’s list the operations we can actually use and consider how they act on the matrix itself and its singular values. There are more operations we can use that aren’t listed here, but these would suffice for our problem.\nOperation Matrix form Action on\nsingular values Tensor cores\nutilization Numerical stability\nat low precision (Auto-)differentiable Linear combination $c_1 W_1 + c_2 W_2$ $c_1 \\Sigma_1 + c_2 \\Sigma_2$ $\\color{green}{\\text{high}}$ $\\color{green}{\\text{yes}}$ $\\color{green}{\\text{yes}}$ Apply polynomial function $\\texttt{p}(W)$ $\\texttt{p}(\\Sigma)$ $\\color{green}{\\text{high}}$ $\\color{green}{\\text{yes}}$ $\\color{green}{\\text{yes}}$ Apply matrix sign function\n(via Newton-Schulz iteration) $\\texttt{msign}(W)$ $\\texttt{sign}(\\Sigma)$ $\\color{green}{\\text{high}}$ $\\color{green}{\\text{yes}}$ $\\color{green}{\\text{yes}}$ Apply matrix sign function\n(via QR-decomposition) $\\texttt{msign}(W)$ $\\texttt{sign}(\\Sigma)$ $\\color{orange}{\\text{medium}}$ $\\color{green}{\\text{yes}}$*\n(bfloat16/float16+) $\\color{green}{\\text{yes}}$\n(in jax) Apply matrix sign function\n(via SVD) $\\texttt{msign}(W)$ $\\texttt{sign}(\\Sigma)$ $\\color{red}{\\text{low}}$ $\\color{red}{\\text{no}}$\n(float32+) $\\color{red}{\\text{no}}$ Let’s reconstruct the $\\mathbb{R} \\to \\mathbb{R}$ clipping on the singular values with these elementary functions first, then let’s use it to construct the matrix form. Here we take advantage of the following identity, $$\\begin{equation}|x| = x \\cdot \\texttt{sign}(x)\\end{equation}$$ With this, we can now construct $\\texttt{clip}$ as follows,\nProposition 2 (Computing $\\texttt{clip}$ via $\\texttt{sign}$). Let $\\alpha, \\beta \\in \\mathbb{R} \\cup \\{-\\infty, \\infty\\}$ and $\\texttt{clip}: \\mathbb{R} \\to \\mathbb{R}$ be the clipping function defined in Definition 1. Then, $$\\begin{equation}\\texttt{clip}_{[\\alpha, \\beta]}(x) = \\frac{\\alpha + \\beta + (\\alpha - x)\\texttt{sign}(\\alpha - x) - (\\beta - x)\\texttt{sign}(\\beta - x)}{2}\\label{4}\\end{equation}$$\nProof: It would suffice to show that, $$\\begin{equation}\\texttt{clip}_{[\\alpha, \\beta]}(x) = \\frac{\\alpha + \\beta + |\\alpha - x| - |\\beta - x|}{2}\\end{equation}$$ For this, we can simply check case-by-case,\n$x$ $\\left | \\alpha - x\\right | $ $\\left | \\beta - x\\right | $ $\\frac{\\alpha + \\beta + | \\alpha - x | - | \\beta - x | }{2}$ $x \u003c \\alpha$ $\\alpha - x$ $\\beta - x$ $\\alpha$ $\\alpha \\leq x \\leq \\beta $ $x - \\alpha$ $\\beta - x$ $x$ $\\beta \u003c x$ $x - \\alpha$ $x - \\beta$ $\\beta$ Combining Equations (3) and (5) then gives us Equation $\\eqref{4}$. $\\blacksquare$\nLifting to matrix form (the naive \u0026 incorrect way) A naive way to lift Equation $\\eqref{4}$ above to matrix form is to simply replace the variables, scalar constants, and scalar (sub-)functions with their corresponding matrix form, i.e., replace $x$ with $W$, $1$ with $I$, and $\\texttt{sign}(\\cdot)$ with $\\texttt{msign}(\\cdot)$. This gives us the following matrix function,\n$$\\begin{align} \\texttt{f}(W) \u0026= \\frac{1}{2} [(\\alpha + \\beta)I + (\\alpha I - W) \\texttt{msign}(\\alpha I - W)^T\\nonumber\\\\ \u0026\\qquad\\qquad\\qquad\\qquad\\;\\;- (\\beta I - W) \\texttt{msign}(\\beta I - W)^T] \\end{align}$$\nHowever, as communicated to me by You Jiacheng \u0026 Su Jianlin, this does not work (see figure above) because $I$ may not share the same singular vectors as $W$.\nAnother problem is that $\\texttt{f}$ does not preserve the dimensions of the input matrix $W$. To see this, note that both $\\alpha I - W$ and $\\texttt{msign}(\\alpha I - W)$ have shape $m \\times n$ and so $(\\alpha I - W) \\texttt{msign}(\\alpha I - W)^T$ must have shape $m \\times m$. The same is true for the other term.\n$$\\begin{aligned} \\texttt{f}(W) \u0026= \\frac{1}{2} [(\\alpha + \\beta)I_{\\color{red}{m \\times m}} + (\\alpha I - W) \\texttt{msign}(\\alpha I - W)^T\\\\ \u0026\\qquad\\qquad\\qquad\\qquad\\qquad- \\underbrace{\\underbrace{(\\beta I - W)}_{m \\times n} \\underbrace{\\texttt{msign}(\\beta I - W)^T}_{n \\times m}}_{\\color{red}{m \\times m}}] \\end{aligned}$$\nLifting to matrix form (the proper way) To properly lift Equation $\\eqref{4}$ to matrix form, let’s combine it with Equation $\\eqref{1}$, $$\\begin{align} \\texttt{spectral\\_clip}_{[\\alpha, \\beta]}(W) \u0026= U \\texttt{clip}_{[\\alpha, \\beta]}(\\Sigma) V^T\\nonumber\\\\ \u0026= U \\frac{(\\alpha + \\beta) I + (\\alpha I - \\Sigma)\\texttt{sign}(\\alpha I - \\Sigma) - (\\beta I - \\Sigma)\\texttt{sign}(\\beta I - \\Sigma)}{2} V^T\\nonumber\\\\ \u0026= \\frac{1}{2} [(\\alpha + \\beta) UV^T\\nonumber\\\\ \u0026\\qquad\\qquad+ U (\\alpha I - \\Sigma ) \\texttt{sign}(\\alpha I - \\Sigma) V^T\\nonumber\\\\ \u0026\\qquad\\qquad- U (\\beta I - \\Sigma ) \\texttt{sign}(\\beta I - \\Sigma) V^T]\\nonumber\\\\ \u0026= \\frac{1}{2} [(\\alpha + \\beta) UV^T\\nonumber\\\\ \u0026\\qquad\\qquad+ U (\\alpha I - \\Sigma ) (V^TV) \\texttt{sign}(\\alpha I - \\Sigma) (U^TU) V^T\\nonumber\\\\ \u0026\\qquad\\qquad- U (\\beta I - \\Sigma ) (V^TV) \\texttt{sign}(\\beta I - \\Sigma) (U^TU) V^T]\\nonumber\\\\ \u0026= \\frac{1}{2} [(\\alpha + \\beta) UV^T\\nonumber\\\\ \u0026\\qquad\\qquad+ (\\alpha UV^T - U\\Sigma V^T) (V \\texttt{sign}(\\alpha I - \\Sigma) U^T)(UV^T)\\nonumber\\\\ \u0026\\qquad\\qquad- (\\beta UV^T - U\\Sigma V^T) (V \\texttt{sign}(\\beta I - \\Sigma) U^T)(UV^T)]\\nonumber\\\\ \u0026= \\frac{1}{2} [(\\alpha + \\beta) UV^T\\nonumber\\\\ \u0026\\qquad\\qquad+ (\\alpha UV^T - U\\Sigma V^T) (U \\texttt{sign}(\\alpha I - \\Sigma) V^T)^T(UV^T)\\nonumber\\\\ \u0026\\qquad\\qquad- (\\beta UV^T - U\\Sigma V^T) (U \\texttt{sign}(\\beta I - \\Sigma) V^T)^T(UV^T)]\\nonumber\\\\ \u0026= \\frac{1}{2} [(\\alpha + \\beta) \\texttt{msign}(W)\\nonumber\\\\ \u0026\\qquad\\qquad+ (\\alpha \\cdot\\texttt{msign}(W) - W) \\texttt{msign}(\\alpha \\cdot\\texttt{msign}(W) - W)^T\\texttt{msign}(W)\\nonumber\\\\ \u0026\\qquad\\qquad- (\\beta \\cdot\\texttt{msign}(W) - W) \\texttt{msign}(\\beta \\cdot\\texttt{msign}(W) - W)^T\\texttt{msign}(W)]\\nonumber\\\\ \\texttt{spectral\\_clip}_{[\\alpha, \\beta]}(W) \u0026= \\frac{1}{2} [(\\alpha + \\beta)I\\nonumber\\\\ \u0026\\qquad\\qquad+ (\\alpha \\cdot\\texttt{msign}(W) - W) \\texttt{msign}(\\alpha \\cdot\\texttt{msign}(W) - W)^T\\nonumber\\\\ \u0026\\qquad\\qquad- (\\beta \\cdot\\texttt{msign}(W) - W) \\texttt{msign}(\\beta \\cdot\\texttt{msign}(W) - W)^T\\nonumber\\\\ \u0026\\qquad\\qquad]\\;\\texttt{msign}(W)\\label{7} \\end{align}$$\nAnd viola, we’re done. The following code implements this in JAX,\ndef spectral_clip(W: jax.Array, alpha: float=-1., beta: float=1.): if flip := W.shape[0] \u003e W.shape[1]: W = W.T OW = _orthogonalize_via_newton_schulz(W) result = (1/2) * ( (alpha + beta) * jnp.eye(W.shape[0]) + (alpha * OW - W) @ _orthogonalize_via_newton_schulz(alpha * OW - W).T - (beta * OW - W) @ _orthogonalize_via_newton_schulz(beta * OW - W).T ) @ OW if flip: result = result.T return result where _orthogonalize_via_newton_schulz above implements Jordan et al.’s (2024) Newton-Schulz iteration for computing the matrix sign function. Note that we’re calling _orthogonalize_via_newton_schulz thrice here, which is not ideal.\n3. Variants and optimizations Sanity check: orthogonalization and scaling As a simple test-case, let’s verify that setting the lower and upper bounds to be equal results in orthogonalization and scaling of the input matrix, i.e., $\\texttt{spectral\\_clip}_{[\\sigma, \\sigma]}(W) = \\sigma \\cdot \\texttt{msign}(W)$. From Equation $\\eqref{7}$, we have,\n$$\\begin{aligned} \\texttt{spectral\\_clip}_{[\\sigma, \\sigma]}(W) \u0026= \\frac{1}{2} [(\\sigma + \\sigma)I\\nonumber\\\\ \u0026\\qquad\\qquad\\cancel{+ (\\sigma \\cdot\\texttt{msign}(W) - W) \\texttt{msign}(\\sigma \\cdot\\texttt{msign}(W) - W)^T}\\nonumber\\\\ \u0026\\qquad\\qquad\\cancel{- (\\sigma \\cdot\\texttt{msign}(W) - W) \\texttt{msign}(\\sigma \\cdot\\texttt{msign}(W) - W)^T}\\nonumber\\\\ \u0026\\qquad\\qquad]\\;\\texttt{msign}(W)\\\\ \\texttt{spectral\\_clip}_{[\\sigma, \\sigma]}(W) \u0026= \\sigma \\cdot \\texttt{msign}(W)\\quad\\blacksquare \\end{aligned}$$\nUnbounded below: Spectral Hardcapping Note: Su (2025) calls this “Singular Value Clipping” or “SVC” while our upcoming paper calls this “Spectral Hardcapping”.\nSingular values are guaranteed to be non-negative, so if we only want to bound the singular values from above, we can simply set $\\alpha = 0$ in Equation $\\eqref{4}$, i.e., $$\\begin{align} \\texttt{clip}_{[0, \\beta]}(x) \u0026= \\frac{0 + \\beta + (0 - x)\\texttt{sign}(0 - x) - (\\beta - x)\\texttt{sign}(\\beta - x)}{2}\\nonumber\\\\ \\texttt{clip}_{[0, \\beta]}(x) \u0026= \\frac{\\beta + x - (\\beta - x)\\texttt{sign}(\\beta - x)}{2} \\end{align}$$ Setting $\\beta = 1$ recovers Su’s (2025) and You’s (2025) results. And following the approach above, we get, $$\\begin{aligned} \\texttt{spectral\\_hardcap}_\\beta(W) \u0026= \\texttt{spectral\\_clip}_{[0, \\beta]}(W)\\\\ \\texttt{spectral\\_hardcap}_\\beta(W) \u0026= \\frac{1}{2} [\\beta \\cdot \\texttt{msign}(W) + W\\\\ \u0026\\qquad\\qquad- (\\beta \\cdot\\texttt{msign}(W) - W) \\texttt{msign}(\\beta \\cdot\\texttt{msign}(W) - W)^T \\texttt{msign}(W)] \\end{aligned}$$\nThe following code implements this in JAX,\ndef spectral_hardcap(W: jax.Array, beta: float=1.): if flip := W.shape[0] \u003e W.shape[1]: W = W.T OW = _orthogonalize_via_newton_schulz(W) aW = beta * OW - W result = (1/2) * (beta*OW + W - aW @ _orthogonalize_via_newton_schulz(aW).T @ OW) if flip: result = result.T return result We are now only calling _orthogonalize_via_newton_schulz twice here.\nUnbounded above: Spectral (Shifted-)ReLU If we only want to bound the singular values from below, we set $\\beta = +\\infty$ in Equation $\\eqref{4}$. First note that for a fixed $x \\in [0, \\infty)$, $$\\lim_{\\beta \\to +\\infty} \\texttt{sign}(\\beta - x) = +1$$ Thus, $$\\begin{align} \\texttt{clip}_{[\\alpha, +\\infty]}(x) \u0026= \\lim_{\\beta \\to +\\infty}\\frac{\\alpha + \\beta + (\\alpha - x)\\texttt{sign}(\\alpha - x) - (\\beta - x)\\texttt{sign}(\\beta - x)}{2}\\nonumber\\\\ \\texttt{clip}_{[\\alpha, +\\infty]}(x) \u0026= \\frac{\\alpha + x + (\\alpha - x)\\texttt{sign}(\\alpha - x)}{2} \\end{align}$$ And following the approach above, we get, $$\\begin{aligned} \\texttt{spectral\\_relu}_\\alpha(W) \u0026= \\texttt{spectral\\_clip}_{[\\alpha, +\\infty]}(W)\\\\ \\texttt{spectral\\_relu}_\\alpha(W) \u0026= \\frac{1}{2} [\\alpha \\cdot \\texttt{msign}(W) + W\\\\ \u0026\\qquad\\qquad+ (\\alpha \\cdot\\texttt{msign}(W) - W) \\texttt{msign}(\\alpha \\cdot\\texttt{msign}(W) - W)^T \\texttt{msign}(W)] \\end{aligned}$$\nThe following code implements this in JAX,\ndef spectral_relu(W: jax.Array, alpha: float=1.): if flip := W.shape[0] \u003e W.shape[1]: W = W.T OW = _orthogonalize_via_newton_schulz(W) aW = alpha * OW - W result = (1/2) * (alpha*OW + W + aW @ _orthogonalize_via_newton_schulz(aW).T @ OW) if flip: result = result.T return result 4. An alternative approach: Higham’s Anti-Block-Diagonal Trick In the previous sections, we apply our matrix function directly on $W$ resulting in nested applications of $\\texttt{msign}$. Here, we will instead use Higham’s anti-block-diagonal trick (Higham, 2008). This allows us to compute $\\texttt{msign}$ only once, reducing the complexity of the operations albeit at the cost of more compute and memory usage. This trick may not be practical in most settings, but the reduced complexity of the operations may be worth it when designing linear attention mechanisms with the spectral clipping function as a “sub-network”. A neat property is that this would allow us to naturally scale test-time compute by scaling the number of steps in $\\texttt{msign}$.\nTheorem 3 (Higham’s Anti-Block-Diagonal Trick). Let $g: \\mathbb{R} \\to \\mathbb{R}$ be an odd analytic scalar function, $W \\in \\mathbb{R}^{m \\times n}$, and construct the block matrix $S \\in \\mathbb{R}^{(m+n) \\times (m+n)}$ as, $$S := \\begin{bmatrix} 0 \u0026 W \\\\ W^T \u0026 0 \\end{bmatrix}$$ and let $g(S)$ as the primary matrix function defined from the scalar function $g$. Then, $$g(S) = \\begin{bmatrix} 0 \u0026 g(W) \\\\ g(W)^T \u0026 0 \\end{bmatrix}$$ and hence, $$g(W) = [g(S)]_{12} = [g(S)]_{21}^T$$\nNote that, for our optimization tricks below to work, our scalar function $\\texttt{clip}_{[\\alpha, \\beta]}$ has to be odd which we will impose by setting, $$\\alpha = -\\beta.$$ Also note that, $$\\texttt{clip}_{[-\\sigma_{max}, \\sigma_{max}]}(x) = \\sigma_{max} \\cdot \\texttt{clip}_{[-1, 1]}(x / \\sigma_{max})$$ and thus it would suffice to construct $\\texttt{spectral\\_clip}_{[-1, 1]}(\\cdot)$ first and then, $$\\begin{equation} \\texttt{spectral\\_clip}_{[-\\sigma_{max}, \\sigma_{max}]}(W) = \\sigma_{max}\\cdot\\texttt{spectral\\_clip}_{[-1, 1]}(W / \\sigma_{max}). \\end{equation}$$\nNow, applying Theorem 3 with $g = \\texttt{clip}_{[-1, 1]}$ gives us, $$\\begin{equation}\\texttt{spectral\\_clip}_{[-1, 1]}(W) = \\left[ \\frac{(I+S) \\texttt{msign}(I+S) - (I-S) \\texttt{msign}(I-S)}{2} \\right]_{12}\\end{equation}$$\nThe following code implements this in JAX,\ndef _spectral_clip(W: jax.Array): m, n = W.shape I = jnp.eye(m + n) S = jnp.block([[jnp.zeros((m, m)), W], [W.T, jnp.zeros((n, n))]]) gS = (1/2) * ( (I + S) @ _orthogonalize_via_newton_schulz (I + S) - (I - S) @ _orthogonalize_via_newton_schulz (I - S) ) return gS[:m, m:] # read off the top-right block def spectral_clip(W: jax.Array, sigma_max: float=1.): return sigma_max * _spectral_clip(W / sigma_max) Note that we are still calling _orthogonalize_via_newton_schulz twice here, which is not ideal either. Luckily, there’s a neat trick that allows us to compute it only once.\nOptimizing the implementation via abstract algebra First, notice that both $$I + S = \\begin{bmatrix} I_m \u0026 W \\\\ W^T \u0026 I_n \\end{bmatrix}\\qquad I - S = \\begin{bmatrix} I_m \u0026 -W \\\\ -W^T \u0026 I_n \\end{bmatrix}$$ are block matrices of the form $$\\begin{bmatrix} P \u0026 Q \\\\ Q^T \u0026 R \\end{bmatrix}$$ where $P, R$ are symmetric matrices and $Q$ is an arbitrary matrix. It is a well-known result that such matrices form a linear sub-algebra $\\mathcal{A}$, i.e., they are closed under addition, scalar multiplication, and matrix multiplication. This means that applying any polynomial function to these matrices will yield another matrix of the same form. And since we’re calculating the matrix sign function with Newton-Schulz iteration, which is a composition of polynomial functions, its result must also be of the same form.\nAnother neat property we can take advantage of is that flipping the signs of the anti-diagonal blocks gets preserved under application of any analytic matrix function.\nProposition 4 (Parity w.r.t. $Q \\to -Q$ when applying analytic matrix function $f(\\cdot)$). Let $A \\in \\mathcal{A}$ such that, $$A := \\begin{bmatrix} P \u0026 Q \\\\ Q^T \u0026 R \\end{bmatrix}$$ for some arbitrary matrix $Q \\in \\mathbb{R}^{m \\times n}$ and symmetric matrices $P \\in \\mathbb{R}^{m \\times m}$, $R \\in \\mathbb{R}^{n \\times n}$, let $f: \\mathcal{A} \\to \\mathcal{A}$ be an analytic matrix function, and let $$\\begin{bmatrix} \\widetilde{P} \u0026 \\widetilde{Q} \\\\ \\widetilde{Q}^T \u0026 \\widetilde{R} \\end{bmatrix} := f(A) = f\\left(\\begin{bmatrix} P \u0026 Q \\\\ Q^T \u0026 R \\end{bmatrix}\\right).$$ Then, $$\\begin{bmatrix} \\widetilde{P} \u0026 -\\widetilde{Q} \\\\ -\\widetilde{Q}^T \u0026 \\widetilde{R} \\end{bmatrix} = f\\left(\\begin{bmatrix} P \u0026 -Q \\\\ -Q^T \u0026 R \\end{bmatrix}\\right).$$\nThis is a standard result. To see why,\nProof. Let $J = \\text{diag}(I_m, -I_n)$ so that $J^2 = I$ and $J^{-1} = J$. This makes $J A J = J A J^{-1}$ simply a change of basis, which is preserved under application of analytic matrix functions. Thus we have, $$\\begin{aligned} Jf(A) J \u0026= f(JAJ)\\\\ \\begin{bmatrix} I_m \u0026 0 \\\\ 0 \u0026 -I_n \\end{bmatrix} \\begin{bmatrix} \\widetilde{P} \u0026 -\\widetilde{Q} \\\\ -\\widetilde{Q}^T \u0026 \\widetilde{R} \\end{bmatrix} \\begin{bmatrix} I_m \u0026 0 \\\\ 0 \u0026 -I_n \\end{bmatrix} \u0026= f\\left(\\begin{bmatrix} I_m \u0026 0 \\\\ 0 \u0026 -I_n \\end{bmatrix} \\begin{bmatrix} P \u0026 -Q \\\\ -Q^T \u0026 R \\end{bmatrix} \\begin{bmatrix} I_m \u0026 0 \\\\ 0 \u0026 -I_n \\end{bmatrix}\\right)\\\\ \\begin{bmatrix} \\widetilde{P} \u0026 -\\widetilde{Q} \\\\ -\\widetilde{Q}^T \u0026 \\widetilde{R} \\end{bmatrix} \u0026= f\\left(\\begin{bmatrix} P \u0026 -Q \\\\ -Q^T \u0026 R \\end{bmatrix}\\right) \\end{aligned} \\quad \\blacksquare$$\nThus we have, $$\\begin{bmatrix} \\widetilde{P} \u0026 \\widetilde{Q} \\\\ \\widetilde{Q}^T \u0026 \\widetilde{R} \\end{bmatrix} = \\texttt{msign}(I + S)\\qquad\\qquad \\begin{bmatrix} \\widetilde{P} \u0026 -\\widetilde{Q} \\\\ -\\widetilde{Q}^T \u0026 \\widetilde{R} \\end{bmatrix} = \\texttt{msign}(I - S)$$ for some $\\widetilde{Q} \\in \\mathbb{R}^{m \\times n}$ and symmetric $\\widetilde{P} \\in \\mathbb{R}^{m \\times m}$, $\\widetilde{R} \\in \\mathbb{R}^{n \\times n}$. Together with Equation 11, we get,\n$$\\begin{align} \\texttt{spectral\\_clip}_{[-1, 1]}(W) \u0026= \\frac{1}{2}\\left[\\begin{bmatrix} I_m \u0026 W \\\\ W^T \u0026 I_n \\end{bmatrix} \\begin{bmatrix} \\widetilde{P} \u0026 \\widetilde{Q} \\\\ \\widetilde{Q}^T \u0026 \\widetilde{R} \\end{bmatrix} - \\begin{bmatrix} I_m \u0026 -W \\\\ -W^T \u0026 I_n \\end{bmatrix} \\begin{bmatrix} \\widetilde{P} \u0026 -\\widetilde{Q} \\\\ -\\widetilde{Q}^T \u0026 \\widetilde{R} \\end{bmatrix}\\right]_{12}\\nonumber\\\\ \u0026= \\frac{1}{2} \\left[\\begin{bmatrix} \\widetilde{P} + WQ^{*T} \u0026 \\widetilde{Q} + W\\widetilde{R} \\\\ W^T\\widetilde{P}+\\widetilde{Q}^T \u0026 W^T\\widetilde{Q}^T + \\widetilde{R} \\end{bmatrix} - \\begin{bmatrix} \\widetilde{P} + WQ^{*T} \u0026 -(\\widetilde{Q} + W\\widetilde{R}) \\\\ -(W^T\\widetilde{P}+\\widetilde{Q}^T) \u0026 W^T\\widetilde{Q}^T + \\widetilde{R} \\end{bmatrix}\\right]_{12}\\nonumber\\\\ \u0026= \\begin{bmatrix} 0 \u0026 \\widetilde{Q} + W\\widetilde{R} \\\\ (\\widetilde{Q} + \\widetilde{P}W)^T \u0026 0 \\end{bmatrix}_{12} \\\\ \\texttt{spectral\\_clip}_{[-1, 1]}(W) \u0026= \\widetilde{Q} + W\\widetilde{R}\\qquad\\text{ or }\\qquad\\widetilde{Q} + \\widetilde{P} W\\nonumber \\end{align}$$\nThis means that we only need to call msign once, and simply read off the blocks to compute the final result, leading to massive speedups. Also note that the diagonal blocks in Equation (12) are zero, which is what we expect from Theorem 3.\nIn JAX, this looks like the following:\ndef _spectral_clip(W: jax.Array): m, n = W.shape H = jnp.block([[jnp.eye(m), W], [W.T, jnp.eye(n)]]) OH = _orthogonalize_via_newton_schulz(H) P, Q = OH[:m, :m], OH[:m, m:] return Q + P @ W # Q, R = OH[:m, m:], OH[m:, m:] # return Q + W @ R def spectral_clip(W: jax.Array, sigma_max: float=1.): return sigma_max * _spectral_clip(W / sigma_max, 1) And a codegolf version would be,\ndef spectral_clip_minimal(W: jax.Array, sigma_max: float=1., ortho_dtype=jnp.float32): OH = _orthogonalize_via_newton_schulz (jnp.block([[jnp.eye(W.shape[0]), W / sigma_max], [W.T / sigma_max, jnp.eye(W.shape[1])]]).astype(ortho_dtype)).astype(W.dtype) return sigma_max*OH[:W.shape[0], W.shape[0]:] + OH[:W.shape[0], :W.shape[0]] @ W # return sigma_max*OH[:W.shape[0], W.shape[0]:] + W @ OH[W.shape[0]:, W.shape[0]:] Taking advantage of symmetry The crux is that since both $I + S$ and $I - S$ are in the sub-algebra $\\mathcal{A}$, Newton-Schulz iteration must preserve their block structure. Thus, we do not actually need to materialize the entire $(m + n) \\times (m + n)$ block matrices. And note that,\n$$\\begin{aligned} \\begin{bmatrix} P_i \u0026 Q_i\\\\ Q_i^T \u0026 R_i \\end{bmatrix}\\begin{bmatrix} P_j \u0026 Q_j\\\\ Q_j^T \u0026 R_j \\end{bmatrix}^T \u0026= \\begin{bmatrix} P_i P_j + Q_i Q_j^T \u0026 P_i Q_j + Q_i R_j\\\\ Q_i^T P_j + R_i Q_j^T \u0026 Q_i^T Q_j + R_i R_j \\end{bmatrix} \\end{aligned}$$ Thus we can implement the (blocked) matrix multiplications as,\n@jax.jit def block_matmul( P1: jax.Array, Q1: jax.Array, R1: jax.Array, P2: jax.Array, Q2: jax.Array, R2: jax.Array, ) -\u003e Tuple[jax.Array, jax.Array, jax.Array]: P = P1 @ P2 + Q1 @ Q2.T Q = P1 @ Q2 + Q1 @ R2 R = Q1.T @ Q2 + R1 @ R2 return P, Q, R and implement one step of Newton-Schulz iteration as,\ndef newton_schulz_iter( P: jax.Array, Q: jax.Array, R: jax.Array, a: float, b: float, c: float, ): I_P = a * jnp.eye(P.shape[0], dtype=P.dtype) I_R = a * jnp.eye(R.shape[0], dtype=R.dtype) P2, Q2, R2 = block_matmul(P, Q, R, P, Q, R) P4, Q4, R4 = block_matmul(P2, Q2, R2, P2, Q2, R2) Ppoly = I_P + b * P2 + c * P4 Qpoly = b * Q2 + c * Q4 Rpoly = I_R + b * R2 + c * R4 return block_matmul(P, Q, R, Ppoly, Qpoly, Rpoly) We then initialize the blocks as $P_0 = I_{m}$, $Q_0 = W$, and $R_0 = I_m$, apply Newton-Schulz iteration as described above to get $(\\widetilde{P}, \\widetilde{Q}, \\widetilde{R})$, and finally return $\\widetilde{Q} + W\\widetilde{R}$ or $\\widetilde{Q} + \\widetilde{P} W$. This should give efficiency gains vs. the naive implementation.\n5. Runtime analysis From Jordan et al. (2024), computing the matrix sign function on a $m \\times n$ matrix (WLOG let $m \\leq n$) via $T$ steps of Newton-Schulz iterations with 5th degree odd polynomials requires at most $\\approx 6Tnm^2$ matmul FLOPs. Thus,\nOperation Number of $\\texttt{msign}$ calls Total FLOPs FLOPs overhead\n(w/ NanoGPT-140M\nspeedrun configs) $\\texttt{msign}$ via Newton-Schulz $1$ $6Tnm^2$ 0.98% $\\texttt{spectral\\_clip}_{[\\alpha, \\beta]}$\n(via nested $\\texttt{msign}$ in Section (2)) $3$ $(18T + 6)nm^2$ 3.13% $\\texttt{spectral\\_hardcap}$ $2$ $(12T + 4)nm^2$ 2.08% $\\texttt{spectral\\_relu}$ $2$ $(12T + 4)nm^2$ 2.08% $\\texttt{spectral\\_clip}_{[-\\beta, \\beta]}$\n(via full-matrix anti-block-diagonal trick) $1$\n$(m+n) \\times (m+n)$ $6T(n+m)^3$ 7.81% $\\texttt{msign}$ via block-wise Newton-Schulz $1$ (block-wise) $36Tn^3$ - $\\texttt{spectral\\_clip}_{[-\\beta, \\beta]}$\n(via block-wise anti-block-diagonal trick) $1$ (block-wise) $(36T+1)n^3$ 5.89% 6. Experimental results [Under Construction] This section is still under construction.\nGrokking [Under Construction] [Grokking experiments results will be added here]\nNanoGPT Speedrun results [Under Construction] [NanoGPT Speedrun results will be added here]\nAcknowledgements Many thanks to Rohan Anil for initiating a discussion thread on the topic on Twitter, and to Arthur Breitman, You Jiacheng, and Su Jianlin for productive discussions on the topic.\nHow to Cite @misc{cesista2025spectralclipping, author = {Franz Louis Cesista}, title = {\"Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping Via Newton-Schulz Iteration\"}, year = {2025}, url = {http://leloykun.github.io/ponder/spectral-clipping/}, } References Greg Yang, James B. Simon, Jeremy Bernstein (2024). A Spectral Condition for Feature Learning. URL https://arxiv.org/abs/2310.17813 Tim Large, Yang Liu, Minyoung Huh, Hyojin Bahng, Phillip Isola, Jeremy Bernstein (2024). Scalable Optimization in the Modular Norm. URL https://arxiv.org/abs/2405.14813 Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim (2025). Parallelizing Linear Transformers with the Delta Rule over Sequence Length. URL https://arxiv.org/abs/2406.06484 Riccardo Grazzi, Julien Siems, Jörg K.H. Franke, Arber Zela, Frank Hutter, Massimiliano Pontil (2025). Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues. URL https://arxiv.org/abs/2411.12537 Ali Behrouz, Zeman Li, Praneeth Kacham, Majid Daliri, Yuan Deng, Peilin Zhong, Meisam Razaviyayn, Vahab Mirrokni (2025). ATLAS: Learning to Optimally Memorize the Context at Test Time. URL https://arxiv.org/abs/2505.23735 Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein (2024). Muon: An optimizer for hidden layers in neural networks. Available at: https://kellerjordan.github.io/posts/muon/ Jianlin Su (2025). Higher-order muP: A more concise but more intelligent spectral condition scaling. URL https://kexue.fm/archives/10795 Higham, Nicholas J. (2008). Functions of Matrices: Theory and Computation. SIAM. Jianlin Su (2025). Calculation of spectral_clip (singular value clipping) via msign. Available at: https://kexue.fm/archives/11006 Jiacheng You (2025). On a more efficient way to compute spectral clipping via nested matrix sign functions. Available at: https://x.com/YouJiacheng/status/1931029612102078749 Arthur Breitman (2025). On using the matrix sign function for spectral clipping. Available at: https://x.com/ArthurB/status/1929958284754330007 Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra (2022). Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. URL https://arxiv.org/abs/2201.02177 Lucas Prieto, Melih Barsbey, Pedro A.M. Mediano, Tolga Birdal (2025). Grokking at the Edge of Numerical Stability. URL https://arxiv.org/abs/2501.04697 Amund Tveit, Bjørn Remseth, Arve Skogvold (2025). Muon Optimizer Accelerates Grokking. https://arxiv.org/abs/2504.16041 Jeremy Bernstein and Laker Newhouse. “Old optimizer, new norm: An anthology.” arXiv preprint arXiv:2409.20325 (2024). Zixuan Chen, Xialin He, Yen-Jen Wang, Qiayuan Liao, Yanjie Ze, Zhongyu Li, S. Shankar Sastry, Jiajun Wu, Koushil Sreenath, Saurabh Gupta, Xue Bin Peng (2024). Learning Smooth Humanoid Locomotion through Lipschitz-Constrained Policies. URL https://arxiv.org/abs/2410.11825 ",
  "wordCount" : "4196",
  "inLanguage": "en",
  "image":"https://leloykun.github.io/ponder/spectral-clipping/clip_lifting_trap_fix.png","datePublished": "2025-06-05T00:00:00Z",
  "dateModified": "2025-06-05T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Franz Louis Cesista"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://leloykun.github.io/ponder/spectral-clipping/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Franz Louis Cesista",
    "logo": {
      "@type": "ImageObject",
      "url": "https://leloykun.github.io/favicon.ico"
    }
  }
}
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-BWCGBRX8G1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BWCGBRX8G1');
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"
  integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"
  integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js"
  integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
        { left: "\\begin{equation}", right: "\\end{equation}", display: true },
        { left: "\\begin{equation*}", right: "\\end{equation*}", display: true },
        { left: "\\begin{align}", right: "\\end{align}", display: true },
        { left: "\\begin{align*}", right: "\\end{align*}", display: true },
        { left: "\\begin{alignat}", right: "\\end{alignat}", display: true },
        { left: "\\begin{gather}", right: "\\end{gather}", display: true },
        { left: "\\begin{CD}", right: "\\end{CD}", display: true },
      ],
      throwOnError: false,
      trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
      macros: {
        "\\eqref": "\\href{###1}{(\\text{#1})}",
        "\\ref": "\\href{###1}{\\text{#1}}",
        "\\label": "\\htmlId{#1}{}"
      }
    });
  });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://leloykun.github.io/" accesskey="h" title="Franz Louis Cesista">
                <img src="https://leloykun.github.io/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Franz Louis Cesista</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://leloykun.github.io/cv.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/personal-projects/" title="Personal Projects">
                    <span>Personal Projects</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/ponder/" title="Ponder (Posts)">
                    <span>Ponder (Posts)</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping via Newton-Schulz Iteration
    </h1>
    <div class="post-meta"><span title='2025-06-05 00:00:00 +0000 UTC'>June 5, 2025</span>&nbsp;&middot;&nbsp;Franz Louis Cesista

</div>
  </header> 
<figure class="entry-cover">
            <img loading="eager"
                srcset='https://leloykun.github.io/ponder/spectral-clipping/clip_lifting_trap_fix_hu_8a03ea894ba25f98.png 360w,https://leloykun.github.io/ponder/spectral-clipping/clip_lifting_trap_fix_hu_37802ec989eaf9b7.png 480w,https://leloykun.github.io/ponder/spectral-clipping/clip_lifting_trap_fix_hu_732734ac446bcd59.png 720w,https://leloykun.github.io/ponder/spectral-clipping/clip_lifting_trap_fix_hu_c612e02410460272.png 1080w,https://leloykun.github.io/ponder/spectral-clipping/clip_lifting_trap_fix_hu_e92027da452a7b66.png 1500w,https://leloykun.github.io/ponder/spectral-clipping/clip_lifting_trap_fix.png 2370w'
                src="https://leloykun.github.io/ponder/spectral-clipping/clip_lifting_trap_fix.png"
                sizes="(min-width: 768px) 720px, 100vw"
                width="2370" height="1147"
                alt="Cover">
        
</figure><div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#1-introduction">1. Introduction</a>
      <ul>
        <li><a href="#potential-application-to-test-time-training-ttt">Potential application to test-time training (TTT)</a></li>
      </ul>
    </li>
    <li><a href="#2-towards-hardware-architecture-optimizer-codesign">2. Towards hardware-architecture-optimizer codesign</a>
      <ul>
        <li><a href="#finding-a-suitable-surrogate-function-for-textttclip">Finding a suitable surrogate function for $\texttt{clip}$</a></li>
        <li><a href="#lifting-to-matrix-form-the-naive--incorrect-way">Lifting to matrix form (the naive &amp; incorrect way)</a></li>
        <li><a href="#lifting-to-matrix-form-the-proper-way">Lifting to matrix form (the proper way)</a></li>
      </ul>
    </li>
    <li><a href="#3-variants-and-optimizations">3. Variants and optimizations</a>
      <ul>
        <li><a href="#sanity-check-orthogonalization-and-scaling">Sanity check: orthogonalization and scaling</a></li>
        <li><a href="#unbounded-below-spectral-hardcapping">Unbounded below: Spectral Hardcapping</a></li>
        <li><a href="#unbounded-above-spectral-shifted-relu">Unbounded above: Spectral (Shifted-)ReLU</a></li>
      </ul>
    </li>
    <li><a href="#4-an-alternative-approach-highams-anti-block-diagonal-trick">4. An alternative approach: Higham&rsquo;s Anti-Block-Diagonal Trick</a>
      <ul>
        <li><a href="#optimizing-the-implementation-via-abstract-algebra">Optimizing the implementation via abstract algebra</a></li>
        <li><a href="#taking-advantage-of-symmetry">Taking advantage of symmetry</a></li>
      </ul>
    </li>
    <li><a href="#5-runtime-analysis">5. Runtime analysis</a></li>
    <li><a href="#6-experimental-results-under-construction">6. Experimental results [Under Construction]</a>
      <ul>
        <li><a href="#grokking-under-construction">Grokking [Under Construction]</a></li>
        <li><a href="#nanogpt-speedrun-results-under-construction">NanoGPT Speedrun results [Under Construction]</a></li>
      </ul>
    </li>
    <li><a href="#acknowledgements">Acknowledgements</a></li>
    <li><a href="#how-to-cite">How to Cite</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="1-introduction">1. Introduction<a hidden class="anchor" aria-hidden="true" href="#1-introduction">#</a></h2>
<p>Here I&rsquo;ll discuss a fast, numerically stable, and (auto-)differentiable way to perform spectral clipping, i.e., clipping the singular values of a matrix to a certain range. This is useful in deep learning because it allows us to control the &lsquo;growth&rsquo; of our weights and weight updates, enabling faster and stabler feature learning (Yang et al., 2024; Large et al., 2024). As discussed in a <a href="../steepest-descent-non-riemannian/#22-feature-learning-perspective">previous post</a>,</p>
<blockquote>
<p>If we want the Euclidean norm of our features and feature updates to &lsquo;grow&rsquo; with the model size,
then the <em>Spectral norm</em> of our weights and weight updates must also &lsquo;grow&rsquo; with the model size.</p></blockquote>
<p>There are multiple ways to control the spectral norm of our (matrix-structured) weights and weight updates. One is to &ldquo;pull&rdquo; <strong>all</strong> of the singular values to some target value chosen a priori via the matrix sign function $\texttt{misgn}$. This is what the Muon optimizer already does, but only on the weight updates: it takes the raw gradient and tries to &ldquo;pull&rdquo; its as many of its singular values to $\sqrt{\frac{d_{out}}{d_{in}}}$. This guarantees that the update step merely changes the activation RMS-norm of that layer by at most $1$ unit. We <em>could</em> also apply this process to the weights after every update step to guarantee that the weight norms <em>would not</em> blow up, but constraining the weight space to the Stiefel manifold is too strong of a constraint. We discuss more of this in our upcoming Neurips preprint. For now, we will focus on Spectral Clipping:</p>
<blockquote>
<p><strong>Definition 1 (Spectral Clipping)</strong>. Let $W \in \mathbb{R}^{m \times n}$ and $W = U \Sigma V^T$ be its singular value decomposition where $\Sigma = (\sigma_1, \ldots, \sigma_{min(m,n)})$ are the singular values of $W$. Then we define Spectral Clipping as the following matrix function $\texttt{spectral\_clip}_{[\sigma_{min}, \sigma_{max}]}: \mathbb{R}^{m \times n} \to \mathbb{R}^{m \times n}$,
$$\begin{equation}\texttt{spectral\_clip}_{[\sigma_{min}, \sigma_{max}]}(W) = U \texttt{clip}_{[\sigma_{min}, \sigma_{max}]}(\Sigma) V^T\label{1}\end{equation}$$
where $\sigma_{min}, \sigma_{max} \in [0, \infty)$ are hyperparameters that control the minimum and maximum attainable singular values of the resulting matrix and $\texttt{clip}_{[\alpha, \beta]}: \mathbb{R} \to \mathbb{R}$ is applied element-wise on the singular values of $W$,</p>
<p>$$\begin{equation}\texttt{clip}_{[\alpha, \beta]}(x) = \begin{cases}
\alpha &amp; \texttt{if } x &lt; \alpha \\
x &amp; \texttt{if } \alpha \leq x \leq \beta \\
\beta &amp; \texttt{if } \beta &lt; x
\end{cases}\end{equation}$$
where $\alpha, \beta \in \mathbb{R} \cup \{-\infty, \infty\}$ and $\alpha \leq \beta$.</p></blockquote>
<p>Note that since the singular values of a matrix are guaranteed to be non-negative, $\texttt{clip}$ above does not need to be bidirectional. And setting $\alpha \leq 0$ and/or $\beta = \infty$ massively simpifies our (matrix) function, resulting in efficiency gains,</p>
<ul>
<li>$\texttt{clip}_{[\leq 0, \beta]}(x) = \min(x, \beta)$; and</li>
<li>$\texttt{clip}_{[\alpha, \infty]}(x) = \max(x, \alpha)$ which is simply the (shifted-)$\texttt{ReLU}$.</li>
</ul>
<p>In practice, the former would suffice for constraining the weights of neural networks. However, we will keep both parameters $\alpha, \beta$ in this work for generality and in case one would need to constrain the weights to always be full rank to prevent the activations from collapsing in dimension.</p>
<h3 id="potential-application-to-test-time-training-ttt">Potential application to test-time training (TTT)<a hidden class="anchor" aria-hidden="true" href="#potential-application-to-test-time-training-ttt">#</a></h3>
<p>As discussed in <a href="../test-time-regression/">previous</a> <a href="../blockmat-linear-attn/">posts</a>, (linear) attention mechanisms implicitly or explicitly perform test-time training (TTT) by learning to adapt the <em>attention state</em> as the model ingests more and more context <em>without</em> updating the model parameters. The core idea behind this is that we can hardcode a subnetwork and its optimizer into the model architecture itself and if this subnetwork-optimizer pair is end-to-end (auto-)differentiable, then in theory this should allow the model to learn methods on <em>how to learn</em> from the context it ingests which it can then use at test-time.</p>
<p>Recent work in this direction focuses on optimizing speed, stability, and expressiveness of such architectures (Yang et al., 2025; Grazzi et al., 2025). Hence the design choices in this post. In theory, we could use $\texttt{spectral\_clip}$ we construct here as an inner optimizer in a (linear) attention mechanism. In fact the team behind Atlas (Behrouz et al., 2025) has recently shown that the Muon optimizer (Jordan et al., 2024) <em>can</em> indeed be incorporated into an attention mechanism and that doing so not only improves performance but also reduces accuracy drop at longer context lengths. And as previously discussed by Su (2025),
$$\lim_{k \to \infty}\texttt{spectral\_clip}(kG) = \texttt{misgn}(G)$$
for some scalar $k \in \mathbb{R}$. Thus, we could simply swap in Muon&rsquo;s orthogonalization step with spectral clipping with minimal changes to the architecture. Alternatively, we could also apply it <em>after</em> applying Muon optimizer&rsquo;s update step to control the growth of the attention state and prevent it from blowing up. Think of this as a more theoretically-grounded way of constraining the weights vs. weight decay.</p>
<h2 id="2-towards-hardware-architecture-optimizer-codesign">2. Towards hardware-architecture-optimizer codesign<a hidden class="anchor" aria-hidden="true" href="#2-towards-hardware-architecture-optimizer-codesign">#</a></h2>
<p>In deep learning, we not only have to be mindful of architecture-optimizer codesign but also hardware-software codesign. That is, architectural and optimizer choices and how we implement them have to be hardware-aware so that we can squeeze as much performance as we can from our GPUs/TPUs.</p>
<p>For example, the naive way to compute Spectral Clipping is to directly compute the SVD, clip the singular values we get from it, then reconstruct the matrix using the clipped singular values. A JAX implementation would look like this:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">naive_spectral_clip</span>(W: jax.Array, sigma_min: <span style="color:#0aa">float</span>=-<span style="color:#099">1.</span>, sigma_max: <span style="color:#0aa">float</span>=<span style="color:#099">1.</span>):
</span></span><span style="display:flex;"><span>    U, S, Vt = jnp.linalg.svd(W, full_matrices=<span style="color:#00a">False</span>)
</span></span><span style="display:flex;"><span>    S_clipped = jnp.clip(S, <span style="color:#0aa">min</span>=sigma_min, <span style="color:#0aa">max</span>=sigma_max)
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> U @ jnp.diag(S_clipped) @ Vt
</span></span></code></pre></div><p>However, this is not recommended because computing the SVD directly (1) does not take advantage of the GPUs&rsquo; tensor cores and (2) requires higher numerical precision, typically 32-bit float types. These not only slow things down but also increase precious memory usage, making it hard to scale to larger models.</p>
<p>Ideally, we want to <em>only</em> use operations that (1) have fast implementations on GPUs/TPUs and (2) are stable under lower numerical precision, e.g., 16-bit, 8-bit, even 4-bit float types. So, elementwise operations like matrix addition and scalar multiplication, matrix multiplication, matrix-vector products, among others are preferred, but not operations like matrix inversions or SVD decomposition, etc. With the proper coefficients, (semi-)orthogonalization via Newton-Schulz iteration for computing the matrix sign function has also been shown to be fast and numerically stable under lower precision (Jordan et al., 2024), thus we can use that here.</p>
<h3 id="finding-a-suitable-surrogate-function-for-textttclip">Finding a suitable surrogate function for $\texttt{clip}$<a hidden class="anchor" aria-hidden="true" href="#finding-a-suitable-surrogate-function-for-textttclip">#</a></h3>
<p>This is the fun part.</p>
<p>So, how do we compute spectral clipping while only using simple, but fast &amp; numerically stable operations? First, let&rsquo;s list the operations we can actually use and consider how they act on the matrix itself and its singular values. There are more operations we can use that aren&rsquo;t listed here, but these would suffice for our problem.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"><strong>Operation</strong></th>
          <th style="text-align: center"><strong>Matrix form</strong></th>
          <th style="text-align: center"><strong>Action on<br>singular values</strong></th>
          <th style="text-align: center"><strong>Tensor cores<br>utilization</strong></th>
          <th style="text-align: center"><strong>Numerical stability<br>at low precision</strong></th>
          <th style="text-align: center"><strong>(Auto-)differentiable</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Linear combination</td>
          <td style="text-align: center">$c_1 W_1 + c_2 W_2$</td>
          <td style="text-align: center">$c_1 \Sigma_1 + c_2 \Sigma_2$</td>
          <td style="text-align: center">$\color{green}{\text{high}}$</td>
          <td style="text-align: center">$\color{green}{\text{yes}}$</td>
          <td style="text-align: center">$\color{green}{\text{yes}}$</td>
      </tr>
      <tr>
          <td style="text-align: left">Apply polynomial function</td>
          <td style="text-align: center">$\texttt{p}(W)$</td>
          <td style="text-align: center">$\texttt{p}(\Sigma)$</td>
          <td style="text-align: center">$\color{green}{\text{high}}$</td>
          <td style="text-align: center">$\color{green}{\text{yes}}$</td>
          <td style="text-align: center">$\color{green}{\text{yes}}$</td>
      </tr>
      <tr>
          <td style="text-align: left">Apply matrix sign function<br>(via Newton-Schulz iteration)</td>
          <td style="text-align: center">$\texttt{msign}(W)$</td>
          <td style="text-align: center">$\texttt{sign}(\Sigma)$</td>
          <td style="text-align: center">$\color{green}{\text{high}}$</td>
          <td style="text-align: center">$\color{green}{\text{yes}}$</td>
          <td style="text-align: center">$\color{green}{\text{yes}}$</td>
      </tr>
      <tr>
          <td style="text-align: left">Apply matrix sign function<br>(via QR-decomposition)</td>
          <td style="text-align: center">$\texttt{msign}(W)$</td>
          <td style="text-align: center">$\texttt{sign}(\Sigma)$</td>
          <td style="text-align: center">$\color{orange}{\text{medium}}$</td>
          <td style="text-align: center">$\color{green}{\text{yes}}$*<br>(<code>bfloat16</code>/<code>float16</code>+)</td>
          <td style="text-align: center">$\color{green}{\text{yes}}$<br>(in jax)</td>
      </tr>
      <tr>
          <td style="text-align: left">Apply matrix sign function<br>(via SVD)</td>
          <td style="text-align: center">$\texttt{msign}(W)$</td>
          <td style="text-align: center">$\texttt{sign}(\Sigma)$</td>
          <td style="text-align: center">$\color{red}{\text{low}}$</td>
          <td style="text-align: center">$\color{red}{\text{no}}$<br>(<code>float32</code>+)</td>
          <td style="text-align: center">$\color{red}{\text{no}}$</td>
      </tr>
  </tbody>
</table>
<p>Let&rsquo;s reconstruct the $\mathbb{R} \to \mathbb{R}$ clipping on the singular values with these elementary functions first, then let&rsquo;s use it to construct the matrix form. Here we take advantage of the following identity,
$$\begin{equation}|x| = x \cdot \texttt{sign}(x)\end{equation}$$
With this, we can now construct $\texttt{clip}$ as follows,</p>
<p><img loading="lazy" src="/ponder/spectral-clipping/clip_abs_trick.png#center"></p>
<blockquote>
<p><strong>Proposition 2 (Computing $\texttt{clip}$ via $\texttt{sign}$).</strong> Let $\alpha, \beta \in \mathbb{R} \cup \{-\infty, \infty\}$ and $\texttt{clip}: \mathbb{R} \to \mathbb{R}$ be the clipping function defined in Definition 1. Then,
$$\begin{equation}\texttt{clip}_{[\alpha, \beta]}(x) = \frac{\alpha + \beta + (\alpha - x)\texttt{sign}(\alpha - x) - (\beta - x)\texttt{sign}(\beta - x)}{2}\label{4}\end{equation}$$</p></blockquote>
<blockquote>
<p><strong>Proof:</strong> It would suffice to show that,
$$\begin{equation}\texttt{clip}_{[\alpha, \beta]}(x) = \frac{\alpha + \beta + |\alpha - x| - |\beta - x|}{2}\end{equation}$$
For this, we can simply check case-by-case,</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">$x$</th>
          <th style="text-align: center">$\left | \alpha - x\right | $</th>
          <th style="text-align: center">$\left | \beta - x\right | $</th>
          <th style="text-align: center">$\frac{\alpha + \beta +  | \alpha - x | - | \beta - x | }{2}$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">$x &lt; \alpha$</td>
          <td style="text-align: center">$\alpha - x$</td>
          <td style="text-align: center">$\beta - x$</td>
          <td style="text-align: center">$\alpha$</td>
      </tr>
      <tr>
          <td style="text-align: center">$\alpha \leq x \leq \beta $</td>
          <td style="text-align: center">$x - \alpha$</td>
          <td style="text-align: center">$\beta - x$</td>
          <td style="text-align: center">$x$</td>
      </tr>
      <tr>
          <td style="text-align: center">$\beta &lt; x$</td>
          <td style="text-align: center">$x - \alpha$</td>
          <td style="text-align: center">$x - \beta$</td>
          <td style="text-align: center">$\beta$</td>
      </tr>
  </tbody>
</table>
<p>Combining Equations (3) and (5) then gives us Equation $\eqref{4}$. $\blacksquare$</p></blockquote>
<h3 id="lifting-to-matrix-form-the-naive--incorrect-way">Lifting to matrix form (the naive &amp; incorrect way)<a hidden class="anchor" aria-hidden="true" href="#lifting-to-matrix-form-the-naive--incorrect-way">#</a></h3>
<p><img loading="lazy" src="/ponder/spectral-clipping/clip_lifting_trap.png#center"></p>
<p>A naive way to lift Equation $\eqref{4}$ above to matrix form is to simply replace the variables, scalar constants, and scalar (sub-)functions with their corresponding matrix form, i.e., replace $x$ with $W$, $1$ with $I$, and $\texttt{sign}(\cdot)$ with $\texttt{msign}(\cdot)$. This gives us the following matrix function,</p>
<p>$$\begin{align}
\texttt{f}(W) &amp;= \frac{1}{2} [(\alpha + \beta)I + (\alpha I - W) \texttt{msign}(\alpha I - W)^T\nonumber\\
&amp;\qquad\qquad\qquad\qquad\;\;- (\beta I - W) \texttt{msign}(\beta I - W)^T]
\end{align}$$</p>
<p>However, as communicated to me by You Jiacheng &amp; Su Jianlin, this does not work (see figure above) because $I$ may not share the same singular vectors as $W$.</p>
<p>Another problem is that $\texttt{f}$ does not preserve the dimensions of the input matrix $W$. To see this, note that both $\alpha I - W$ and $\texttt{msign}(\alpha I - W)$ have shape $m \times n$ and so $(\alpha I - W) \texttt{msign}(\alpha I - W)^T$ must have shape $m \times m$. The same is true for the other term.</p>
<p>$$\begin{aligned}
\texttt{f}(W) &amp;= \frac{1}{2} [(\alpha + \beta)I_{\color{red}{m \times m}} + (\alpha I - W) \texttt{msign}(\alpha I - W)^T\\
&amp;\qquad\qquad\qquad\qquad\qquad- \underbrace{\underbrace{(\beta I - W)}_{m \times n} \underbrace{\texttt{msign}(\beta I - W)^T}_{n \times m}}_{\color{red}{m \times m}}]
\end{aligned}$$</p>
<h3 id="lifting-to-matrix-form-the-proper-way">Lifting to matrix form (the proper way)<a hidden class="anchor" aria-hidden="true" href="#lifting-to-matrix-form-the-proper-way">#</a></h3>
<p><img loading="lazy" src="/ponder/spectral-clipping/clip_lifting_trap_fix.png#center"></p>
<p>To properly lift Equation $\eqref{4}$ to matrix form, let&rsquo;s combine it with Equation $\eqref{1}$,
$$\begin{align}
\texttt{spectral\_clip}_{[\alpha, \beta]}(W)
&amp;= U \texttt{clip}_{[\alpha, \beta]}(\Sigma) V^T\nonumber\\
&amp;= U \frac{(\alpha + \beta) I + (\alpha I - \Sigma)\texttt{sign}(\alpha I - \Sigma) - (\beta I - \Sigma)\texttt{sign}(\beta I - \Sigma)}{2} V^T\nonumber\\
&amp;= \frac{1}{2} [(\alpha + \beta) UV^T\nonumber\\
&amp;\qquad\qquad+ U (\alpha I - \Sigma ) \texttt{sign}(\alpha I - \Sigma) V^T\nonumber\\
&amp;\qquad\qquad- U (\beta I - \Sigma ) \texttt{sign}(\beta I - \Sigma) V^T]\nonumber\\
&amp;= \frac{1}{2} [(\alpha + \beta) UV^T\nonumber\\
&amp;\qquad\qquad+ U (\alpha I - \Sigma ) (V^TV) \texttt{sign}(\alpha I - \Sigma) (U^TU) V^T\nonumber\\
&amp;\qquad\qquad- U (\beta I - \Sigma ) (V^TV) \texttt{sign}(\beta I - \Sigma) (U^TU) V^T]\nonumber\\
&amp;= \frac{1}{2} [(\alpha + \beta) UV^T\nonumber\\
&amp;\qquad\qquad+ (\alpha UV^T - U\Sigma V^T) (V \texttt{sign}(\alpha I - \Sigma) U^T)(UV^T)\nonumber\\
&amp;\qquad\qquad- (\beta UV^T - U\Sigma V^T)  (V \texttt{sign}(\beta I - \Sigma) U^T)(UV^T)]\nonumber\\
&amp;= \frac{1}{2} [(\alpha + \beta) UV^T\nonumber\\
&amp;\qquad\qquad+ (\alpha UV^T - U\Sigma V^T) (U \texttt{sign}(\alpha I - \Sigma) V^T)^T(UV^T)\nonumber\\
&amp;\qquad\qquad- (\beta UV^T - U\Sigma V^T)  (U \texttt{sign}(\beta I - \Sigma) V^T)^T(UV^T)]\nonumber\\
&amp;= \frac{1}{2} [(\alpha + \beta) \texttt{msign}(W)\nonumber\\
&amp;\qquad\qquad+ (\alpha \cdot\texttt{msign}(W) - W) \texttt{msign}(\alpha \cdot\texttt{msign}(W) - W)^T\texttt{msign}(W)\nonumber\\
&amp;\qquad\qquad- (\beta  \cdot\texttt{msign}(W) - W) \texttt{msign}(\beta  \cdot\texttt{msign}(W) - W)^T\texttt{msign}(W)]\nonumber\\
\texttt{spectral\_clip}_{[\alpha, \beta]}(W)
&amp;= \frac{1}{2} [(\alpha + \beta)I\nonumber\\
&amp;\qquad\qquad+ (\alpha \cdot\texttt{msign}(W) - W) \texttt{msign}(\alpha \cdot\texttt{msign}(W) - W)^T\nonumber\\
&amp;\qquad\qquad- (\beta  \cdot\texttt{msign}(W) - W) \texttt{msign}(\beta  \cdot\texttt{msign}(W) - W)^T\nonumber\\
&amp;\qquad\qquad]\;\texttt{msign}(W)\label{7}
\end{align}$$</p>
<p>And viola, we&rsquo;re done. The following code implements this in JAX,</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">spectral_clip</span>(W: jax.Array, alpha: <span style="color:#0aa">float</span>=-<span style="color:#099">1.</span>, beta: <span style="color:#0aa">float</span>=<span style="color:#099">1.</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#00a">if</span> flip := W.shape[<span style="color:#099">0</span>] &gt; W.shape[<span style="color:#099">1</span>]:
</span></span><span style="display:flex;"><span>        W = W.T
</span></span><span style="display:flex;"><span>    OW = _orthogonalize_via_newton_schulz(W)
</span></span><span style="display:flex;"><span>    result = (<span style="color:#099">1</span>/<span style="color:#099">2</span>) * (
</span></span><span style="display:flex;"><span>        (alpha + beta) * jnp.eye(W.shape[<span style="color:#099">0</span>])
</span></span><span style="display:flex;"><span>        + (alpha * OW - W) @ _orthogonalize_via_newton_schulz(alpha * OW - W).T
</span></span><span style="display:flex;"><span>        - (beta * OW - W) @ _orthogonalize_via_newton_schulz(beta * OW - W).T
</span></span><span style="display:flex;"><span>    ) @ OW
</span></span><span style="display:flex;"><span>    <span style="color:#00a">if</span> flip:
</span></span><span style="display:flex;"><span>        result = result.T
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> result
</span></span></code></pre></div><p>where <code>_orthogonalize_via_newton_schulz</code> above implements Jordan et al.&rsquo;s (2024) Newton-Schulz iteration for computing the matrix sign function. Note that we&rsquo;re calling <code>_orthogonalize_via_newton_schulz</code> thrice here, which is not ideal.</p>
<h2 id="3-variants-and-optimizations">3. Variants and optimizations<a hidden class="anchor" aria-hidden="true" href="#3-variants-and-optimizations">#</a></h2>
<h3 id="sanity-check-orthogonalization-and-scaling">Sanity check: orthogonalization and scaling<a hidden class="anchor" aria-hidden="true" href="#sanity-check-orthogonalization-and-scaling">#</a></h3>
<p>As a simple test-case, let&rsquo;s verify that setting the lower and upper bounds to be equal results in orthogonalization and scaling of the input matrix, i.e., $\texttt{spectral\_clip}_{[\sigma, \sigma]}(W) = \sigma \cdot \texttt{msign}(W)$. From Equation $\eqref{7}$, we have,</p>
<p>$$\begin{aligned}
\texttt{spectral\_clip}_{[\sigma, \sigma]}(W)
&amp;= \frac{1}{2} [(\sigma + \sigma)I\nonumber\\
&amp;\qquad\qquad\cancel{+ (\sigma \cdot\texttt{msign}(W) - W) \texttt{msign}(\sigma \cdot\texttt{msign}(W) - W)^T}\nonumber\\
&amp;\qquad\qquad\cancel{- (\sigma  \cdot\texttt{msign}(W) - W) \texttt{msign}(\sigma  \cdot\texttt{msign}(W) - W)^T}\nonumber\\
&amp;\qquad\qquad]\;\texttt{msign}(W)\\
\texttt{spectral\_clip}_{[\sigma, \sigma]}(W) &amp;= \sigma \cdot \texttt{msign}(W)\quad\blacksquare
\end{aligned}$$</p>
<h3 id="unbounded-below-spectral-hardcapping">Unbounded below: Spectral Hardcapping<a hidden class="anchor" aria-hidden="true" href="#unbounded-below-spectral-hardcapping">#</a></h3>
<p><img loading="lazy" src="/ponder/spectral-clipping/spectral_hardcap.png#center"></p>
<blockquote>
<p>Note: Su (2025) calls this &ldquo;Singular Value Clipping&rdquo; or &ldquo;SVC&rdquo; while our upcoming paper calls this &ldquo;Spectral Hardcapping&rdquo;.</p></blockquote>
<p>Singular values are guaranteed to be non-negative, so if we only want to bound the singular values from above, we can simply set $\alpha = 0$ in Equation $\eqref{4}$, i.e.,
$$\begin{align}
\texttt{clip}_{[0, \beta]}(x) &amp;= \frac{0 + \beta + (0 - x)\texttt{sign}(0 - x) - (\beta - x)\texttt{sign}(\beta - x)}{2}\nonumber\\
\texttt{clip}_{[0, \beta]}(x) &amp;= \frac{\beta + x - (\beta - x)\texttt{sign}(\beta - x)}{2}
\end{align}$$
Setting $\beta = 1$ recovers Su&rsquo;s (2025) and You&rsquo;s (2025) results. And following the approach above, we get,
$$\begin{aligned}
\texttt{spectral\_hardcap}_\beta(W)
&amp;= \texttt{spectral\_clip}_{[0, \beta]}(W)\\
\texttt{spectral\_hardcap}_\beta(W)
&amp;= \frac{1}{2} [\beta \cdot \texttt{msign}(W) + W\\
&amp;\qquad\qquad- (\beta  \cdot\texttt{msign}(W) - W) \texttt{msign}(\beta  \cdot\texttt{msign}(W) - W)^T \texttt{msign}(W)]
\end{aligned}$$</p>
<p>The following code implements this in JAX,</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">spectral_hardcap</span>(W: jax.Array, beta: <span style="color:#0aa">float</span>=<span style="color:#099">1.</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#00a">if</span> flip := W.shape[<span style="color:#099">0</span>] &gt; W.shape[<span style="color:#099">1</span>]:
</span></span><span style="display:flex;"><span>        W = W.T
</span></span><span style="display:flex;"><span>    OW = _orthogonalize_via_newton_schulz(W)
</span></span><span style="display:flex;"><span>    aW = beta * OW - W
</span></span><span style="display:flex;"><span>    result = (<span style="color:#099">1</span>/<span style="color:#099">2</span>) * (beta*OW + W - aW @ _orthogonalize_via_newton_schulz(aW).T @ OW)
</span></span><span style="display:flex;"><span>    <span style="color:#00a">if</span> flip:
</span></span><span style="display:flex;"><span>        result = result.T
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> result
</span></span></code></pre></div><p>We are now only calling <code>_orthogonalize_via_newton_schulz</code> twice here.</p>
<h3 id="unbounded-above-spectral-shifted-relu">Unbounded above: Spectral (Shifted-)ReLU<a hidden class="anchor" aria-hidden="true" href="#unbounded-above-spectral-shifted-relu">#</a></h3>
<p><img loading="lazy" src="/ponder/spectral-clipping/spectral_relu.png#center"></p>
<p>If we only want to bound the singular values from below, we set $\beta = +\infty$ in Equation $\eqref{4}$. First note that for a fixed $x \in [0, \infty)$,
$$\lim_{\beta \to +\infty} \texttt{sign}(\beta - x) = +1$$
Thus,
$$\begin{align}
\texttt{clip}_{[\alpha, +\infty]}(x)
&amp;= \lim_{\beta \to +\infty}\frac{\alpha + \beta + (\alpha - x)\texttt{sign}(\alpha - x) - (\beta - x)\texttt{sign}(\beta - x)}{2}\nonumber\\
\texttt{clip}_{[\alpha, +\infty]}(x) &amp;= \frac{\alpha + x + (\alpha - x)\texttt{sign}(\alpha - x)}{2}
\end{align}$$
And following the approach above, we get,
$$\begin{aligned}
\texttt{spectral\_relu}_\alpha(W)
&amp;= \texttt{spectral\_clip}_{[\alpha, +\infty]}(W)\\
\texttt{spectral\_relu}_\alpha(W)
&amp;= \frac{1}{2} [\alpha \cdot \texttt{msign}(W) + W\\
&amp;\qquad\qquad+ (\alpha  \cdot\texttt{msign}(W) - W) \texttt{msign}(\alpha  \cdot\texttt{msign}(W) - W)^T \texttt{msign}(W)]
\end{aligned}$$</p>
<p>The following code implements this in JAX,</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">spectral_relu</span>(W: jax.Array, alpha: <span style="color:#0aa">float</span>=<span style="color:#099">1.</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#00a">if</span> flip := W.shape[<span style="color:#099">0</span>] &gt; W.shape[<span style="color:#099">1</span>]:
</span></span><span style="display:flex;"><span>        W = W.T
</span></span><span style="display:flex;"><span>    OW = _orthogonalize_via_newton_schulz(W)
</span></span><span style="display:flex;"><span>    aW = alpha * OW - W
</span></span><span style="display:flex;"><span>    result = (<span style="color:#099">1</span>/<span style="color:#099">2</span>) * (alpha*OW + W + aW @ _orthogonalize_via_newton_schulz(aW).T @ OW)
</span></span><span style="display:flex;"><span>    <span style="color:#00a">if</span> flip:
</span></span><span style="display:flex;"><span>        result = result.T
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> result
</span></span></code></pre></div><h2 id="4-an-alternative-approach-highams-anti-block-diagonal-trick">4. An alternative approach: Higham&rsquo;s Anti-Block-Diagonal Trick<a hidden class="anchor" aria-hidden="true" href="#4-an-alternative-approach-highams-anti-block-diagonal-trick">#</a></h2>
<p><img loading="lazy" src="/ponder/spectral-clipping/spectral_clip_abd_vs_nested_tight.gif#center"></p>
<p>In the previous sections, we apply our matrix function directly on $W$ resulting in nested applications of $\texttt{msign}$. Here, we will instead use Higham&rsquo;s anti-block-diagonal trick (Higham, 2008). This allows us to compute $\texttt{msign}$ only once, reducing the complexity of the operations albeit at the cost of more compute and memory usage. This trick may not be practical in most settings, but the reduced complexity of the operations may be worth it when designing linear attention mechanisms with the spectral clipping function as a &ldquo;sub-network&rdquo;. A neat property is that this would allow us to naturally scale test-time compute by scaling the number of steps in $\texttt{msign}$.</p>
<blockquote>
<p><strong>Theorem 3 (Higham&rsquo;s Anti-Block-Diagonal Trick)</strong>. Let $g: \mathbb{R} \to \mathbb{R}$ be an odd analytic scalar function, $W \in \mathbb{R}^{m \times n}$, and construct the block matrix $S \in \mathbb{R}^{(m+n) \times (m+n)}$ as,
$$S := \begin{bmatrix}
0 &amp; W \\
W^T &amp; 0
\end{bmatrix}$$
and let $g(S)$ as the primary matrix function defined from the scalar function $g$.
Then,
$$g(S) = \begin{bmatrix}
0 &amp; g(W) \\
g(W)^T &amp; 0
\end{bmatrix}$$
and hence,
$$g(W) = [g(S)]_{12} = [g(S)]_{21}^T$$</p></blockquote>
<p>Note that, for our optimization tricks below to work, our scalar function $\texttt{clip}_{[\alpha, \beta]}$ has to be <em>odd</em> which we will impose by setting,
$$\alpha = -\beta.$$
Also note that,
$$\texttt{clip}_{[-\sigma_{max}, \sigma_{max}]}(x) = \sigma_{max} \cdot \texttt{clip}_{[-1, 1]}(x / \sigma_{max})$$
and thus it would suffice to construct $\texttt{spectral\_clip}_{[-1, 1]}(\cdot)$ first and then,
$$\begin{equation}
\texttt{spectral\_clip}_{[-\sigma_{max}, \sigma_{max}]}(W) = \sigma_{max}\cdot\texttt{spectral\_clip}_{[-1, 1]}(W / \sigma_{max}).
\end{equation}$$</p>
<p>Now, applying Theorem 3 with $g = \texttt{clip}_{[-1, 1]}$ gives us,
$$\begin{equation}\texttt{spectral\_clip}_{[-1, 1]}(W) = \left[ \frac{(I+S) \texttt{msign}(I+S) - (I-S) \texttt{msign}(I-S)}{2} \right]_{12}\end{equation}$$</p>
<p>The following code implements this in JAX,</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">_spectral_clip</span>(W: jax.Array):
</span></span><span style="display:flex;"><span>    m, n = W.shape
</span></span><span style="display:flex;"><span>    I = jnp.eye(m + n)
</span></span><span style="display:flex;"><span>    S = jnp.block([[jnp.zeros((m, m)), W], [W.T, jnp.zeros((n, n))]])
</span></span><span style="display:flex;"><span>    gS = (<span style="color:#099">1</span>/<span style="color:#099">2</span>) * (
</span></span><span style="display:flex;"><span>        (I + S) @ _orthogonalize_via_newton_schulz (I + S)
</span></span><span style="display:flex;"><span>        - (I - S) @ _orthogonalize_via_newton_schulz (I - S)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> gS[:m, m:]  <span style="color:#aaa;font-style:italic"># read off the top-right block</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">spectral_clip</span>(W: jax.Array, sigma_max: <span style="color:#0aa">float</span>=<span style="color:#099">1.</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> sigma_max * _spectral_clip(W / sigma_max)
</span></span></code></pre></div><p>Note that we are still calling <code>_orthogonalize_via_newton_schulz</code> twice here, which is not ideal either. Luckily, there&rsquo;s a neat trick that allows us to compute it only once.</p>
<h3 id="optimizing-the-implementation-via-abstract-algebra">Optimizing the implementation via abstract algebra<a hidden class="anchor" aria-hidden="true" href="#optimizing-the-implementation-via-abstract-algebra">#</a></h3>
<p>First, notice that both
$$I + S = \begin{bmatrix}
I_m &amp; W \\
W^T &amp; I_n
\end{bmatrix}\qquad I - S = \begin{bmatrix}
I_m &amp; -W \\
-W^T &amp; I_n
\end{bmatrix}$$
are block matrices of the form
$$\begin{bmatrix}
P &amp; Q \\
Q^T &amp; R
\end{bmatrix}$$
where $P, R$ are symmetric matrices and $Q$ is an arbitrary matrix. It is a well-known result that such matrices form a linear sub-algebra $\mathcal{A}$, i.e., they are closed under addition, scalar multiplication, and matrix multiplication. This means that applying any polynomial function to these matrices will yield another matrix of the same form. And since we&rsquo;re calculating the matrix sign function with Newton-Schulz iteration, which is a composition of polynomial functions, its result must also be of the same form.</p>
<p>Another neat property we can take advantage of is that flipping the signs of the anti-diagonal blocks gets preserved under application of any analytic matrix function.</p>
<blockquote>
<p><strong>Proposition 4 (Parity w.r.t. $Q \to -Q$ when applying analytic matrix function $f(\cdot)$)</strong>.
Let $A \in \mathcal{A}$ such that, $$A := \begin{bmatrix}
P &amp; Q \\
Q^T &amp; R
\end{bmatrix}$$
for some arbitrary matrix $Q \in \mathbb{R}^{m \times n}$ and symmetric matrices $P \in \mathbb{R}^{m \times m}$, $R \in \mathbb{R}^{n \times n}$, let $f: \mathcal{A} \to \mathcal{A}$ be an analytic matrix function, and let
$$\begin{bmatrix}
\widetilde{P} &amp; \widetilde{Q} \\
\widetilde{Q}^T &amp; \widetilde{R}
\end{bmatrix} := f(A) = f\left(\begin{bmatrix}
P &amp; Q \\
Q^T &amp; R
\end{bmatrix}\right).$$
Then,
$$\begin{bmatrix}
\widetilde{P} &amp; -\widetilde{Q} \\
-\widetilde{Q}^T &amp; \widetilde{R}
\end{bmatrix} = f\left(\begin{bmatrix}
P &amp; -Q \\
-Q^T &amp; R
\end{bmatrix}\right).$$</p></blockquote>
<p>This is a standard result. To see why,</p>
<blockquote>
<p><strong>Proof</strong>. Let $J = \text{diag}(I_m, -I_n)$ so that $J^2 = I$ and $J^{-1} = J$. This makes $J A J = J A J^{-1}$ simply a change of basis, which is preserved under application of analytic matrix functions. Thus we have,
$$\begin{aligned}
Jf(A) J &amp;= f(JAJ)\\
\begin{bmatrix}
I_m &amp; 0 \\
0 &amp; -I_n
\end{bmatrix}
\begin{bmatrix}
\widetilde{P} &amp; -\widetilde{Q} \\
-\widetilde{Q}^T &amp; \widetilde{R}
\end{bmatrix}
\begin{bmatrix}
I_m &amp; 0 \\
0 &amp; -I_n
\end{bmatrix} &amp;= f\left(\begin{bmatrix}
I_m &amp; 0 \\
0 &amp; -I_n
\end{bmatrix}
\begin{bmatrix}
P &amp; -Q \\
-Q^T &amp; R
\end{bmatrix}
\begin{bmatrix}
I_m &amp; 0 \\
0 &amp; -I_n
\end{bmatrix}\right)\\
\begin{bmatrix}
\widetilde{P} &amp; -\widetilde{Q} \\
-\widetilde{Q}^T &amp; \widetilde{R}
\end{bmatrix} &amp;= f\left(\begin{bmatrix}
P &amp; -Q \\
-Q^T &amp; R
\end{bmatrix}\right)
\end{aligned} \quad \blacksquare$$</p></blockquote>
<p>Thus we have,
$$\begin{bmatrix}
\widetilde{P} &amp; \widetilde{Q} \\
\widetilde{Q}^T &amp; \widetilde{R}
\end{bmatrix} = \texttt{msign}(I + S)\qquad\qquad
\begin{bmatrix}
\widetilde{P} &amp; -\widetilde{Q} \\
-\widetilde{Q}^T &amp; \widetilde{R}
\end{bmatrix} = \texttt{msign}(I - S)$$
for some $\widetilde{Q} \in \mathbb{R}^{m \times n}$ and symmetric $\widetilde{P} \in \mathbb{R}^{m \times m}$, $\widetilde{R} \in \mathbb{R}^{n \times n}$. Together with Equation 11, we get,</p>
<p>$$\begin{align}
\texttt{spectral\_clip}_{[-1, 1]}(W) &amp;= \frac{1}{2}\left[\begin{bmatrix}
I_m &amp; W \\
W^T &amp; I_n
\end{bmatrix}
\begin{bmatrix}
\widetilde{P} &amp; \widetilde{Q} \\
\widetilde{Q}^T &amp; \widetilde{R}
\end{bmatrix} - \begin{bmatrix}
I_m &amp; -W \\
-W^T &amp; I_n
\end{bmatrix}
\begin{bmatrix}
\widetilde{P} &amp; -\widetilde{Q} \\
-\widetilde{Q}^T &amp; \widetilde{R}
\end{bmatrix}\right]_{12}\nonumber\\
&amp;= \frac{1}{2} \left[\begin{bmatrix}
\widetilde{P} + WQ^{*T} &amp; \widetilde{Q} + W\widetilde{R} \\
W^T\widetilde{P}+\widetilde{Q}^T &amp; W^T\widetilde{Q}^T + \widetilde{R}
\end{bmatrix} - \begin{bmatrix}
\widetilde{P} + WQ^{*T} &amp; -(\widetilde{Q} + W\widetilde{R}) \\
-(W^T\widetilde{P}+\widetilde{Q}^T) &amp; W^T\widetilde{Q}^T + \widetilde{R}
\end{bmatrix}\right]_{12}\nonumber\\
&amp;= \begin{bmatrix}
0 &amp; \widetilde{Q} + W\widetilde{R} \\
(\widetilde{Q} + \widetilde{P}W)^T &amp; 0
\end{bmatrix}_{12} \\
\texttt{spectral\_clip}_{[-1, 1]}(W) &amp;= \widetilde{Q} + W\widetilde{R}\qquad\text{ or }\qquad\widetilde{Q} + \widetilde{P} W\nonumber
\end{align}$$</p>
<p>This means that we only need to call <code>msign</code> once, and simply read off the blocks to compute the final result, leading to massive speedups. Also note that the diagonal blocks in Equation (12) are zero, which is what we expect from Theorem 3.</p>
<p>In JAX, this looks like the following:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">_spectral_clip</span>(W: jax.Array):
</span></span><span style="display:flex;"><span>    m, n = W.shape
</span></span><span style="display:flex;"><span>    H = jnp.block([[jnp.eye(m), W], [W.T, jnp.eye(n)]])
</span></span><span style="display:flex;"><span>    OH = _orthogonalize_via_newton_schulz(H)
</span></span><span style="display:flex;"><span>    P, Q = OH[:m, :m], OH[:m, m:]
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> Q + P @ W
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># Q, R = OH[:m, m:], OH[m:, m:]</span>
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># return Q + W @ R</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">spectral_clip</span>(W: jax.Array, sigma_max: <span style="color:#0aa">float</span>=<span style="color:#099">1.</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> sigma_max * _spectral_clip(W / sigma_max, <span style="color:#099">1</span>)
</span></span></code></pre></div><p>And a codegolf version would be,</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">spectral_clip_minimal</span>(W: jax.Array, sigma_max: <span style="color:#0aa">float</span>=<span style="color:#099">1.</span>, ortho_dtype=jnp.float32):
</span></span><span style="display:flex;"><span>    OH = _orthogonalize_via_newton_schulz (jnp.block([[jnp.eye(W.shape[<span style="color:#099">0</span>]), W / sigma_max], [W.T / sigma_max, jnp.eye(W.shape[<span style="color:#099">1</span>])]]).astype(ortho_dtype)).astype(W.dtype)
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> sigma_max*OH[:W.shape[<span style="color:#099">0</span>], W.shape[<span style="color:#099">0</span>]:] + OH[:W.shape[<span style="color:#099">0</span>], :W.shape[<span style="color:#099">0</span>]] @ W
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># return sigma_max*OH[:W.shape[0], W.shape[0]:] + W @ OH[W.shape[0]:, W.shape[0]:]</span>
</span></span></code></pre></div><h3 id="taking-advantage-of-symmetry">Taking advantage of symmetry<a hidden class="anchor" aria-hidden="true" href="#taking-advantage-of-symmetry">#</a></h3>
<p>The crux is that since both $I + S$ and $I - S$ are in the sub-algebra $\mathcal{A}$, Newton-Schulz iteration must preserve their block structure. Thus, we do not actually need to materialize the entire $(m + n) \times (m + n)$ block matrices. And note that,</p>
<p>$$\begin{aligned}
\begin{bmatrix}
P_i   &amp; Q_i\\
Q_i^T &amp; R_i
\end{bmatrix}\begin{bmatrix}
P_j   &amp; Q_j\\
Q_j^T &amp; R_j
\end{bmatrix}^T &amp;= \begin{bmatrix}
P_i P_j   + Q_i Q_j^T &amp; P_i Q_j   + Q_i R_j\\
Q_i^T P_j + R_i Q_j^T &amp; Q_i^T Q_j + R_i R_j
\end{bmatrix}
\end{aligned}$$
Thus we can implement the (blocked) matrix multiplications as,</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>@jax.jit
</span></span><span style="display:flex;"><span>def block_matmul(
</span></span><span style="display:flex;"><span>    P1: jax.Array, Q1: jax.Array, R1: jax.Array,
</span></span><span style="display:flex;"><span>    P2: jax.Array, Q2: jax.Array, R2: jax.Array,
</span></span><span style="display:flex;"><span>) -&gt; Tuple[jax.Array, jax.Array, jax.Array]:
</span></span><span style="display:flex;"><span>    P = P1 @ P2   + Q1 @ Q2.T
</span></span><span style="display:flex;"><span>    Q = P1 @ Q2   + Q1 @ R2
</span></span><span style="display:flex;"><span>    R = Q1.T @ Q2 + R1 @ R2
</span></span><span style="display:flex;"><span>    return P, Q, R
</span></span></code></pre></div><p>and implement one step of Newton-Schulz iteration as,</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">newton_schulz_iter</span>(
</span></span><span style="display:flex;"><span>    P: jax.Array, Q: jax.Array, R: jax.Array,
</span></span><span style="display:flex;"><span>    a: <span style="color:#0aa">float</span>, b: <span style="color:#0aa">float</span>, c: <span style="color:#0aa">float</span>,
</span></span><span style="display:flex;"><span>):
</span></span><span style="display:flex;"><span>    I_P = a * jnp.eye(P.shape[<span style="color:#099">0</span>], dtype=P.dtype)
</span></span><span style="display:flex;"><span>    I_R = a * jnp.eye(R.shape[<span style="color:#099">0</span>], dtype=R.dtype)
</span></span><span style="display:flex;"><span>    P2, Q2, R2 = block_matmul(P, Q, R, P, Q, R)
</span></span><span style="display:flex;"><span>    P4, Q4, R4 = block_matmul(P2, Q2, R2, P2, Q2, R2)
</span></span><span style="display:flex;"><span>    Ppoly = I_P + b * P2 + c * P4
</span></span><span style="display:flex;"><span>    Qpoly =       b * Q2 + c * Q4
</span></span><span style="display:flex;"><span>    Rpoly = I_R + b * R2 + c * R4
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> block_matmul(P, Q, R, Ppoly, Qpoly, Rpoly)
</span></span></code></pre></div><p>We then initialize the blocks as $P_0 = I_{m}$, $Q_0 = W$, and $R_0 = I_m$, apply Newton-Schulz iteration as described above to get $(\widetilde{P}, \widetilde{Q}, \widetilde{R})$, and finally return $\widetilde{Q} + W\widetilde{R}$ or $\widetilde{Q} + \widetilde{P} W$. This should give efficiency gains vs. the naive implementation.</p>
<h2 id="5-runtime-analysis">5. Runtime analysis<a hidden class="anchor" aria-hidden="true" href="#5-runtime-analysis">#</a></h2>
<p>From Jordan et al. (2024), computing the matrix sign function on a $m \times n$ matrix (WLOG let $m \leq n$) via $T$ steps of Newton-Schulz iterations with 5th degree odd polynomials requires at most $\approx 6Tnm^2$ matmul FLOPs. Thus,</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Operation</th>
          <th style="text-align: center">Number of $\texttt{msign}$ calls</th>
          <th style="text-align: right">Total FLOPs</th>
          <th style="text-align: right">FLOPs overhead<br>(w/ NanoGPT-140M<br>speedrun configs)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">$\texttt{msign}$ via Newton-Schulz</td>
          <td style="text-align: center">$1$</td>
          <td style="text-align: right">$6Tnm^2$</td>
          <td style="text-align: right">0.98%</td>
      </tr>
      <tr>
          <td style="text-align: left">$\texttt{spectral\_clip}_{[\alpha, \beta]}$<br>(via nested $\texttt{msign}$ in Section (2))</td>
          <td style="text-align: center">$3$</td>
          <td style="text-align: right">$(18T + 6)nm^2$</td>
          <td style="text-align: right">3.13%</td>
      </tr>
      <tr>
          <td style="text-align: left">$\texttt{spectral\_hardcap}$</td>
          <td style="text-align: center">$2$</td>
          <td style="text-align: right">$(12T + 4)nm^2$</td>
          <td style="text-align: right">2.08%</td>
      </tr>
      <tr>
          <td style="text-align: left">$\texttt{spectral\_relu}$</td>
          <td style="text-align: center">$2$</td>
          <td style="text-align: right">$(12T + 4)nm^2$</td>
          <td style="text-align: right">2.08%</td>
      </tr>
      <tr>
          <td style="text-align: left">$\texttt{spectral\_clip}_{[-\beta, \beta]}$<br>(via full-matrix anti-block-diagonal trick)</td>
          <td style="text-align: center">$1$<br>$(m+n) \times (m+n)$</td>
          <td style="text-align: right">$6T(n+m)^3$</td>
          <td style="text-align: right">7.81%</td>
      </tr>
      <tr>
          <td style="text-align: left">$\texttt{msign}$ via block-wise Newton-Schulz</td>
          <td style="text-align: center">$1$ (block-wise)</td>
          <td style="text-align: right">$36Tn^3$</td>
          <td style="text-align: right">-</td>
      </tr>
      <tr>
          <td style="text-align: left">$\texttt{spectral\_clip}_{[-\beta, \beta]}$<br>(via block-wise anti-block-diagonal trick)</td>
          <td style="text-align: center">$1$ (block-wise)</td>
          <td style="text-align: right">$(36T+1)n^3$</td>
          <td style="text-align: right">5.89%</td>
      </tr>
  </tbody>
</table>
<h2 id="6-experimental-results-under-construction">6. Experimental results [Under Construction]<a hidden class="anchor" aria-hidden="true" href="#6-experimental-results-under-construction">#</a></h2>
<p>This section is still under construction.</p>
<h3 id="grokking-under-construction">Grokking [Under Construction]<a hidden class="anchor" aria-hidden="true" href="#grokking-under-construction">#</a></h3>
<p>[Grokking experiments results will be added here]</p>
<h3 id="nanogpt-speedrun-results-under-construction">NanoGPT Speedrun results [Under Construction]<a hidden class="anchor" aria-hidden="true" href="#nanogpt-speedrun-results-under-construction">#</a></h3>
<p>[NanoGPT Speedrun results will be added here]</p>
<h2 id="acknowledgements">Acknowledgements<a hidden class="anchor" aria-hidden="true" href="#acknowledgements">#</a></h2>
<p>Many thanks to Rohan Anil for initiating a <a href="https://x.com/_arohan_/status/1929945590366122037" target="_blank">discussion thread on the topic on Twitter</a>, and to Arthur Breitman, You Jiacheng, and Su Jianlin for <a href="https://x.com/ArthurB/status/1929958284754330007" target="_blank">productive</a> <a href="https://x.com/YouJiacheng/status/1931029612102078749" target="_blank">discussions</a> on <a href="https://kexue.fm/archives/11006" target="_blank">the topic</a>.</p>
<h2 id="how-to-cite">How to Cite<a hidden class="anchor" aria-hidden="true" href="#how-to-cite">#</a></h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bibtex" data-lang="bibtex"><span style="display:flex;"><span><span style="color:#0a0;text-decoration:underline">@misc</span>{cesista2025spectralclipping,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">author</span> = <span style="color:#a50">{Franz Louis Cesista}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">title</span> = <span style="color:#a50">{&#34;Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping Via Newton-Schulz Iteration&#34;}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">year</span> = <span style="color:#a50">{2025}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">url</span> = <span style="color:#a50">{http://leloykun.github.io/ponder/spectral-clipping/}</span>,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ol>
<li>Greg Yang, James B. Simon, Jeremy Bernstein (2024). A Spectral Condition for Feature Learning. URL <a href="https://arxiv.org/abs/2310.17813" target="_blank">https://arxiv.org/abs/2310.17813</a></li>
<li>Tim Large, Yang Liu, Minyoung Huh, Hyojin Bahng, Phillip Isola, Jeremy Bernstein (2024). Scalable Optimization in the Modular Norm. URL <a href="https://arxiv.org/abs/2405.14813" target="_blank">https://arxiv.org/abs/2405.14813</a></li>
<li>Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim (2025). Parallelizing Linear Transformers with the Delta Rule over Sequence Length. URL <a href="https://arxiv.org/abs/2406.06484" target="_blank">https://arxiv.org/abs/2406.06484</a></li>
<li>Riccardo Grazzi, Julien Siems, Jörg K.H. Franke, Arber Zela, Frank Hutter, Massimiliano Pontil (2025). Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues. URL <a href="https://arxiv.org/abs/2411.12537" target="_blank">https://arxiv.org/abs/2411.12537</a></li>
<li>Ali Behrouz, Zeman Li, Praneeth Kacham, Majid Daliri, Yuan Deng, Peilin Zhong, Meisam Razaviyayn, Vahab Mirrokni (2025). ATLAS: Learning to Optimally Memorize the Context at Test Time. URL <a href="https://arxiv.org/abs/2505.23735" target="_blank">https://arxiv.org/abs/2505.23735</a></li>
<li>Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein (2024). Muon: An optimizer for hidden layers in neural networks. Available at: <a href="https://kellerjordan.github.io/posts/muon/" target="_blank">https://kellerjordan.github.io/posts/muon/</a></li>
<li>Jianlin Su (2025). Higher-order muP: A more concise but more intelligent spectral condition scaling. URL <a href="https://kexue.fm/archives/10795" target="_blank">https://kexue.fm/archives/10795</a></li>
<li>Higham, Nicholas J. (2008). Functions of Matrices: Theory and Computation. SIAM.</li>
<li>Jianlin Su (2025). Calculation of spectral_clip (singular value clipping) via msign. Available at: <a href="https://kexue.fm/archives/11006" target="_blank">https://kexue.fm/archives/11006</a></li>
<li>Jiacheng You (2025). On a more efficient way to compute spectral clipping via nested matrix sign functions. Available at: <a href="https://x.com/YouJiacheng/status/1931029612102078749" target="_blank">https://x.com/YouJiacheng/status/1931029612102078749</a></li>
<li>Arthur Breitman (2025). On using the matrix sign function for spectral clipping. Available at: <a href="https://x.com/ArthurB/status/1929958284754330007" target="_blank">https://x.com/ArthurB/status/1929958284754330007</a></li>
<li>Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra (2022). Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. URL <a href="https://arxiv.org/abs/2201.02177" target="_blank">https://arxiv.org/abs/2201.02177</a></li>
<li>Lucas Prieto, Melih Barsbey, Pedro A.M. Mediano, Tolga Birdal (2025). Grokking at the Edge of Numerical Stability. URL <a href="https://arxiv.org/abs/2501.04697" target="_blank">https://arxiv.org/abs/2501.04697</a></li>
<li>Amund Tveit, Bjørn Remseth, Arve Skogvold (2025). Muon Optimizer Accelerates Grokking. <a href="https://arxiv.org/abs/2504.16041" target="_blank">https://arxiv.org/abs/2504.16041</a></li>
<li>Jeremy Bernstein and Laker Newhouse. “Old optimizer, new norm: An anthology.” arXiv preprint arXiv:2409.20325 (2024).</li>
<li>Zixuan Chen, Xialin He, Yen-Jen Wang, Qiayuan Liao, Yanjie Ze, Zhongyu Li, S. Shankar Sastry, Jiajun Wu, Koushil Sreenath, Saurabh Gupta, Xue Bin Peng (2024). Learning Smooth Humanoid Locomotion through Lipschitz-Constrained Policies. URL <a href="https://arxiv.org/abs/2410.11825" target="_blank">https://arxiv.org/abs/2410.11825</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://leloykun.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://leloykun.github.io/tags/optimizers/">Optimizers</a></li>
      <li><a href="https://leloykun.github.io/tags/architecture-optimizer-codesign/">Architecture-Optimizer Codesign</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping via Newton-Schulz Iteration on x"
            href="https://x.com/intent/tweet/?text=Fast%2c%20Numerically%20Stable%2c%20and%20Auto-Differentiable%20Spectral%20Clipping%20via%20Newton-Schulz%20Iteration&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fspectral-clipping%2f&amp;hashtags=MachineLearning%2cOptimizers%2cArchitecture-OptimizerCodesign">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping via Newton-Schulz Iteration on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fspectral-clipping%2f&amp;title=Fast%2c%20Numerically%20Stable%2c%20and%20Auto-Differentiable%20Spectral%20Clipping%20via%20Newton-Schulz%20Iteration&amp;summary=Fast%2c%20Numerically%20Stable%2c%20and%20Auto-Differentiable%20Spectral%20Clipping%20via%20Newton-Schulz%20Iteration&amp;source=https%3a%2f%2fleloykun.github.io%2fponder%2fspectral-clipping%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping via Newton-Schulz Iteration on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fleloykun.github.io%2fponder%2fspectral-clipping%2f&title=Fast%2c%20Numerically%20Stable%2c%20and%20Auto-Differentiable%20Spectral%20Clipping%20via%20Newton-Schulz%20Iteration">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping via Newton-Schulz Iteration on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fleloykun.github.io%2fponder%2fspectral-clipping%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping via Newton-Schulz Iteration on whatsapp"
            href="https://api.whatsapp.com/send?text=Fast%2c%20Numerically%20Stable%2c%20and%20Auto-Differentiable%20Spectral%20Clipping%20via%20Newton-Schulz%20Iteration%20-%20https%3a%2f%2fleloykun.github.io%2fponder%2fspectral-clipping%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping via Newton-Schulz Iteration on telegram"
            href="https://telegram.me/share/url?text=Fast%2c%20Numerically%20Stable%2c%20and%20Auto-Differentiable%20Spectral%20Clipping%20via%20Newton-Schulz%20Iteration&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fspectral-clipping%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Fast, Numerically Stable, and Auto-Differentiable Spectral Clipping via Newton-Schulz Iteration on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Fast%2c%20Numerically%20Stable%2c%20and%20Auto-Differentiable%20Spectral%20Clipping%20via%20Newton-Schulz%20Iteration&u=https%3a%2f%2fleloykun.github.io%2fponder%2fspectral-clipping%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://leloykun.github.io/">Franz Louis Cesista</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
