<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Steepest Descent on Finsler-Structured (Matrix) Manifolds | Franz Louis Cesista</title>
<meta name="keywords" content="Machine Learning, Optimizers">
<meta name="description" content="Fast and robust model training.">
<meta name="author" content="Franz Louis Cesista">
<link rel="canonical" href="https://leloykun.github.io/ponder/steepest-descent-finsler/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e50e51e1b615f62676c32fb60e15e4b5e3a80ade29f3c6f9c953c528f576fb9f.css" integrity="sha256-5Q5R4bYV9iZ2wy&#43;2DhXkteOoCt4p88b5yVPFKPV2&#43;58=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://leloykun.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://leloykun.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://leloykun.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://leloykun.github.io/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://leloykun.github.io/ponder/steepest-descent-finsler/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Steepest Descent on Finsler-Structured (Matrix) Manifolds" />
<meta property="og:description" content="Fast and robust model training." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://leloykun.github.io/ponder/steepest-descent-finsler/" /><meta property="article:section" content="ponder" />
<meta property="article:published_time" content="2025-08-20T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-08-20T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Steepest Descent on Finsler-Structured (Matrix) Manifolds"/>
<meta name="twitter:description" content="Fast and robust model training."/>
<meta name="twitter:site" content="@leloykun"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Ponder",
      "item": "https://leloykun.github.io/ponder/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Steepest Descent on Finsler-Structured (Matrix) Manifolds",
      "item": "https://leloykun.github.io/ponder/steepest-descent-finsler/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Steepest Descent on Finsler-Structured (Matrix) Manifolds",
  "name": "Steepest Descent on Finsler-Structured (Matrix) Manifolds",
  "description": "Fast and robust model training.",
  "keywords": [
    "Machine Learning", "Optimizers"
  ],
  "articleBody": " If you find this post useful, please consider supporting my work by sponsoring me on GitHub: 1. Introduction In this blog post, we shall consider the problem of steepest descent on Finsler-structured (matrix) manifolds. This problem naturally arises in deep learning optimization because we want model training to be fast and robust. That is, we want our weight updates to maximally change activations (or outputs) while keeping both activations and weights stable.\nAs discussed in prior blog posts and our latest paper, we can achieve this by properly considering the geometry in which to ‘place’ our weights in. This then begs the questions,\nWhich geometry should we ‘place’ our weights in? And, How do we perform optimization in this geometry? For (1), note that we have two degrees of freedom here: the choice of the underlying manifold and the choice of metric or norm to equip to the tangent spaces of the manifold. The latter makes (2) tricky because the manifold we end up with could not only be non-Euclidean but even non-Riemannian–and work on non-Riemannian optimization is scarce to almost non-existent.\nWhile it might seem that we’re just inventing a difficult problem for bored mathematicians to solve, we will show in the next sections that we can motivate such problems with simple arguments and even lead to 1.5x to 2x speedup in large-scale LLM training.\nThis blog post generalizes work by Jeremy Bernstein and Jianlin Su on ‘Stiefel Muon’ to optimization on Finsler-structured (matrix) manifolds.\n2. Case studies 2.1. Case study #1: Muon Show contents of Section 2.1. Following Bernstein \u0026 Newhouse (2024), one can think of the Muon optimizer (Jordan et al., 2024) as doing steepest descent under the spectral norm on $\\mathbb{R}^{m \\times n}$. But why choose the spectral norm in the first place? Why not the simpler Frobenius norm?\nAs we discussed in previous blog posts,\nIf we want the “natural” norm of our features and feature updates to be stable regardless of the model size, then the “natural” norm of our weights and weight updates must also be stable regardless of the model size.\nwhere the ’natural’ feature norm here is the RMS norm or the scaled Euclidean norm while the ’natural’ weight norm is the RMS-to-RMS norm or the scaled spectral norm.\nNote that the spectral norm does not follow the Parallelogram Law and so it is not induced by an inner product and therefore non-Riemannian. It does, however, induce a Finsler-structure on the manifold–an example of what we’re trying to generalize here!\n2.2. Case study #2: steepest descent on spectral norm Finsler-structured spectral norm ball around the origin Show contents of Section 2.2. In our latest paper titled, Training Transformers with Enforced Lipschitz Bounds, we provide methods for keeping the weight norms regulated in addition to using the Muon optimizer. Although we did not explicitly mention it, one can interpret our approach as performing steepest descent on the spectral norm Finsler-structured spectral norm ball around the origin. Inside the norm ball, the space is locally similar to the previous case. But whenever the weights get sent outside of the norm ball, we retract them back via the weight norm controls we introduced in our paper. 2.3. Case study #3: steepest descent on spectral norm Finsler-structured Stiefel manifold The problem Jeremy, Jianlin, and I have been trying to solve then is this:\nGiven the current weight $W \\in \\texttt{St}(m, n)$ and a “raw gradient” we get via e.g. backpropagation $G \\in \\mathbb{R}^{m \\times n}$, we want to find the optimal update $A^*$ such that, $$\\begin{equation} A^* = \\arg\\max_{A \\in \\mathbb{R}^{m \\times n}} \\langle G, A \\rangle \\quad \\text{ s.t. } \\quad \\| A \\|_{2 \\to 2} \\leq 1,\\quad A \\in T_{W}\\texttt{St}(m, n) \\end{equation}$$\nInspired by a partial solution by Jianlin (which did not yet work at the time), I proposed heuristic solutions here. Jianlin then solved the problem via a fixed-point iteration method. Finally, Jeremy proposed a more general solution via the dual ascent algorithm. Cédric Simal also independently proposed studying the dual problem to me and Jeremy.\n3. General solution via block-wise Primal-Dual Hybrid Gradient Let $\\mathcal{M}$ be a (matrix) manifold and $\\| \\cdot \\|$ be a Finsler norm defined on the tangent spaces of $\\mathcal{M}$, both chosen a priori. We want to solve the problem,\nGiven the current weight $W \\in \\mathcal{M}$ and a “raw gradient” or differential we get via e.g. backpropagation $G \\in T_{W}^*\\mathcal{M} \\subseteq \\mathbb{R}^{m \\times n}$, we want to find the optimal update $A^*$ such that, $$\\begin{equation} A^* = \\arg\\max_{A \\in \\mathbb{R}^{m \\times n}} \\langle G, A \\rangle \\quad \\text{ s.t. } \\quad \\| A \\| \\leq 1,\\quad A \\in T_{W}\\mathcal{M} \\end{equation}$$\nReplacing the constraints with indicator functions yields, $$\\begin{equation} A^* = -\\arg\\min_{A \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\langle G, A \\rangle + \\mathcal{i}_{\\| \\cdot \\| \\leq 1}(A) + \\mathcal{i}_{T_{W}\\mathcal{M}}(A) \\right\\} \\end{equation}$$ where, $$ \\mathcal{i}_{\\| \\cdot \\| \\leq 1}(A) = \\begin{cases} 0 \u0026\\text{ if } \\| A \\| \\leq 1 \\\\ \\infty \u0026\\text{ otherwise} \\end{cases} \\qquad \\text{ and } \\qquad \\mathcal{i}_{T_{W}\\mathcal{M}}(A) = \\begin{cases} 0 \u0026\\text{ if } A \\in T_{W}\\mathcal{M} \\\\ \\infty \u0026\\text{ otherwise} \\end{cases} $$\nEquivalently, $$\\begin{equation} A^* = -\\arg\\min_{A \\in \\mathbb{R}^{m \\times n}} \\left\\{ f(A) + g(A) \\right\\} \\end{equation}$$ where $f(\\cdot) := \\mathcal{i}_{\\| \\cdot \\|_{\\leq 1}}(\\cdot)$ and $g(\\cdot) := \\mathcal{i}_{T_{W}\\mathcal{M}}(\\cdot) + \\langle G, \\cdot \\rangle$. Note that we can move the $\\langle G, \\cdot \\rangle$ term to $f$ instead, but as we will see later, the proximal operator for $g$ is simpler so we keep it there for improved numerical stability.\nWe can then split Equation (4) into two subproblems by ‘copying’ $A$, $$\\begin{equation} A^* = -\\left[\\arg\\min_{A,B \\in \\mathbb{R}^{m \\times n}} \\{f(A) + g(B)\\} \\quad \\text{ s.t. } \\quad A - B = 0\\right]_{A} \\end{equation}$$ This effectively blows up our solution search space, but one can easily prove that the optimal solution to the problem above also solves our original problem!\n3.1. Recasting as a primal-dual problem Define, $$ \\begin{align*} X \u0026:= \\begin{bmatrix} A \\\\ B \\end{bmatrix}\\\\ L \u0026:= \\begin{bmatrix} I \u0026 -I \\end{bmatrix} \\\\ \\mathcal{F}(X) \u0026:= f(A) + g(B) \\\\ \\mathcal{G}(Y) \u0026:= \\mathcal{i}_{\\{0\\}}(Y) = \\begin{cases} 0 \u0026\\text{ if } Y = 0 \\\\ \\infty \u0026\\text{ otherwise} \\end{cases} \\end{align*} $$ where $X \\in \\mathcal{X} = \\mathbb{R}^{2m \\times n}$, $Y \\in \\mathcal{Y} = \\mathbb{R}^{m \\times n}$, $L: \\mathcal{X} \\to \\mathcal{Y}$ is a linear operator, $\\mathcal{F}: \\mathcal{X} \\to \\mathbb{R}$, and $\\mathcal{G}: \\mathcal{Y} \\to \\mathbb{R}$.\nThen Equation (5) can be rewritten to, $$\\begin{align} A^* \u0026= -\\left[ \\arg\\min_{X \\in \\mathcal{X}} \\{\\mathcal{F}(X) + \\mathcal{G}(LX)\\} \\right]_{1} \\end{align}$$\nFenchel duality then yields the saddle problem, $$\\begin{align} \\min_{X \\in \\mathcal{X}} \\max_{Y \\in \\mathcal{Y}} \\mathcal{L}(X,Y) \u0026:= \\mathcal{F}(X) + \\langle LX, Y \\rangle - \\mathcal{G}^*(Y) \\nonumber \\\\ \u0026\\ = \\mathcal{F}(X) + \\langle LX, Y \\rangle \\end{align}$$ since $\\mathcal{G}^*(Y) = \\sup_{Z \\in \\mathcal{Y}} \\{ \\langle Y, Z \\rangle - \\underbrace{\\mathcal{G}(Z)}_{=\\infty \\text{ if } Z \\neq 0} \\} = \\langle Y, 0 \\rangle + \\mathcal{G}(0) = 0$ for all $Y \\in \\mathcal{Y}$.\n3.2. Block-wise Primal-Dual Hybrid Gradient Following ODL’s page on PDHG, we choose $\\tau_A, \\tau_B, \\sigma \u003e 0$, $\\theta \\in [0,1]$, and initialize $X_0 \\in \\mathcal{X}$, $Y_0 \\in \\mathcal{Y}$, and $\\widetilde{X}_0 = X_0$. We then iterate, $$\\begin{align} Y_{k+1} \u0026= \\texttt{prox}_{\\sigma \\mathcal{G}^*} (Y_{k} + \\sigma L \\widetilde{X}_{k}) \\\\ X_{k+1} \u0026= \\texttt{prox}_{\\tau \\mathcal{F}} (X_{k} - \\tau L^T Y_{k+1}) \\\\ \\widetilde{X}_{k+1} \u0026= X_{k+1} + \\theta (X_{k+1} - X_{k}) \\end{align}$$ where $\\tau = \\text{diag}(\\tau_A I_m, \\tau_B I_m)$ and $\\texttt{prox}$ is the proximal operator.\nTo speed up convergence, we can also re-use the $X^*$ and $Y^*$ from the previous optimization step to initialize $X_0$ and $Y_0$. This is especially useful when e.g. using (nesterov) momentum on $G$, guaranteeing that the ‘input gradients’ do not vary too much.\n3.2.1. Converting proximal operators to projections For the $Y$-variable, $$\\begin{align*} Y_{k+1} \u0026= \\texttt{prox}_{\\sigma \\mathcal{G}^*} (Y_{k} + \\sigma L \\widetilde{X}_{k}) \\\\ \u0026= \\arg\\min_{Y \\in \\mathcal{Y}} \\left\\{ \\sigma \\cancel{\\mathcal{G}^*(Y)} + \\frac{1}{2} \\| Y - (Y_{k} + \\sigma L \\widetilde{X}_{k}) \\|_F^2 \\right\\} \\\\ \u0026= Y_{k} + \\sigma L \\widetilde{X}_{k} \\end{align*}$$\nFor the $X$-variable, $$\\begin{align*} X_{k+1} \u0026= \\texttt{prox}_{\\tau \\mathcal{F}} (X_{k} - \\tau L^T Y_{k+1}) \\\\ \u0026= \\arg\\min_{X \\in \\mathcal{X}} \\left\\{ \\tau \\mathcal{F}(X) + \\frac{1}{2} \\| X - (X_{k} - \\tau L^T Y_{k+1}) \\|_F^2 \\right\\} \\\\ \u0026= \\arg\\min_{X \\in \\mathcal{X}} \\left\\{ \\tau_A f(A) + \\tau_B g(B) + \\frac{1}{2} \\left\\| \\begin{bmatrix} A - (A_k - \\tau_A Y_{k+1}) \\\\ B - (B_k + \\tau_B Y_{k+1}) \\end{bmatrix} \\right\\|_F^2 \\right\\} \\\\ \u0026= \\arg\\min_{X \\in \\mathcal{X}} \\{ \\tau_A f(A) + \\frac{1}{2} \\left\\| A - (A_k - \\tau_A Y_{k+1}) \\right\\|_F^2 \\\\ \u0026\\qquad\\qquad + \\tau_B g(B) + \\frac{1}{2} \\left\\| B - (B_k + \\tau_B Y_{k+1}) \\right\\|_F^2 \\} \\\\ \\end{align*}$$\nNote that we can optimize for $A$ and $B$ separately and thus get, $$\\begin{align*} A_{k+1} \u0026= \\arg\\min_{A \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\tau_A f(A) + \\frac{1}{2} \\left\\| A - (A_k - \\tau_A Y_{k+1}) \\right\\|_F^2 \\right\\} \\\\ \u0026= \\arg\\min_{\\| A \\| \\leq 1} \\left\\{ \\frac{1}{2} \\left\\| A - (A_k - \\tau_A Y_{k+1}) \\right\\|_F^2 \\right\\} \\\\ \u0026= \\texttt{proj}_{\\| \\cdot \\| \\leq 1} (A_k - \\tau_A Y_{k+1}) \\\\ \\end{align*}$$ where $\\texttt{proj}_{\\| \\cdot \\| \\leq 1}$ is the projection onto the unit norm ball. Likewise, $$\\begin{align*} B_{k+1} \u0026= \\arg\\min_{B \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\tau_B g(B) + \\frac{1}{2} \\left\\| B - (B_k + \\tau_B Y_{k+1}) \\right\\|_F^2 \\right\\} \\\\ \u0026= \\arg\\min_{B \\in T_W\\mathcal{M}} \\left\\{ \\tau_B \\langle G, B \\rangle + \\frac{1}{2} \\left\\| B - (B_k + \\tau_B Y_{k+1}) \\right\\|_F^2 \\right\\} \\\\ \u0026= \\arg\\min_{B \\in T_W\\mathcal{M}} \\left\\{ \\tau_B \\langle G, B \\rangle + \\frac{1}{2} \\| B \\|_F^2 - \\langle B, B_k + \\tau_B Y_{k+1} \\rangle + \\frac{1}{2} \\| B_k + \\tau_B Y_{k+1} \\|_F^2 \\right\\} \\\\ \u0026= \\arg\\min_{B \\in T_W\\mathcal{M}} \\left\\{ \\frac{1}{2} \\| B \\|_F^2 - \\langle B, B_k + \\tau_B Y_{k+1} - \\tau_B G \\rangle + \\text{ constant} \\right\\} \\\\ \u0026= \\arg\\min_{B \\in T_W\\mathcal{M}} \\left\\{ \\frac{1}{2} \\| B - (B_k + \\tau_B Y_{k+1} - \\tau_B G) \\|_F^2 + \\text{ constant} \\right\\} \\\\ \u0026= \\texttt{proj}_{T_W\\mathcal{M}} (B_k + \\tau_B Y_{k+1} - \\tau_B G) \\end{align*}$$ Thus, $$ \\begin{equation} X_{k+1} = \\begin{bmatrix} \\texttt{proj}_{\\| \\cdot \\| \\leq 1} (A_k - \\tau_A Y_{k+1}) \\\\ \\texttt{proj}_{T_W\\mathcal{M}} (B_k + \\tau_B Y_{k+1} - \\tau_B G) \\end{bmatrix} \\end{equation} $$\n4. Alternative solution to Stiefel Muon via Primal-Dual Hybrid Gradient Here we have $\\mathcal{M} = \\texttt{St}(m, n)$ and $\\| \\cdot \\| = \\| \\cdot \\|_{2 \\to 2}$. For the projection to the unit spectral norm ball, $\\texttt{proj}_{\\| \\cdot \\|_{2 \\to 2} \\leq 1}$, we can use the GPU/TPU-friendly spectral hardcap function discussed in my previous blog post and in our latest paper.\ndef spectral_hardcap(W: jax.Array): if transpose := W.shape[0] \u003e W.shape[1]: W = W.T OW = _orthogonalize_via_newton_schulz(W) aW = OW - W result = (1/2) * (OW + W - aW @ _orthogonalize_via_newton_schulz(aW).T @ OW) if transpose: result = result.T return result And for the projection to the tangent space at $W \\in \\texttt{St}(m, n)$, we can use the projection map discussed in Theorem 2 in this blog post,\n$$\\texttt{proj}_{T_W\\texttt{St}(m, n)}(V) = V - W \\text{sym}(W^T V)$$\n4.1. Full implementation with adaptive step sizes def pdhg_stiefel_spectral( W, G, *, tau_A=1.0, tau_B=1.0, sigma=0.49, gamma=1., max_iters=200, tol=1e-6, A0=None, B0=None, y0=None ): m, n = W.shape A = jnp.zeros((m, n), W.dtype) if A0 is None else A0 B = jnp.zeros((m, n), W.dtype) if B0 is None else B0 y = jnp.zeros((m, n), W.dtype) if y0 is None else y0 A_bar, B_bar = A, B def cond(state): _, _, _, _, _, k, res, *_= state return jnp.logical_and(k \u003c max_iters, res \u003e tol) def body(state): A, B, y, A_bar, B_bar, k, _, tau_A, tau_B, sigma = state # Dual ascent y_new = y + sigma * (A_bar - B_bar) # Primal descent (A \u0026 B updates) A_new = spectral_hardcap(A - tau_A * y_new) B_new = project_to_stiefel_tangent_space(W, B + tau_B * y_new - tau_B * G) # update step-sizes tau = 0.5 * (tau_A + tau_B) theta = 1 / jnp.sqrt(1 + 2 * gamma * tau) tau_A = theta * tau_A tau_B = theta * tau_B sigma = sigma / theta # Extrapolation A_bar_new = A_new + theta * (A_new - A) B_bar_new = B_new + theta * (B_new - B) res = jnp.linalg.norm(A_new - B_new) return (A_new, B_new, y_new, A_bar_new, B_bar_new, k+1, res, tau_A, tau_B, sigma) init = (A, B, y, A_bar, B_bar, 0, jnp.inf, tau_A, tau_B, sigma) A, B, y, *_ = jax.lax.while_loop(cond, body, init) return -A 4.2. Experimental results Here I’ve plotted the alignment \u003c-\u003e off-tangency frontier for the different methods proposed by myself, Jeremy and Jianlin. The alternating projections method seems to do well despite being provably suboptimal in some cases. But the PDHG method closes the gap as we increase the number of iteration. If we initialize $X_0$ and $Y_0$ from the previous optimization step, we can save compute while potentially improving performance.\n5. Generalization to arbitrary number of constraints on the update Our solution above generalizes to arbitrary number of constraints on $A$ so long as the feasible set for each constraint is convex. We then only need to find the metric projection onto each feasible set.\nFor example, suppose we add another constraint $A \\in S$ in Equation (2) above where $S$ is a convex set and $\\texttt{proj}_{S}(\\cdot)$ is the (metric) projection onto $S$. Then our Equation (5) becomes, $$\\begin{equation} A^* = -\\left[\\arg\\min_{A,B,C \\in \\mathbb{R}^{m \\times n}} \\{f(A) + g(B) + h(C)\\} \\quad \\text{ s.t. } \\quad A - B = A - C = 0\\right]_{A} \\end{equation}$$ where, $$ h(C) := \\mathcal{i}_{S}(C) = \\begin{cases} 0 \u0026\\text{ if } C \\in S \\\\ \\infty \u0026\\text{ otherwise} \\end{cases} $$\nWe then define, $$ \\begin{align*} X \u0026:= \\begin{bmatrix} A \\\\ B \\\\ C \\end{bmatrix}\\\\ L \u0026:= \\begin{bmatrix} I \u0026 -I \u0026 \\\\ I \u0026 \u0026 -I \\end{bmatrix} \\\\ \\mathcal{F}(X) \u0026:= f(A) + g(B) + h(C) \\\\ \\end{align*} $$ and the rest then follows and Equation (11) becomes, $$ \\begin{equation} X_{k+1} = \\begin{bmatrix} \\texttt{proj}_{\\| \\cdot \\| \\leq 1} (A_k - \\tau_A [Y_{k+1}]_1 - \\tau_A [Y_{k+1}]_2) \\\\ \\texttt{proj}_{T_W\\mathcal{M}} (B_k + \\tau_B [Y_{k+1}]_1 - \\tau_B G) \\\\ \\texttt{proj}_{S} (C_k + \\tau_C [Y_{k+1}]_2) \\end{bmatrix} \\end{equation} $$\nAcknowledgements Big thanks to Jeremy Bernstein and Cédric Simal for productive discussions on the topic!\nHow to cite @misc{cesista2025steepestdescentfinsler, author = {Franz Louis Cesista}, title = {\"Steepest Descent on Finsler-Structured (Matrix) Manifolds\"}, year = {2025}, url = {http://leloykun.github.io/ponder/steepest-descent-finsler/}, } If you find this post useful, please consider supporting my work by sponsoring me on GitHub: References Jeremy Bernstein (2025). Stiefel manifold. URL https://docs.modula.systems/algorithms/manifold/stiefel/ Jianlin Su (2025). Muon + Stiefel. URL https://kexue.fm/archives/11221 Laker Newhouse, R. Preston Hess, Franz Cesista, Andrii Zahorodnii, Jeremy Bernstein, Phillip Isola (2025). Training Transformers with Enforced Lipschitz Bounds. URL https://arxiv.org/abs/2507.13338 Jeremy Bernstein \u0026 Laker Newhouse (2024). Old optimizer, new norm: an anthology. URL https://arxiv.org/abs/2409.20325 Keller Jordan and Yuchen Jin and Vlado Boza and Jiacheng You and Franz Cesista and Laker Newhouse and Jeremy Bernstein (2024). Muon: An optimizer for hidden layers in neural networks. URL https://kellerjordan.github.io/posts/muon/ Greg Yang, James B. Simon, Jeremy Bernstein (2024). A Spectral Condition for Feature Learning. URL https://arxiv.org/abs/2310.17813 ODL (2020). Primal-Dual Hybrid Gradient Algorithm (PDHG). URL https://odlgroup.github.io/odl/math/solvers/nonsmooth/pdhg.html ",
  "wordCount" : "2461",
  "inLanguage": "en",
  "datePublished": "2025-08-20T00:00:00Z",
  "dateModified": "2025-08-20T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Franz Louis Cesista"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://leloykun.github.io/ponder/steepest-descent-finsler/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Franz Louis Cesista",
    "logo": {
      "@type": "ImageObject",
      "url": "https://leloykun.github.io/favicon.ico"
    }
  }
}
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-BWCGBRX8G1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BWCGBRX8G1');
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"
  integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"
  integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js"
  integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
        { left: "\\begin{equation}", right: "\\end{equation}", display: true },
        { left: "\\begin{equation*}", right: "\\end{equation*}", display: true },
        { left: "\\begin{align}", right: "\\end{align}", display: true },
        { left: "\\begin{align*}", right: "\\end{align*}", display: true },
        { left: "\\begin{alignat}", right: "\\end{alignat}", display: true },
        { left: "\\begin{gather}", right: "\\end{gather}", display: true },
        { left: "\\begin{CD}", right: "\\end{CD}", display: true },
      ],
      throwOnError: false,
      trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
      macros: {
        "\\eqref": "\\href{###1}{(\\text{#1})}",
        "\\ref": "\\href{###1}{\\text{#1}}",
        "\\label": "\\htmlId{#1}{}"
      }
    });
  });
</script>
 


</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://leloykun.github.io/" accesskey="h" title="Franz Louis Cesista">
                <img src="https://leloykun.github.io/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Franz Louis Cesista</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://leloykun.github.io/cv.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/personal-projects/" title="Personal Projects">
                    <span>Personal Projects</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/ponder/" title="Ponder (Posts)">
                    <span>Ponder (Posts)</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Steepest Descent on Finsler-Structured (Matrix) Manifolds
    </h1>
    <div class="post-meta"><span title='2025-08-20 00:00:00 +0000 UTC'>August 20, 2025</span>&nbsp;&middot;&nbsp;12 min&nbsp;&middot;&nbsp;Franz Louis Cesista

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#1-introduction">1. Introduction</a></li>
    <li><a href="#2-case-studies">2. Case studies</a>
      <ul>
        <li><a href="#21-case-study-1-muon">2.1. Case study #1: Muon</a></li>
        <li><a href="#22-case-study-2-steepest-descent-on-spectral-norm-finsler-structured-spectral-norm-ball-around-the-origin">2.2. Case study #2: steepest descent on spectral norm Finsler-structured spectral norm ball around the origin</a></li>
        <li><a href="#23-case-study-3-steepest-descent-on-spectral-norm-finsler-structured-stiefel-manifold">2.3. Case study #3: steepest descent on spectral norm Finsler-structured Stiefel manifold</a></li>
      </ul>
    </li>
    <li><a href="#3-general-solution-via-block-wise-primal-dual-hybrid-gradient">3. General solution via block-wise Primal-Dual Hybrid Gradient</a>
      <ul>
        <li><a href="#31-recasting-as-a-primal-dual-problem">3.1. Recasting as a primal-dual problem</a></li>
        <li><a href="#32-block-wise-primal-dual-hybrid-gradient">3.2. Block-wise Primal-Dual Hybrid Gradient</a></li>
      </ul>
    </li>
    <li><a href="#4-alternative-solution-to-stiefel-muon-via-primal-dual-hybrid-gradient">4. Alternative solution to Stiefel Muon via Primal-Dual Hybrid Gradient</a>
      <ul>
        <li><a href="#41-full-implementation-with-adaptive-step-sizes">4.1. Full implementation with adaptive step sizes</a></li>
        <li><a href="#42-experimental-results">4.2. Experimental results</a></li>
      </ul>
    </li>
    <li><a href="#5-generalization-to-arbitrary-number-of-constraints-on-the-update">5. Generalization to arbitrary number of constraints on the update</a></li>
    <li><a href="#acknowledgements">Acknowledgements</a></li>
    <li><a href="#how-to-cite">How to cite</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>If you find this post useful, please consider supporting my work by sponsoring me on GitHub: <a href="https://github.com/sponsors/leloykun" target="_blank"><img alt="Sponsor on GitHub" loading="lazy" src="https://img.shields.io/badge/%F0%9F%A4%9D-Sponsor%20me-1da1f2?logo=github&style=flat-square"></a></p></blockquote>
<h2 id="1-introduction">1. Introduction<a hidden class="anchor" aria-hidden="true" href="#1-introduction">#</a></h2>
<p>In this blog post, we shall consider the problem of steepest descent on <a href="https://en.wikipedia.org/wiki/Finsler_manifold" target="_blank">Finsler-structured (matrix) manifolds</a>. This problem naturally arises in deep learning optimization because we want model training to be <em>fast</em> and <em>robust</em>. That is, we want our weight updates to maximally change activations (or outputs) while keeping both activations and weights stable.</p>
<p>As discussed in <a href="../steepest-descent-opt/">prior</a> <a href="../steepest-descent-non-riemannian/">blog</a> <a href="../steepest-descent-finsler/">posts</a> and our <a href="https://arxiv.org/abs/2507.13338" target="_blank">latest paper</a>, we can achieve this by properly considering the geometry in which to &lsquo;place&rsquo; our weights in. This then begs the questions,</p>
<ol>
<li><em>Which</em> geometry should we &lsquo;place&rsquo; our weights in? And,</li>
<li><em>How</em> do we perform optimization in this geometry?</li>
</ol>
<p>For (1), note that we have <em>two</em> degrees of freedom here: the choice of the underlying manifold and the choice of metric or norm to equip to the tangent spaces of the manifold. The latter makes (2) tricky because the manifold we end up with could not only be non-Euclidean but even non-Riemannian&ndash;and work on non-Riemannian optimization is scarce to almost non-existent.</p>
<p>While it might seem that we&rsquo;re just inventing a difficult problem for bored mathematicians to solve, we will show in the next sections that we can motivate such problems with simple arguments and even lead to 1.5x to 2x speedup in large-scale LLM training.</p>
<p>This blog post generalizes work by <a href="https://docs.modula.systems/algorithms/manifold/stiefel/" target="_blank">Jeremy Bernstein</a> and <a href="https://kexue.fm/archives/11221" target="_blank">Jianlin Su</a> on &lsquo;Stiefel Muon&rsquo; to optimization on Finsler-structured (matrix) manifolds.</p>
<h2 id="2-case-studies">2. Case studies<a hidden class="anchor" aria-hidden="true" href="#2-case-studies">#</a></h2>
<h3 id="21-case-study-1-muon">2.1. Case study #1: Muon<a hidden class="anchor" aria-hidden="true" href="#21-case-study-1-muon">#</a></h3>


<p><details >
  <summary markdown="span">Show contents of <em>Section 2.1.</em></summary>
  <p>Following <a href="https://arxiv.org/abs/2409.20325" target="_blank">Bernstein &amp; Newhouse (2024)</a>, one can think of the <a href="https://kellerjordan.github.io/posts/muon/" target="_blank">Muon optimizer (Jordan et al., 2024)</a> as doing steepest descent under the spectral norm on $\mathbb{R}^{m \times n}$. But why choose the spectral norm in the first place? Why not the simpler Frobenius norm?</p>
<p>As we discussed in previous <a href="../steepest-descent-non-riemannian/">blog</a> <a href="../steepest-descent-finsler/">posts</a>,</p>
<blockquote>
<p>If we want the &ldquo;natural&rdquo; norm of our features and feature updates to be stable regardless of the model size,
then the &ldquo;natural&rdquo; norm of our weights and weight updates must also be stable regardless of the model size.</p></blockquote>
<p>where the &rsquo;natural&rsquo; feature norm here is the RMS norm or the scaled Euclidean norm while the &rsquo;natural&rsquo; weight norm is the RMS-to-RMS norm or the scaled spectral norm.</p>
<p>Note that the spectral norm does not follow the <a href="https://en.wikipedia.org/wiki/Parallelogram_law" target="_blank">Parallelogram Law</a> and so it is not induced by an inner product and therefore non-Riemannian. It does, however, induce a Finsler-structure on the manifold&ndash;an example of what we&rsquo;re trying to generalize here!</p>

</details></p>

<h3 id="22-case-study-2-steepest-descent-on-spectral-norm-finsler-structured-spectral-norm-ball-around-the-origin">2.2. Case study #2: steepest descent on spectral norm Finsler-structured spectral norm ball around the origin<a hidden class="anchor" aria-hidden="true" href="#22-case-study-2-steepest-descent-on-spectral-norm-finsler-structured-spectral-norm-ball-around-the-origin">#</a></h3>


<p><details >
  <summary markdown="span">Show contents of <em>Section 2.2.</em></summary>
  In our latest paper titled, <a href="https://arxiv.org/abs/2507.13338" target="_blank">Training Transformers with Enforced Lipschitz Bounds</a>, we provide methods for keeping the weight norms regulated in addition to using the Muon optimizer. Although we did not explicitly mention it, one can interpret our approach as performing steepest descent on the spectral norm Finsler-structured spectral norm ball around the origin. Inside the norm ball, the space is locally similar to the previous case. But whenever the weights get sent outside of the norm ball, we retract them back via the weight norm controls we introduced in our paper.
</details></p>

<h3 id="23-case-study-3-steepest-descent-on-spectral-norm-finsler-structured-stiefel-manifold">2.3. Case study #3: steepest descent on spectral norm Finsler-structured Stiefel manifold<a hidden class="anchor" aria-hidden="true" href="#23-case-study-3-steepest-descent-on-spectral-norm-finsler-structured-stiefel-manifold">#</a></h3>
<p>The problem Jeremy, Jianlin, and I have been trying to solve then is this:</p>
<blockquote>
<p>Given the current weight $W \in \texttt{St}(m, n)$ and a &ldquo;raw gradient&rdquo; we get via e.g. backpropagation $G \in \mathbb{R}^{m \times n}$, we want to find the optimal update $A^*$ such that,
$$\begin{equation}
A^* = \arg\max_{A \in \mathbb{R}^{m \times n}} \langle G, A \rangle \quad \text{ s.t. } \quad \| A \|_{2 \to 2} \leq 1,\quad A \in T_{W}\texttt{St}(m, n)
\end{equation}$$</p></blockquote>
<p>Inspired by a partial solution by Jianlin (which did not yet work at the time), I proposed <a href="../steepest-descent-stiefel/">heuristic solutions here</a>. Jianlin then <a href="https://kexue.fm/archives/11221" target="_blank">solved the problem</a> via a fixed-point iteration method. Finally, Jeremy proposed a <a href="https://docs.modula.systems/algorithms/manifold/stiefel/" target="_blank">more general solution</a> via the dual ascent algorithm. <a href="https://scholar.google.com/citations?user=Vo3M-WIAAAAJ&amp;hl" target="_blank">Cédric Simal</a> also independently proposed studying the dual problem to me and Jeremy.</p>
<h2 id="3-general-solution-via-block-wise-primal-dual-hybrid-gradient">3. General solution via block-wise Primal-Dual Hybrid Gradient<a hidden class="anchor" aria-hidden="true" href="#3-general-solution-via-block-wise-primal-dual-hybrid-gradient">#</a></h2>
<p>Let $\mathcal{M}$ be a (matrix) manifold and $\| \cdot \|$ be a Finsler norm defined on the tangent spaces of $\mathcal{M}$, both chosen a priori. We want to solve the problem,</p>
<blockquote>
<p>Given the current weight $W \in \mathcal{M}$ and a &ldquo;raw gradient&rdquo; or differential we get via e.g. backpropagation $G \in T_{W}^*\mathcal{M} \subseteq \mathbb{R}^{m \times n}$, we want to find the optimal update $A^*$ such that,
$$\begin{equation} A^* = \arg\max_{A \in \mathbb{R}^{m \times n}} \langle G, A \rangle \quad \text{ s.t. } \quad \| A \| \leq 1,\quad A \in T_{W}\mathcal{M} \end{equation}$$</p></blockquote>
<p>Replacing the constraints with indicator functions yields,
$$\begin{equation} A^* = -\arg\min_{A \in \mathbb{R}^{m \times n}} \left\{ \langle G, A \rangle + \mathcal{i}_{\| \cdot \| \leq 1}(A) + \mathcal{i}_{T_{W}\mathcal{M}}(A) \right\} \end{equation}$$
where,
$$ \mathcal{i}_{\| \cdot \| \leq 1}(A) =
\begin{cases}
0 &amp;\text{ if } \| A \| \leq 1 \\
\infty &amp;\text{ otherwise}
\end{cases}
\qquad \text{ and } \qquad
\mathcal{i}_{T_{W}\mathcal{M}}(A) =
\begin{cases}
0 &amp;\text{ if } A \in T_{W}\mathcal{M} \\
\infty &amp;\text{ otherwise}
\end{cases}
$$</p>
<p>Equivalently,
$$\begin{equation} A^* = -\arg\min_{A \in \mathbb{R}^{m \times n}} \left\{ f(A) + g(A) \right\} \end{equation}$$
where $f(\cdot) := \mathcal{i}_{\| \cdot \|_{\leq 1}}(\cdot)$ and $g(\cdot) := \mathcal{i}_{T_{W}\mathcal{M}}(\cdot) + \langle G, \cdot \rangle$. Note that we can move the $\langle G, \cdot \rangle$ term to $f$ instead, but as we will see later, the proximal operator for $g$ is simpler so we keep it there for improved numerical stability.</p>
<p>We can then split Equation (4) into two subproblems by &lsquo;copying&rsquo; $A$,
$$\begin{equation} A^* = -\left[\arg\min_{A,B \in \mathbb{R}^{m \times n}} \{f(A) + g(B)\} \quad \text{ s.t. } \quad A - B = 0\right]_{A} \end{equation}$$
This effectively blows up our solution search space, but one can easily prove that the optimal solution to the problem above also solves our original problem!</p>
<h3 id="31-recasting-as-a-primal-dual-problem">3.1. Recasting as a primal-dual problem<a hidden class="anchor" aria-hidden="true" href="#31-recasting-as-a-primal-dual-problem">#</a></h3>
<p>Define,
$$
\begin{align*}
X &amp;:= \begin{bmatrix}
A \\
B
\end{bmatrix}\\
L &amp;:= \begin{bmatrix} I &amp; -I \end{bmatrix} \\
\mathcal{F}(X) &amp;:= f(A) + g(B) \\
\mathcal{G}(Y) &amp;:= \mathcal{i}_{\{0\}}(Y) = \begin{cases}
0 &amp;\text{ if } Y = 0 \\
\infty &amp;\text{ otherwise}
\end{cases}
\end{align*}
$$
where $X \in \mathcal{X} = \mathbb{R}^{2m \times n}$, $Y \in \mathcal{Y} = \mathbb{R}^{m \times n}$, $L: \mathcal{X} \to \mathcal{Y}$ is a linear operator, $\mathcal{F}: \mathcal{X} \to \mathbb{R}$, and $\mathcal{G}: \mathcal{Y} \to \mathbb{R}$.</p>
<p>Then Equation (5) can be rewritten to,
$$\begin{align}
A^* &amp;= -\left[ \arg\min_{X \in \mathcal{X}} \{\mathcal{F}(X) + \mathcal{G}(LX)\} \right]_{1}
\end{align}$$</p>
<p>Fenchel duality then yields the saddle problem,
$$\begin{align}
\min_{X \in \mathcal{X}} \max_{Y \in \mathcal{Y}} \mathcal{L}(X,Y)
&amp;:= \mathcal{F}(X) + \langle LX, Y \rangle - \mathcal{G}^*(Y) \nonumber \\
&amp;\ = \mathcal{F}(X) + \langle LX, Y \rangle
\end{align}$$
since $\mathcal{G}^*(Y) = \sup_{Z \in \mathcal{Y}} \{ \langle Y, Z \rangle - \underbrace{\mathcal{G}(Z)}_{=\infty \text{ if } Z \neq 0} \} = \langle Y, 0 \rangle + \mathcal{G}(0) = 0$ for all $Y \in \mathcal{Y}$.</p>
<h3 id="32-block-wise-primal-dual-hybrid-gradient">3.2. Block-wise Primal-Dual Hybrid Gradient<a hidden class="anchor" aria-hidden="true" href="#32-block-wise-primal-dual-hybrid-gradient">#</a></h3>
<p>Following <a href="https://odlgroup.github.io/odl/math/solvers/nonsmooth/pdhg.html" target="_blank">ODL&rsquo;s page on PDHG</a>, we choose $\tau_A, \tau_B, \sigma &gt; 0$, $\theta \in [0,1]$, and initialize $X_0 \in \mathcal{X}$, $Y_0 \in \mathcal{Y}$, and $\widetilde{X}_0 = X_0$. We then iterate,
$$\begin{align}
Y_{k+1} &amp;= \texttt{prox}_{\sigma \mathcal{G}^*} (Y_{k} + \sigma L \widetilde{X}_{k}) \\
X_{k+1} &amp;= \texttt{prox}_{\tau \mathcal{F}} (X_{k} - \tau L^T Y_{k+1}) \\
\widetilde{X}_{k+1} &amp;= X_{k+1} + \theta (X_{k+1} - X_{k})
\end{align}$$
where $\tau = \text{diag}(\tau_A I_m, \tau_B I_m)$ and $\texttt{prox}$ is the proximal operator.</p>
<p>To speed up convergence, we can also re-use the $X^*$ and $Y^*$ from the previous optimization step to initialize $X_0$ and $Y_0$. This is especially useful when e.g. using (nesterov) momentum on $G$, guaranteeing that the &lsquo;input gradients&rsquo; do not vary too much.</p>
<h4 id="321-converting-proximal-operators-to-projections">3.2.1. Converting proximal operators to projections<a hidden class="anchor" aria-hidden="true" href="#321-converting-proximal-operators-to-projections">#</a></h4>
<p>For the $Y$-variable,
$$\begin{align*}
Y_{k+1}
&amp;= \texttt{prox}_{\sigma \mathcal{G}^*} (Y_{k} + \sigma L \widetilde{X}_{k}) \\
&amp;= \arg\min_{Y \in \mathcal{Y}} \left\{ \sigma \cancel{\mathcal{G}^*(Y)} + \frac{1}{2} \| Y - (Y_{k} + \sigma L \widetilde{X}_{k}) \|_F^2 \right\} \\
&amp;= Y_{k} + \sigma L \widetilde{X}_{k}
\end{align*}$$</p>
<p>For the $X$-variable,
$$\begin{align*}
X_{k+1}
&amp;= \texttt{prox}_{\tau \mathcal{F}} (X_{k} - \tau L^T Y_{k+1}) \\
&amp;= \arg\min_{X \in \mathcal{X}} \left\{ \tau \mathcal{F}(X) + \frac{1}{2} \| X - (X_{k} - \tau L^T Y_{k+1}) \|_F^2 \right\} \\
&amp;= \arg\min_{X \in \mathcal{X}} \left\{ \tau_A f(A) + \tau_B g(B) + \frac{1}{2} \left\| \begin{bmatrix}
A - (A_k - \tau_A Y_{k+1}) \\
B - (B_k + \tau_B Y_{k+1})
\end{bmatrix} \right\|_F^2 \right\} \\
&amp;= \arg\min_{X \in \mathcal{X}} \{ \tau_A f(A) + \frac{1}{2} \left\| A - (A_k - \tau_A Y_{k+1}) \right\|_F^2 \\
&amp;\qquad\qquad + \tau_B g(B) + \frac{1}{2} \left\| B - (B_k + \tau_B Y_{k+1}) \right\|_F^2 \} \\
\end{align*}$$</p>
<p>Note that we can optimize for $A$ and $B$ separately and thus get,
$$\begin{align*}
A_{k+1}
&amp;= \arg\min_{A \in \mathbb{R}^{m \times n}} \left\{ \tau_A f(A) + \frac{1}{2} \left\| A - (A_k - \tau_A Y_{k+1}) \right\|_F^2 \right\} \\
&amp;= \arg\min_{\| A \| \leq 1} \left\{ \frac{1}{2} \left\| A - (A_k - \tau_A Y_{k+1}) \right\|_F^2 \right\} \\
&amp;= \texttt{proj}_{\| \cdot \| \leq 1} (A_k - \tau_A Y_{k+1}) \\
\end{align*}$$
where $\texttt{proj}_{\| \cdot \| \leq 1}$ is the projection onto the unit norm ball. Likewise,
$$\begin{align*}
B_{k+1}
&amp;= \arg\min_{B \in \mathbb{R}^{m \times n}} \left\{ \tau_B g(B) + \frac{1}{2} \left\| B - (B_k + \tau_B Y_{k+1}) \right\|_F^2 \right\} \\
&amp;= \arg\min_{B \in T_W\mathcal{M}} \left\{ \tau_B \langle G, B \rangle + \frac{1}{2} \left\| B - (B_k + \tau_B Y_{k+1}) \right\|_F^2 \right\} \\
&amp;= \arg\min_{B \in T_W\mathcal{M}} \left\{ \tau_B \langle G, B \rangle + \frac{1}{2} \| B \|_F^2 - \langle B, B_k + \tau_B Y_{k+1} \rangle     + \frac{1}{2} \| B_k + \tau_B Y_{k+1} \|_F^2 \right\} \\
&amp;= \arg\min_{B \in T_W\mathcal{M}} \left\{ \frac{1}{2} \| B \|_F^2 - \langle B, B_k + \tau_B Y_{k+1} - \tau_B G \rangle + \text{ constant} \right\} \\
&amp;= \arg\min_{B \in T_W\mathcal{M}} \left\{ \frac{1}{2} \| B - (B_k + \tau_B Y_{k+1} - \tau_B G) \|_F^2 + \text{ constant} \right\} \\
&amp;= \texttt{proj}_{T_W\mathcal{M}} (B_k + \tau_B Y_{k+1} - \tau_B G)
\end{align*}$$
Thus,
$$
\begin{equation}
X_{k+1} = \begin{bmatrix}
\texttt{proj}_{\| \cdot \| \leq 1} (A_k - \tau_A Y_{k+1}) \\
\texttt{proj}_{T_W\mathcal{M}} (B_k + \tau_B Y_{k+1} - \tau_B G)
\end{bmatrix}
\end{equation}
$$</p>
<h2 id="4-alternative-solution-to-stiefel-muon-via-primal-dual-hybrid-gradient">4. Alternative solution to Stiefel Muon via Primal-Dual Hybrid Gradient<a hidden class="anchor" aria-hidden="true" href="#4-alternative-solution-to-stiefel-muon-via-primal-dual-hybrid-gradient">#</a></h2>
<p>Here we have $\mathcal{M} = \texttt{St}(m, n)$ and $\| \cdot \| = \| \cdot \|_{2 \to 2}$. For the projection to the unit spectral norm ball, $\texttt{proj}_{\| \cdot \|_{2 \to 2} \leq 1}$, we can use the GPU/TPU-friendly spectral hardcap function discussed in my <a href="../spectral-clipping/">previous blog post</a> and in <a href="https://arxiv.org/abs/2507.13338" target="_blank">our latest paper</a>.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">spectral_hardcap</span>(W: jax.Array):
</span></span><span style="display:flex;"><span>    <span style="color:#00a">if</span> transpose := W.shape[<span style="color:#099">0</span>] &gt; W.shape[<span style="color:#099">1</span>]:
</span></span><span style="display:flex;"><span>        W = W.T
</span></span><span style="display:flex;"><span>    OW = _orthogonalize_via_newton_schulz(W)
</span></span><span style="display:flex;"><span>    aW = OW - W
</span></span><span style="display:flex;"><span>    result = (<span style="color:#099">1</span>/<span style="color:#099">2</span>) * (OW + W - aW @ _orthogonalize_via_newton_schulz(aW).T @ OW)
</span></span><span style="display:flex;"><span>    <span style="color:#00a">if</span> transpose:
</span></span><span style="display:flex;"><span>        result = result.T
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> result
</span></span></code></pre></div><p>And for the projection to the tangent space at $W \in \texttt{St}(m, n)$, we can use the projection map discussed in <a href="../steepest-descent-stiefel/">Theorem 2 in this blog post</a>,</p>
<p>$$\texttt{proj}_{T_W\texttt{St}(m, n)}(V) = V - W \text{sym}(W^T V)$$</p>
<h3 id="41-full-implementation-with-adaptive-step-sizes">4.1. Full implementation with adaptive step sizes<a hidden class="anchor" aria-hidden="true" href="#41-full-implementation-with-adaptive-step-sizes">#</a></h3>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">pdhg_stiefel_spectral</span>(
</span></span><span style="display:flex;"><span>    W, G, *,
</span></span><span style="display:flex;"><span>    tau_A=<span style="color:#099">1.0</span>, tau_B=<span style="color:#099">1.0</span>, sigma=<span style="color:#099">0.49</span>, gamma=<span style="color:#099">1.</span>,
</span></span><span style="display:flex;"><span>    max_iters=<span style="color:#099">200</span>, tol=<span style="color:#099">1e-6</span>,
</span></span><span style="display:flex;"><span>    A0=<span style="color:#00a">None</span>, B0=<span style="color:#00a">None</span>, y0=<span style="color:#00a">None</span>
</span></span><span style="display:flex;"><span>):
</span></span><span style="display:flex;"><span>    m, n = W.shape
</span></span><span style="display:flex;"><span>    A = jnp.zeros((m, n), W.dtype) <span style="color:#00a">if</span> A0 <span style="color:#00a">is</span> <span style="color:#00a">None</span> <span style="color:#00a">else</span> A0
</span></span><span style="display:flex;"><span>    B = jnp.zeros((m, n), W.dtype) <span style="color:#00a">if</span> B0 <span style="color:#00a">is</span> <span style="color:#00a">None</span> <span style="color:#00a">else</span> B0
</span></span><span style="display:flex;"><span>    y = jnp.zeros((m, n), W.dtype) <span style="color:#00a">if</span> y0 <span style="color:#00a">is</span> <span style="color:#00a">None</span> <span style="color:#00a">else</span> y0
</span></span><span style="display:flex;"><span>    A_bar, B_bar = A, B
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a">def</span> <span style="color:#0a0">cond</span>(state):
</span></span><span style="display:flex;"><span>        _, _, _, _, _, k, res, *_= state
</span></span><span style="display:flex;"><span>        <span style="color:#00a">return</span> jnp.logical_and(k &lt; max_iters, res &gt; tol)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a">def</span> <span style="color:#0a0">body</span>(state):
</span></span><span style="display:flex;"><span>        A, B, y, A_bar, B_bar, k, _, tau_A, tau_B, sigma = state
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># Dual ascent</span>
</span></span><span style="display:flex;"><span>        y_new = y + sigma * (A_bar - B_bar)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># Primal descent (A &amp; B updates)</span>
</span></span><span style="display:flex;"><span>        A_new = spectral_hardcap(A - tau_A * y_new)
</span></span><span style="display:flex;"><span>        B_new = project_to_stiefel_tangent_space(W, B + tau_B * y_new - tau_B * G)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># update step-sizes</span>
</span></span><span style="display:flex;"><span>        tau = <span style="color:#099">0.5</span> * (tau_A + tau_B)
</span></span><span style="display:flex;"><span>        theta = <span style="color:#099">1</span> / jnp.sqrt(<span style="color:#099">1</span> + <span style="color:#099">2</span> * gamma * tau)
</span></span><span style="display:flex;"><span>        tau_A = theta * tau_A
</span></span><span style="display:flex;"><span>        tau_B = theta * tau_B
</span></span><span style="display:flex;"><span>        sigma = sigma / theta
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># Extrapolation</span>
</span></span><span style="display:flex;"><span>        A_bar_new = A_new + theta * (A_new - A)
</span></span><span style="display:flex;"><span>        B_bar_new = B_new + theta * (B_new - B)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        res = jnp.linalg.norm(A_new - B_new)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#00a">return</span> (A_new, B_new, y_new, A_bar_new, B_bar_new, k+<span style="color:#099">1</span>, res, tau_A, tau_B, sigma)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    init = (A, B, y, A_bar, B_bar, <span style="color:#099">0</span>, jnp.inf, tau_A, tau_B, sigma)
</span></span><span style="display:flex;"><span>    A, B, y, *_ = jax.lax.while_loop(cond, body, init)
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> -A
</span></span></code></pre></div><h3 id="42-experimental-results">4.2. Experimental results<a hidden class="anchor" aria-hidden="true" href="#42-experimental-results">#</a></h3>
<p><img loading="lazy" src="/ponder/steepest-descent-finsler/pareto-frontier-stiefel.png"></p>
<p>Here I&rsquo;ve plotted the alignment &lt;-&gt; off-tangency frontier for the different methods proposed by myself, Jeremy and Jianlin. The alternating projections method seems to do well despite being provably suboptimal in some cases. But the PDHG method closes the gap as we increase the number of iteration. If we initialize $X_0$ and $Y_0$ from the previous optimization step, we can save compute while potentially improving performance.</p>
<h2 id="5-generalization-to-arbitrary-number-of-constraints-on-the-update">5. Generalization to arbitrary number of constraints on the update<a hidden class="anchor" aria-hidden="true" href="#5-generalization-to-arbitrary-number-of-constraints-on-the-update">#</a></h2>
<p>Our solution above generalizes to arbitrary number of constraints on $A$ so long as the feasible set for each constraint is convex. We then only need to find the metric projection onto each feasible set.</p>
<p>For example, suppose we add another constraint $A \in S$ in Equation (2) above where $S$ is a convex set and $\texttt{proj}_{S}(\cdot)$ is the (metric) projection onto $S$. Then our Equation (5) becomes,
$$\begin{equation} A^* = -\left[\arg\min_{A,B,C \in \mathbb{R}^{m \times n}} \{f(A) + g(B) + h(C)\} \quad \text{ s.t. } \quad A - B = A - C = 0\right]_{A} \end{equation}$$
where,
$$
h(C) := \mathcal{i}_{S}(C) =
\begin{cases}
0 &amp;\text{ if } C \in S \\
\infty &amp;\text{ otherwise}
\end{cases}
$$</p>
<p>We then define,
$$
\begin{align*}
X &amp;:= \begin{bmatrix}
A \\
B \\
C
\end{bmatrix}\\
L &amp;:= \begin{bmatrix}
I &amp; -I &amp;  \\
I &amp;    &amp; -I
\end{bmatrix} \\
\mathcal{F}(X) &amp;:= f(A) + g(B) + h(C) \\
\end{align*}
$$
and the rest then follows and Equation (11) becomes,
$$
\begin{equation}
X_{k+1} = \begin{bmatrix}
\texttt{proj}_{\| \cdot \| \leq 1} (A_k - \tau_A [Y_{k+1}]_1 - \tau_A [Y_{k+1}]_2) \\
\texttt{proj}_{T_W\mathcal{M}} (B_k + \tau_B [Y_{k+1}]_1 - \tau_B G) \\
\texttt{proj}_{S} (C_k + \tau_C [Y_{k+1}]_2)
\end{bmatrix}
\end{equation}
$$</p>
<h2 id="acknowledgements">Acknowledgements<a hidden class="anchor" aria-hidden="true" href="#acknowledgements">#</a></h2>
<p>Big thanks to Jeremy Bernstein and Cédric Simal for productive discussions on the topic!</p>
<h2 id="how-to-cite">How to cite<a hidden class="anchor" aria-hidden="true" href="#how-to-cite">#</a></h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bibtex" data-lang="bibtex"><span style="display:flex;"><span><span style="color:#0a0;text-decoration:underline">@misc</span>{cesista2025steepestdescentfinsler,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">author</span> = <span style="color:#a50">{Franz Louis Cesista}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">title</span> = <span style="color:#a50">{&#34;Steepest Descent on Finsler-Structured (Matrix) Manifolds&#34;}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">year</span> = <span style="color:#a50">{2025}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">url</span> = <span style="color:#a50">{http://leloykun.github.io/ponder/steepest-descent-finsler/}</span>,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><blockquote>
<p>If you find this post useful, please consider supporting my work by sponsoring me on GitHub: <a href="https://github.com/sponsors/leloykun" target="_blank"><img alt="Sponsor on GitHub" loading="lazy" src="https://img.shields.io/badge/%F0%9F%A4%9D-Sponsor%20me-1da1f2?logo=github&style=flat-square"></a></p></blockquote>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ol>
<li>Jeremy Bernstein (2025). Stiefel manifold. URL <a href="https://docs.modula.systems/algorithms/manifold/stiefel/" target="_blank">https://docs.modula.systems/algorithms/manifold/stiefel/</a></li>
<li>Jianlin Su (2025). Muon + Stiefel. URL <a href="https://kexue.fm/archives/11221" target="_blank">https://kexue.fm/archives/11221</a></li>
<li>Laker Newhouse, R. Preston Hess, Franz Cesista, Andrii Zahorodnii, Jeremy Bernstein, Phillip Isola (2025). Training Transformers with Enforced Lipschitz Bounds. URL <a href="https://arxiv.org/abs/2507.13338" target="_blank">https://arxiv.org/abs/2507.13338</a></li>
<li>Jeremy Bernstein &amp; Laker Newhouse (2024). Old optimizer, new norm: an anthology. URL <a href="https://arxiv.org/abs/2409.20325" target="_blank">https://arxiv.org/abs/2409.20325</a></li>
<li>Keller Jordan and Yuchen Jin and Vlado Boza and Jiacheng You and Franz Cesista and Laker Newhouse and Jeremy Bernstein (2024). Muon: An optimizer for hidden layers in neural networks. URL <a href="https://kellerjordan.github.io/posts/muon/" target="_blank">https://kellerjordan.github.io/posts/muon/</a></li>
<li>Greg Yang, James B. Simon, Jeremy Bernstein (2024). A Spectral Condition for Feature Learning. URL <a href="https://arxiv.org/abs/2310.17813" target="_blank">https://arxiv.org/abs/2310.17813</a></li>
<li>ODL (2020). Primal-Dual Hybrid Gradient Algorithm (PDHG). URL <a href="https://odlgroup.github.io/odl/math/solvers/nonsmooth/pdhg.html" target="_blank">https://odlgroup.github.io/odl/math/solvers/nonsmooth/pdhg.html</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://leloykun.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://leloykun.github.io/tags/optimizers/">Optimizers</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Steepest Descent on Finsler-Structured (Matrix) Manifolds on x"
            href="https://x.com/intent/tweet/?text=Steepest%20Descent%20on%20Finsler-Structured%20%28Matrix%29%20Manifolds&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-finsler%2f&amp;hashtags=MachineLearning%2cOptimizers">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Steepest Descent on Finsler-Structured (Matrix) Manifolds on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-finsler%2f&amp;title=Steepest%20Descent%20on%20Finsler-Structured%20%28Matrix%29%20Manifolds&amp;summary=Steepest%20Descent%20on%20Finsler-Structured%20%28Matrix%29%20Manifolds&amp;source=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-finsler%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Steepest Descent on Finsler-Structured (Matrix) Manifolds on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-finsler%2f&title=Steepest%20Descent%20on%20Finsler-Structured%20%28Matrix%29%20Manifolds">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Steepest Descent on Finsler-Structured (Matrix) Manifolds on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-finsler%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Steepest Descent on Finsler-Structured (Matrix) Manifolds on whatsapp"
            href="https://api.whatsapp.com/send?text=Steepest%20Descent%20on%20Finsler-Structured%20%28Matrix%29%20Manifolds%20-%20https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-finsler%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Steepest Descent on Finsler-Structured (Matrix) Manifolds on telegram"
            href="https://telegram.me/share/url?text=Steepest%20Descent%20on%20Finsler-Structured%20%28Matrix%29%20Manifolds&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-finsler%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Steepest Descent on Finsler-Structured (Matrix) Manifolds on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Steepest%20Descent%20on%20Finsler-Structured%20%28Matrix%29%20Manifolds&u=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-finsler%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://leloykun.github.io/">Franz Louis Cesista</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
