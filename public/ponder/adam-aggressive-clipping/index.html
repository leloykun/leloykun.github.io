<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Adam with Agressive Gradient Clipping ≈ Smoothed SignSGD/NormSGD | Franz Louis Cesista</title>
<meta name="keywords" content="Machine Learning, Optimizers">
<meta name="description" content="Why does Adam with aggressive gradient value/norm clipping have sparse updates and do well with higher learning rates? Here we show that it is essentially equivalent to a smoothed version of SignSGD/NormSGD.">
<meta name="author" content="Franz Louis Cesista">
<link rel="canonical" href="https://leloykun.github.io/ponder/adam-aggressive-clipping/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e50e51e1b615f62676c32fb60e15e4b5e3a80ade29f3c6f9c953c528f576fb9f.css" integrity="sha256-5Q5R4bYV9iZ2wy&#43;2DhXkteOoCt4p88b5yVPFKPV2&#43;58=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://leloykun.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://leloykun.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://leloykun.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://leloykun.github.io/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://leloykun.github.io/ponder/adam-aggressive-clipping/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Adam with Agressive Gradient Clipping ≈ Smoothed SignSGD/NormSGD" />
<meta property="og:description" content="Why does Adam with aggressive gradient value/norm clipping have sparse updates and do well with higher learning rates? Here we show that it is essentially equivalent to a smoothed version of SignSGD/NormSGD." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://leloykun.github.io/ponder/adam-aggressive-clipping/" /><meta property="article:section" content="ponder" />
<meta property="article:published_time" content="2025-07-03T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-07-03T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Adam with Agressive Gradient Clipping ≈ Smoothed SignSGD/NormSGD"/>
<meta name="twitter:description" content="Why does Adam with aggressive gradient value/norm clipping have sparse updates and do well with higher learning rates? Here we show that it is essentially equivalent to a smoothed version of SignSGD/NormSGD."/>
<meta name="twitter:site" content="@leloykun"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Ponder",
      "item": "https://leloykun.github.io/ponder/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Adam with Agressive Gradient Clipping ≈ Smoothed SignSGD/NormSGD",
      "item": "https://leloykun.github.io/ponder/adam-aggressive-clipping/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Adam with Agressive Gradient Clipping ≈ Smoothed SignSGD/NormSGD",
  "name": "Adam with Agressive Gradient Clipping ≈ Smoothed SignSGD\/NormSGD",
  "description": "Why does Adam with aggressive gradient value/norm clipping have sparse updates and do well with higher learning rates? Here we show that it is essentially equivalent to a smoothed version of SignSGD/NormSGD.",
  "keywords": [
    "Machine Learning", "Optimizers"
  ],
  "articleBody": "@kalomaze recently shared an interesting observation that Adam with aggressive gradient clipping induces update sparsity while maintaining good performance (and at higher learning rates). Here we will show that Adam with aggressive gradient value/norm clipping is essentially equivalent to a smoothed version of SignSGD/NormSGD. We will also explain why the commulative updates are sparse and why it does well with higher learning rates.\nSmoothed SignSGD and Smoothed NormSGD Smoothed SignSGD and Smoothed NormSGD are special cases of ALMOND (Averaged LMO directioNal Descent) optimizers by Pethick et al. (2025). Unlike Signum which applies momentum first before taking the sign, Smoothed SignSGD applies the sign first before applying momentum.\nDefinition 1 (Smoothed SignSGD). Let $\\eta \u003e 0$ be the learning rate, $\\beta \\in [0, 1)$ be the momentum coefficient, and $G_t$ be the gradient at time step $t$. The update rule for Smoothed SignSGD is given by: $$\\begin{align} M_{t}^{\\text{sssgd}} \u0026= \\beta M_{t-1}^{\\text{sssgd}} + (1 - \\beta) \\text{sign}(G_t) \\\\ W_{t+1} \u0026= W_t - \\eta M_{t}^{\\text{sssgd}}\\nonumber \\end{align}$$ where $M_t$ is the momentum at time step $t$, $W_t$ is the model parameters at time step $t$, and $\\text{sign}(\\cdot)$ is the element-wise sign function.\nNote that unfolding the recurrence in Equation (1) gives us, $$\\begin{equation}M_{t}^{\\text{sssgd}} = (1 - \\beta)\\sum_{k=0}^{t-1}\\beta^k\\text{sign}(G_{t-k})\\end{equation}$$\nLikewise, we can define Smoothed NormSGD as follows:\nDefinition 2 (Smoothed NormSGD). Let $\\eta \u003e 0$ be the learning rate, $\\beta \\in [0, 1)$ be the momentum coefficient, $||\\cdot||$ be a norm chosen a priori, and $G_t$ be the gradient at time step $t$. The update rule for Smoothed NormSGD is given by: $$\\begin{align} M_{t}^{\\text{snsgd}} \u0026= \\beta M_{t-1}^{\\text{snsgd}} + (1 - \\beta) \\frac{G_t}{||G_t||} \\\\ W_{t+1} \u0026= W_t - \\eta M_{t}^{\\text{snsgd}}\\nonumber \\end{align}$$ where $M_t$ is the momentum at time step $t$, $W_t$ is the model parameters at time step $t$.\nAnd again, unfolding the recurrence in Equation (3) gives us, $$\\begin{equation}M_{t}^{\\text{snsgd}} = (1 - \\beta)\\sum_{k=0}^{t-1}\\beta^k\\frac{G_{t-k}}{||G_{t-k}||}\\end{equation}$$\nAdam with aggressive gradient value clipping is equivalent to Smoothed SignSGD Here we apply gradient clipping element-wise with threshold $\\alpha \u003e 0$: $$G_{t,i,j}^{\\text{clipped}} = \\text{clip}_{[-\\alpha, \\alpha]}(G_{t,i,j}) = \\begin{cases} \\alpha\\cdot\\text{sign}(G_{t,i,j}) \u0026 \\text{if } |G_{t,i,j}| \\geq \\alpha \\\\ G_{t,i,j} \u0026 \\text{if } |G_{t,i,j}| \u003c \\alpha \\end{cases} $$ With aggressive gradient value clipping (i.e., $\\alpha \\to 0$), we can make the simplifying assumtion that $|G_{t,i,j}| \\geq \\alpha$ for all $t, i, j$. Thus we have, $$G_{t}^{\\text{clipped}} = \\alpha\\cdot\\text{sign}(G_{t})$$\nPassing this through Adam’s update rule, we get:\n$$\\begin{align*} M_{t}^{\\text{adam}} \u0026= \\beta_1 M_{t-1}^{\\text{adam}} + (1 - \\beta_1)G_{t}^{\\text{clipped}} \\\\ \u0026= \\beta_1 M_{t-1}^{\\text{adam}} + (1 - \\beta_1)\\alpha\\cdot\\text{sign}(G_t) \\\\ \u0026= \\alpha(1 - \\beta_1)\\sum_{k=0}^{t-1}\\beta_1^k\\text{sign}(G_{t-k}) \\\\ M_{t}^{\\text{adam}} \u0026= \\alpha M_{t}^{\\text{sssgd}} \\end{align*}$$\nand,\n$$\\begin{align*} V_{t}^{\\text{adam}} \u0026= \\beta_2 V_{t-1}^{\\text{adam}} + (1 - \\beta_2)(G_{t}^{\\text{clipped}})^2 \\\\ \u0026= \\beta_2 V_{t-1}^{\\text{adam}} + (1 - \\beta_2)(\\alpha\\cdot\\text{sign}(G_t))^2 \\\\ \u0026= \\alpha^2(1 - \\beta_2)\\sum_{k=0}^{t-1}\\beta_2^k\\text{sign}(G_{t-k})^2 \\\\ \u0026= \\alpha^2(1 - \\beta_2)\\sum_{k=0}^{t-1}\\beta_2^k\\mathbb{1}\\qquad\\qquad\\text{from the assumption that } |G_{t-k}| \\geq \\alpha \\\\ V_{t}^{\\text{adam}} \u0026=\\alpha^2 (1 - \\beta_2^t)\\mathbb{1} \\end{align*}$$\nThus the update direction becomes, $$\\begin{align*} U_t \u0026= \\frac{M_t^{\\text{adam}} / (1 - \\beta_1^t)}{\\sqrt{V_t^{\\text{adam}} / (1 - \\beta_2^t)}} \\\\ \u0026= \\frac{\\alpha M_t^{\\text{sssgd}} / (1 - \\beta_1^t)}{\\sqrt{\\alpha^2 (1 - \\beta_2^t)\\mathbb{1} / (1 - \\beta_2^t)}} \\\\ U_t \u0026= \\frac{1}{(1 - \\beta_1^t)} M_t^{\\text{sssgd}} \\end{align*}$$ Note that the $\\alpha$ terms cancel out. And as $t \\to \\infty$, we have $\\beta_1^t \\to 0$. Thus, $$U_t \\to M_t^{\\text{sssgd}}\\qquad\\text{as}\\qquad t \\to \\infty$$\nHence Adam with aggressive gradient value clipping is just Smoothed SignSGD!\nWhy are Smoothed SignSGD updates sparse? Let’s go back to Equation (2): $$M_{t}^{\\text{sssgd}} = (1 - \\beta)\\sum_{k=0}^{t-1}\\beta^k\\text{sign}(G_{t-k})$$ and let’s pick an arbitrary entry $G_{t,i,j}$. Notice that if the signs of the recent $G_{t,i,j}$s flip too much, then the $\\beta^0$, $\\beta^1$, $\\beta^2$, … terms effectively cancel each other out. Thus that entry will not contribute to the update. On the other hand, if the signs of the recent $G_{t,i,j}$s are aligned, then $M_{t,i,j}^{\\text{sssgd}} \\to \\pm 1$. What this means is that for a given entry, the weights only get updated if the signs of the recent gradients are aligned.\nAdam with aggressive gradient norm clipping is essentially equivalent Smoothed NormSGD Unlike the previous case, here we apply the clipping on the norm of the gradient. That is, for a given threshold $\\alpha \u003e 0$, we have: $$G_{t}^{\\text{clipped}} = \\begin{cases} \\frac{\\alpha}{||G_{t}||}G_{t} \u0026 \\text{if } ||G_{t,i,j}|| \\geq \\alpha \\\\ G_{t,i,j} \u0026 \\text{if } ||G_{t,i,j}|| \u003c \\alpha \\end{cases}$$ And with aggressive gradient norm clipping, we can assume that $||G_{t}|| \\geq \\alpha$ for all $t$.\nAnd like before, passing this through Adam’s update rule, we get: $$\\begin{align*} M_{t}^{\\text{adam}} \u0026= \\beta_1 M_{t-1}^{\\text{adam}} + (1 - \\beta_1)G_{t}^{\\text{clipped}} \\\\ \u0026= \\beta_1 M_{t-1}^{\\text{adam}} + (1 - \\beta_1)\\frac{\\alpha}{||G_{t}||}G_{t} \\\\ \u0026= \\alpha(1 - \\beta_1)\\sum_{k=0}^{t-1}\\beta_1^k \\frac{G_{t-k}}{||G_{t}||} \\\\ M_{t}^{\\text{adam}} \u0026= \\alpha M_{t}^{\\text{snsgd}} \\end{align*}$$ and, $$\\begin{align*} V_{t}^{\\text{adam}} \u0026= \\beta_2 V_{t-1}^{\\text{adam}} + (1 - \\beta_2)(G_{t}^{\\text{clipped}})^2 \\\\ \u0026= \\beta_2 V_{t-1}^{\\text{adam}} + (1 - \\beta_2)\\left(\\frac{\\alpha}{||G_{t}||}G_{t}\\right)^2 \\\\ \u0026= \\alpha^2(1 - \\beta_2)\\sum_{k=0}^{t-1}\\beta_2^k\\left(\\frac{G_{t-k}}{||G_{t-k}||}\\right)^2 \\\\ V_{t}^{\\text{adam}} \u0026= \\alpha^2(1 - \\beta_2^t) S_t \\end{align*}$$ where $$S_t = \\frac{1 - \\beta_2}{1 - \\beta_2^t} \\sum_{k=0}^{t-1}\\beta_2^k\\left(\\frac{G_{t-k}}{||G_{t-k}||}\\right)^2$$\nFor an arbitrary index $i,j$, notice that $G_{t-k,i,j} / ||G_{t-k}|| \\leq 1$ for all $k$. The (entrywise) sum in the RHS then is minimized when $G_{t-k,i,j} = 0$ for all $k$. And the sum is maximized when $G_{t-k,i,j} / ||G_{t-k}|| = 1$ for all $k$. Together, we have: $$0 \\leq S_{t,i,j} \\leq \\frac{1 - \\beta_2}{1 - \\beta_2^t} \\sum_{k=0}^{t-1}\\beta_2^k = \\frac{1 - \\beta_2}{1 - \\beta_2^t} \\left( \\frac{1 - \\beta_2^t}{1 - \\beta_2} \\right) = 1$$\nThe (Adam) update direction then becomes: $$\\begin{align*} U_t \u0026= \\frac{M_t^{\\text{adam}} / (1 - \\beta_1^t)}{\\sqrt{V_t^{\\text{adam}} / (1 - \\beta_2^t)}} \\\\ \u0026= \\frac{\\alpha M_t^{\\text{snsgd}} / (1 - \\beta_1^t)}{\\sqrt{\\alpha^2(1 - \\beta_2^t) S_t / (1 - \\beta_2^t)}} \\\\ U_t \u0026= \\frac{1}{(1 - \\beta_1^t)S_t} M_t^{\\text{snsgd}} \\end{align*}$$ And if we make the further assumption that the gradients are isotopic, or more intuitively speaking, the gradients statistically do not ‘change’ over time, then we can treat $S_t$ as a constant. Thus, $$U_t \\to \\text{constant}\\cdot M_t^{\\text{snsgd}}\\qquad\\text{as}\\qquad t \\to \\infty$$ Hence Adam with aggressive gradient norm clipping is essentially just Smoothed NormSGD.\nWhy are Smoothed NormSGD updates sparse? Roughly the same reasoning applies as in the case of Smoothed SignSGD. The main difference is that the weights to the betas are, in a sense, “denser”: $$\\text{sign}(G_{t-k,i,j}) \\in \\{-1, 0, 1\\}\\qquad\\text{ vs. }\\qquad\\frac{G_{t-k,i,j}}{||G_{t-k}||} \\in [-1, 1]$$\nAnd so, we do have to be careful of the case where the gradient norms have a lot of variance. But in practice, the norms of the gradients are usually stable, except for some gradient spikes here and there, so it’s a non-issue. That said, let’s go back to Equation (4): $$M_{t}^{\\text{snsgd}} = (1 - \\beta)\\sum_{k=0}^{t-1}\\beta^k\\frac{G_{t-k}}{||G_{t-k}||}$$ and let’s pick an arbitrary entry $G_{t,i,j}$. Like before, if the signs of the recent $G_{t,i,j}$s flip too much, then the $\\beta^0$, $\\beta^1$, $\\beta^2$, … terms effectively cancel each other out (we use the assumption that the norms are stable here). Otherwise, if they’re aligned, then $M_{t,i,j}^{\\text{snsgd}} \\to \\pm 1$. Thus, as before, the weights only get updated if the recent gradients are aligned.\nWhy do Smoothed SignSGD and Smoothed NormSGD do well with higher learning rates? (Smoothed) SignSGD and NormSGD destroy the magnitude information of the gradients. So picking a higher learning rate is, in a sense, a way to compensate for the lost magnitude information. The momentum also stabilizes the updates, so we can afford to pick a higher learning rate without worrying about overshooting that much.\nHow to Cite @misc{cesista2025adamaggressiveclipping, author = {Franz Louis Cesista}, title = {\"Adam with Agressive Gradient Clipping ≈ Smoothed SignSGD/NormSGD\"}, year = {2025}, url = {http://leloykun.github.io/ponder/adam-aggressive-clipping/}, } References @kalomaze (2025). On Adam with aggressive gradient clipping causing sparse updates. URL https://x.com/kalomaze/status/1940424032119316813 Thomas Pethick, Wanyun Xie, Kimon Antonakopoulos, Zhenyu Zhu, Antonio Silveti-Falls, Volkan Cevher (2025). Training Deep Learning Models with Norm-Constrained LMOs. URL https://arxiv.org/abs/2502.07529 Diederik P. Kingma, Jimmy Ba (2014). Adam: A Method for Stochastic Optimization. URL https://arxiv.org/abs/1412.6980 Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, Anima Anandkumar (2018). signSGD: Compressed Optimisation for Non-Convex Problems. URL https://arxiv.org/abs/1802.04434 ",
  "wordCount" : "1272",
  "inLanguage": "en",
  "datePublished": "2025-07-03T00:00:00Z",
  "dateModified": "2025-07-03T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Franz Louis Cesista"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://leloykun.github.io/ponder/adam-aggressive-clipping/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Franz Louis Cesista",
    "logo": {
      "@type": "ImageObject",
      "url": "https://leloykun.github.io/favicon.ico"
    }
  }
}
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-BWCGBRX8G1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BWCGBRX8G1');
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"
  integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"
  integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js"
  integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
        { left: "\\begin{equation}", right: "\\end{equation}", display: true },
        { left: "\\begin{equation*}", right: "\\end{equation*}", display: true },
        { left: "\\begin{align}", right: "\\end{align}", display: true },
        { left: "\\begin{align*}", right: "\\end{align*}", display: true },
        { left: "\\begin{alignat}", right: "\\end{alignat}", display: true },
        { left: "\\begin{gather}", right: "\\end{gather}", display: true },
        { left: "\\begin{CD}", right: "\\end{CD}", display: true },
      ],
      throwOnError: false,
      trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
      macros: {
        "\\eqref": "\\href{###1}{(\\text{#1})}",
        "\\ref": "\\href{###1}{\\text{#1}}",
        "\\label": "\\htmlId{#1}{}"
      }
    });
  });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://leloykun.github.io/" accesskey="h" title="Franz Louis Cesista">
                <img src="https://leloykun.github.io/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Franz Louis Cesista</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://leloykun.github.io/cv.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/personal-projects/" title="Personal Projects">
                    <span>Personal Projects</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/ponder/" title="Ponder (Posts)">
                    <span>Ponder (Posts)</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Adam with Agressive Gradient Clipping ≈ Smoothed SignSGD/NormSGD
    </h1>
    <div class="post-meta"><span title='2025-07-03 00:00:00 +0000 UTC'>July 3, 2025</span>&nbsp;&middot;&nbsp;Franz Louis Cesista&nbsp;&middot;&nbsp;<a href="https://x.com/leloykun/status/1941067659157913625" rel="noopener noreferrer" target="_blank">Crossposted on X (formerly Twitter)</a>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#smoothed-signsgd-and-smoothed-normsgd">Smoothed SignSGD and Smoothed NormSGD</a></li>
    <li><a href="#adam-with-aggressive-gradient-value-clipping-is-equivalent-to-smoothed-signsgd">Adam with aggressive gradient <em>value</em> clipping is equivalent to Smoothed SignSGD</a>
      <ul>
        <li><a href="#why-are-smoothed-signsgd-updates-sparse">Why are Smoothed SignSGD updates sparse?</a></li>
      </ul>
    </li>
    <li><a href="#adam-with-aggressive-gradient-norm-clipping-is-essentially-equivalent-smoothed-normsgd">Adam with aggressive gradient <em>norm</em> clipping is essentially equivalent Smoothed NormSGD</a>
      <ul>
        <li><a href="#why-are-smoothed-normsgd-updates-sparse">Why are Smoothed NormSGD updates sparse?</a></li>
        <li><a href="#why-do-smoothed-signsgd-and-smoothed-normsgd-do-well-with-higher-learning-rates">Why do Smoothed SignSGD and Smoothed NormSGD do well with higher learning rates?</a></li>
      </ul>
    </li>
    <li><a href="#how-to-cite">How to Cite</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><p><a href="https://x.com/kalomaze/status/1940424032119316813" target="_blank">@kalomaze recently shared an interesting observation</a> that Adam with aggressive gradient clipping induces update sparsity while maintaining good performance (and at higher learning rates). Here we will show that Adam with aggressive gradient value/norm clipping is essentially equivalent to a smoothed version of SignSGD/NormSGD. We will also explain why the commulative updates are sparse and why it does well with higher learning rates.</p>
<h2 id="smoothed-signsgd-and-smoothed-normsgd">Smoothed SignSGD and Smoothed NormSGD<a hidden class="anchor" aria-hidden="true" href="#smoothed-signsgd-and-smoothed-normsgd">#</a></h2>
<p>Smoothed SignSGD and Smoothed NormSGD are special cases of ALMOND (Averaged LMO directioNal Descent) optimizers by Pethick et al. (2025). Unlike Signum which applies momentum first before taking the sign, Smoothed SignSGD applies the sign first before applying momentum.</p>
<blockquote>
<p><strong>Definition 1 (Smoothed SignSGD).</strong> Let $\eta &gt; 0$ be the learning rate, $\beta \in [0, 1)$ be the momentum coefficient, and $G_t$ be the gradient at time step $t$. The update rule for Smoothed SignSGD is given by:
$$\begin{align}
M_{t}^{\text{sssgd}} &amp;= \beta M_{t-1}^{\text{sssgd}} + (1 - \beta) \text{sign}(G_t) \\
W_{t+1} &amp;= W_t - \eta M_{t}^{\text{sssgd}}\nonumber
\end{align}$$
where $M_t$ is the momentum at time step $t$, $W_t$ is the model parameters at time step $t$, and $\text{sign}(\cdot)$ is the element-wise sign function.</p>
</blockquote>
<p>Note that unfolding the recurrence in Equation (1) gives us,
$$\begin{equation}M_{t}^{\text{sssgd}} = (1 - \beta)\sum_{k=0}^{t-1}\beta^k\text{sign}(G_{t-k})\end{equation}$$</p>
<p>Likewise, we can define Smoothed NormSGD as follows:</p>
<blockquote>
<p><strong>Definition 2 (Smoothed NormSGD).</strong> Let $\eta &gt; 0$ be the learning rate, $\beta \in [0, 1)$ be the momentum coefficient, $||\cdot||$ be a norm chosen a priori, and $G_t$ be the gradient at time step $t$. The update rule for Smoothed NormSGD is given by:
$$\begin{align}
M_{t}^{\text{snsgd}} &amp;= \beta M_{t-1}^{\text{snsgd}} + (1 - \beta) \frac{G_t}{||G_t||} \\
W_{t+1} &amp;= W_t - \eta M_{t}^{\text{snsgd}}\nonumber
\end{align}$$
where $M_t$ is the momentum at time step $t$, $W_t$ is the model parameters at time step $t$.</p>
</blockquote>
<p>And again, unfolding the recurrence in Equation (3) gives us,
$$\begin{equation}M_{t}^{\text{snsgd}} = (1 - \beta)\sum_{k=0}^{t-1}\beta^k\frac{G_{t-k}}{||G_{t-k}||}\end{equation}$$</p>
<h2 id="adam-with-aggressive-gradient-value-clipping-is-equivalent-to-smoothed-signsgd">Adam with aggressive gradient <em>value</em> clipping is equivalent to Smoothed SignSGD<a hidden class="anchor" aria-hidden="true" href="#adam-with-aggressive-gradient-value-clipping-is-equivalent-to-smoothed-signsgd">#</a></h2>
<p>Here we apply gradient clipping element-wise with threshold $\alpha &gt; 0$:
$$G_{t,i,j}^{\text{clipped}} = \text{clip}_{[-\alpha, \alpha]}(G_{t,i,j})
= \begin{cases}
\alpha\cdot\text{sign}(G_{t,i,j}) &amp; \text{if } |G_{t,i,j}| \geq \alpha \\
G_{t,i,j} &amp; \text{if } |G_{t,i,j}| &lt; \alpha
\end{cases}
$$
With <em>aggressive gradient value clipping</em> (i.e., $\alpha \to 0$), we can make the simplifying assumtion that $|G_{t,i,j}| \geq \alpha$ for all $t, i, j$. Thus we have,
$$G_{t}^{\text{clipped}} = \alpha\cdot\text{sign}(G_{t})$$</p>
<p>Passing this through Adam&rsquo;s update rule, we get:</p>
<p>$$\begin{align*}
M_{t}^{\text{adam}}
&amp;= \beta_1 M_{t-1}^{\text{adam}} + (1 - \beta_1)G_{t}^{\text{clipped}} \\
&amp;= \beta_1 M_{t-1}^{\text{adam}} + (1 - \beta_1)\alpha\cdot\text{sign}(G_t) \\
&amp;= \alpha(1 - \beta_1)\sum_{k=0}^{t-1}\beta_1^k\text{sign}(G_{t-k}) \\
M_{t}^{\text{adam}} &amp;= \alpha M_{t}^{\text{sssgd}}
\end{align*}$$</p>
<p>and,</p>
<p>$$\begin{align*}
V_{t}^{\text{adam}}
&amp;= \beta_2 V_{t-1}^{\text{adam}} + (1 - \beta_2)(G_{t}^{\text{clipped}})^2 \\
&amp;= \beta_2 V_{t-1}^{\text{adam}} + (1 - \beta_2)(\alpha\cdot\text{sign}(G_t))^2 \\
&amp;= \alpha^2(1 - \beta_2)\sum_{k=0}^{t-1}\beta_2^k\text{sign}(G_{t-k})^2 \\
&amp;= \alpha^2(1 - \beta_2)\sum_{k=0}^{t-1}\beta_2^k\mathbb{1}\qquad\qquad\text{from the assumption that } |G_{t-k}| \geq \alpha \\
V_{t}^{\text{adam}}
&amp;=\alpha^2 (1 - \beta_2^t)\mathbb{1}
\end{align*}$$</p>
<p>Thus the update direction becomes,
$$\begin{align*}
U_t &amp;= \frac{M_t^{\text{adam}} / (1 - \beta_1^t)}{\sqrt{V_t^{\text{adam}} / (1 - \beta_2^t)}} \\
&amp;= \frac{\alpha M_t^{\text{sssgd}} / (1 - \beta_1^t)}{\sqrt{\alpha^2 (1 - \beta_2^t)\mathbb{1} / (1 - \beta_2^t)}} \\
U_t &amp;= \frac{1}{(1 - \beta_1^t)} M_t^{\text{sssgd}}
\end{align*}$$
Note that the $\alpha$ terms cancel out. And as $t \to \infty$, we have $\beta_1^t \to 0$. Thus,
$$U_t \to M_t^{\text{sssgd}}\qquad\text{as}\qquad t \to \infty$$</p>
<p>Hence Adam with aggressive gradient value clipping is just Smoothed SignSGD!</p>
<h3 id="why-are-smoothed-signsgd-updates-sparse">Why are Smoothed SignSGD updates sparse?<a hidden class="anchor" aria-hidden="true" href="#why-are-smoothed-signsgd-updates-sparse">#</a></h3>
<p>Let&rsquo;s go back to Equation (2):
$$M_{t}^{\text{sssgd}} = (1 - \beta)\sum_{k=0}^{t-1}\beta^k\text{sign}(G_{t-k})$$
and let&rsquo;s pick an arbitrary entry $G_{t,i,j}$. Notice that if the signs of the recent $G_{t,i,j}$s flip too much, then the $\beta^0$, $\beta^1$, $\beta^2$, &hellip; terms effectively cancel each other out. Thus that entry will not contribute to the update. On the other hand, if the signs of the recent $G_{t,i,j}$s are aligned, then $M_{t,i,j}^{\text{sssgd}} \to \pm 1$. What this means is that for a given entry, the weights only get updated if the signs of the recent gradients are aligned.</p>
<h2 id="adam-with-aggressive-gradient-norm-clipping-is-essentially-equivalent-smoothed-normsgd">Adam with aggressive gradient <em>norm</em> clipping is essentially equivalent Smoothed NormSGD<a hidden class="anchor" aria-hidden="true" href="#adam-with-aggressive-gradient-norm-clipping-is-essentially-equivalent-smoothed-normsgd">#</a></h2>
<p>Unlike the previous case, here we apply the clipping on the norm of the gradient. That is, for a given threshold $\alpha &gt; 0$, we have:
$$G_{t}^{\text{clipped}} = \begin{cases}
\frac{\alpha}{||G_{t}||}G_{t} &amp; \text{if } ||G_{t,i,j}|| \geq \alpha \\
G_{t,i,j} &amp; \text{if } ||G_{t,i,j}|| &lt; \alpha
\end{cases}$$
And with <em>aggressive gradient norm clipping</em>, we can assume that $||G_{t}|| \geq \alpha$ for all $t$.</p>
<p>And like before, passing this through Adam&rsquo;s update rule, we get:
$$\begin{align*}
M_{t}^{\text{adam}}
&amp;= \beta_1 M_{t-1}^{\text{adam}} + (1 - \beta_1)G_{t}^{\text{clipped}} \\
&amp;= \beta_1 M_{t-1}^{\text{adam}} + (1 - \beta_1)\frac{\alpha}{||G_{t}||}G_{t} \\
&amp;= \alpha(1 - \beta_1)\sum_{k=0}^{t-1}\beta_1^k \frac{G_{t-k}}{||G_{t}||} \\
M_{t}^{\text{adam}} &amp;= \alpha M_{t}^{\text{snsgd}}
\end{align*}$$
and,
$$\begin{align*}
V_{t}^{\text{adam}}
&amp;= \beta_2 V_{t-1}^{\text{adam}} + (1 - \beta_2)(G_{t}^{\text{clipped}})^2 \\
&amp;= \beta_2 V_{t-1}^{\text{adam}} + (1 - \beta_2)\left(\frac{\alpha}{||G_{t}||}G_{t}\right)^2 \\
&amp;= \alpha^2(1 - \beta_2)\sum_{k=0}^{t-1}\beta_2^k\left(\frac{G_{t-k}}{||G_{t-k}||}\right)^2 \\
V_{t}^{\text{adam}}
&amp;= \alpha^2(1 - \beta_2^t) S_t
\end{align*}$$
where
$$S_t = \frac{1 - \beta_2}{1 - \beta_2^t} \sum_{k=0}^{t-1}\beta_2^k\left(\frac{G_{t-k}}{||G_{t-k}||}\right)^2$$</p>
<p>For an arbitrary index $i,j$, notice that $G_{t-k,i,j} / ||G_{t-k}|| \leq 1$ for all $k$. The (entrywise) sum in the RHS then is minimized when $G_{t-k,i,j} = 0$ for all $k$. And the sum is maximized when $G_{t-k,i,j} / ||G_{t-k}|| = 1$ for all $k$. Together, we have:
$$0 \leq S_{t,i,j} \leq \frac{1 - \beta_2}{1 - \beta_2^t} \sum_{k=0}^{t-1}\beta_2^k = \frac{1 - \beta_2}{1 - \beta_2^t} \left( \frac{1 - \beta_2^t}{1 - \beta_2} \right) = 1$$</p>
<p>The (Adam) update direction then becomes:
$$\begin{align*}
U_t &amp;= \frac{M_t^{\text{adam}} / (1 - \beta_1^t)}{\sqrt{V_t^{\text{adam}} / (1 - \beta_2^t)}} \\
&amp;= \frac{\alpha M_t^{\text{snsgd}} / (1 - \beta_1^t)}{\sqrt{\alpha^2(1 - \beta_2^t) S_t / (1 - \beta_2^t)}} \\
U_t &amp;= \frac{1}{(1 - \beta_1^t)S_t} M_t^{\text{snsgd}}
\end{align*}$$
And if we make the further assumption that the gradients are isotopic, or more intuitively speaking, the gradients statistically do not &lsquo;change&rsquo; over time, then we can treat $S_t$ as a constant. Thus,
$$U_t \to \text{constant}\cdot M_t^{\text{snsgd}}\qquad\text{as}\qquad t \to \infty$$
Hence Adam with aggressive gradient norm clipping is essentially just Smoothed NormSGD.</p>
<h3 id="why-are-smoothed-normsgd-updates-sparse">Why are Smoothed NormSGD updates sparse?<a hidden class="anchor" aria-hidden="true" href="#why-are-smoothed-normsgd-updates-sparse">#</a></h3>
<p>Roughly the same reasoning applies as in the case of Smoothed SignSGD. The main difference is that the weights to the betas are, in a sense, &ldquo;denser&rdquo;:
$$\text{sign}(G_{t-k,i,j}) \in \{-1, 0, 1\}\qquad\text{ vs. }\qquad\frac{G_{t-k,i,j}}{||G_{t-k}||} \in [-1, 1]$$</p>
<p>And so, we do have to be careful of the case where the gradient norms have a lot of variance. But in practice, the norms of the gradients are usually stable, except for some gradient spikes here and there, so it&rsquo;s a non-issue. That said, let&rsquo;s go back to Equation (4):
$$M_{t}^{\text{snsgd}} = (1 - \beta)\sum_{k=0}^{t-1}\beta^k\frac{G_{t-k}}{||G_{t-k}||}$$
and let&rsquo;s pick an arbitrary entry $G_{t,i,j}$. Like before, if the signs of the recent $G_{t,i,j}$s flip too much, then the $\beta^0$, $\beta^1$, $\beta^2$, &hellip; terms effectively cancel each other out (we use the assumption that the norms are stable here). Otherwise, if they&rsquo;re aligned, then $M_{t,i,j}^{\text{snsgd}} \to \pm 1$. Thus, as before, the weights only get updated if the recent gradients are aligned.</p>
<h3 id="why-do-smoothed-signsgd-and-smoothed-normsgd-do-well-with-higher-learning-rates">Why do Smoothed SignSGD and Smoothed NormSGD do well with higher learning rates?<a hidden class="anchor" aria-hidden="true" href="#why-do-smoothed-signsgd-and-smoothed-normsgd-do-well-with-higher-learning-rates">#</a></h3>
<p>(Smoothed) SignSGD and NormSGD destroy the magnitude information of the gradients. So picking a higher learning rate is, in a sense, a way to compensate for the lost magnitude information. The momentum also stabilizes the updates, so we can afford to pick a higher learning rate without worrying about overshooting that much.</p>
<h2 id="how-to-cite">How to Cite<a hidden class="anchor" aria-hidden="true" href="#how-to-cite">#</a></h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bibtex" data-lang="bibtex"><span style="display:flex;"><span><span style="color:#0a0;text-decoration:underline">@misc</span>{cesista2025adamaggressiveclipping,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">author</span> = <span style="color:#a50">{Franz Louis Cesista}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">title</span> = <span style="color:#a50">{&#34;Adam with Agressive Gradient Clipping ≈ Smoothed SignSGD/NormSGD&#34;}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">year</span> = <span style="color:#a50">{2025}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">url</span> = <span style="color:#a50">{http://leloykun.github.io/ponder/adam-aggressive-clipping/}</span>,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ol>
<li>@kalomaze (2025). On Adam with aggressive gradient clipping causing sparse updates. URL <a href="https://x.com/kalomaze/status/1940424032119316813" target="_blank">https://x.com/kalomaze/status/1940424032119316813</a></li>
<li>Thomas Pethick, Wanyun Xie, Kimon Antonakopoulos, Zhenyu Zhu, Antonio Silveti-Falls, Volkan Cevher (2025). Training Deep Learning Models with Norm-Constrained LMOs. URL <a href="https://arxiv.org/abs/2502.07529" target="_blank">https://arxiv.org/abs/2502.07529</a></li>
<li>Diederik P. Kingma, Jimmy Ba (2014). Adam: A Method for Stochastic Optimization. URL <a href="https://arxiv.org/abs/1412.6980" target="_blank">https://arxiv.org/abs/1412.6980</a></li>
<li>Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, Anima Anandkumar (2018). signSGD: Compressed Optimisation for Non-Convex Problems. URL <a href="https://arxiv.org/abs/1802.04434" target="_blank">https://arxiv.org/abs/1802.04434</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://leloykun.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://leloykun.github.io/tags/optimizers/">Optimizers</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Adam with Agressive Gradient Clipping ≈ Smoothed SignSGD/NormSGD on x"
            href="https://x.com/intent/tweet/?text=Adam%20with%20Agressive%20Gradient%20Clipping%20%e2%89%88%20Smoothed%20SignSGD%2fNormSGD&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fadam-aggressive-clipping%2f&amp;hashtags=MachineLearning%2cOptimizers">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Adam with Agressive Gradient Clipping ≈ Smoothed SignSGD/NormSGD on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fadam-aggressive-clipping%2f&amp;title=Adam%20with%20Agressive%20Gradient%20Clipping%20%e2%89%88%20Smoothed%20SignSGD%2fNormSGD&amp;summary=Adam%20with%20Agressive%20Gradient%20Clipping%20%e2%89%88%20Smoothed%20SignSGD%2fNormSGD&amp;source=https%3a%2f%2fleloykun.github.io%2fponder%2fadam-aggressive-clipping%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Adam with Agressive Gradient Clipping ≈ Smoothed SignSGD/NormSGD on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fleloykun.github.io%2fponder%2fadam-aggressive-clipping%2f&title=Adam%20with%20Agressive%20Gradient%20Clipping%20%e2%89%88%20Smoothed%20SignSGD%2fNormSGD">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Adam with Agressive Gradient Clipping ≈ Smoothed SignSGD/NormSGD on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fleloykun.github.io%2fponder%2fadam-aggressive-clipping%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Adam with Agressive Gradient Clipping ≈ Smoothed SignSGD/NormSGD on whatsapp"
            href="https://api.whatsapp.com/send?text=Adam%20with%20Agressive%20Gradient%20Clipping%20%e2%89%88%20Smoothed%20SignSGD%2fNormSGD%20-%20https%3a%2f%2fleloykun.github.io%2fponder%2fadam-aggressive-clipping%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Adam with Agressive Gradient Clipping ≈ Smoothed SignSGD/NormSGD on telegram"
            href="https://telegram.me/share/url?text=Adam%20with%20Agressive%20Gradient%20Clipping%20%e2%89%88%20Smoothed%20SignSGD%2fNormSGD&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fadam-aggressive-clipping%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Adam with Agressive Gradient Clipping ≈ Smoothed SignSGD/NormSGD on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Adam%20with%20Agressive%20Gradient%20Clipping%20%e2%89%88%20Smoothed%20SignSGD%2fNormSGD&u=https%3a%2f%2fleloykun.github.io%2fponder%2fadam-aggressive-clipping%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://leloykun.github.io/">Franz Louis Cesista</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
