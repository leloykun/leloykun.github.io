<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Heuristic Solutions for Steepest Descent on the Stiefel Manifold | Franz Louis Cesista</title>
<meta name="keywords" content="Machine Learning, Optimizers">
<meta name="description" content="What would Muon look like if we constrained the weights to be semi-orthogonal?">
<meta name="author" content="Franz Louis Cesista">
<link rel="canonical" href="https://leloykun.github.io/ponder/steepest-descent-stiefel/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e50e51e1b615f62676c32fb60e15e4b5e3a80ade29f3c6f9c953c528f576fb9f.css" integrity="sha256-5Q5R4bYV9iZ2wy&#43;2DhXkteOoCt4p88b5yVPFKPV2&#43;58=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://leloykun.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://leloykun.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://leloykun.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://leloykun.github.io/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://leloykun.github.io/ponder/steepest-descent-stiefel/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Heuristic Solutions for Steepest Descent on the Stiefel Manifold" />
<meta property="og:description" content="What would Muon look like if we constrained the weights to be semi-orthogonal?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://leloykun.github.io/ponder/steepest-descent-stiefel/" />
<meta property="og:image" content="https://leloykun.github.io/ponder/steepest-descent-stiefel/steepest-descent-stiefel-manifold.jpg" /><meta property="article:section" content="ponder" />
<meta property="article:published_time" content="2025-07-18T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-07-18T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://leloykun.github.io/ponder/steepest-descent-stiefel/steepest-descent-stiefel-manifold.jpg" />
<meta name="twitter:title" content="Heuristic Solutions for Steepest Descent on the Stiefel Manifold"/>
<meta name="twitter:description" content="What would Muon look like if we constrained the weights to be semi-orthogonal?"/>
<meta name="twitter:site" content="@leloykun"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Ponder",
      "item": "https://leloykun.github.io/ponder/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Heuristic Solutions for Steepest Descent on the Stiefel Manifold",
      "item": "https://leloykun.github.io/ponder/steepest-descent-stiefel/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Heuristic Solutions for Steepest Descent on the Stiefel Manifold",
  "name": "Heuristic Solutions for Steepest Descent on the Stiefel Manifold",
  "description": "What would Muon look like if we constrained the weights to be semi-orthogonal?",
  "keywords": [
    "Machine Learning", "Optimizers"
  ],
  "articleBody": " If you find this post useful, please consider supporting my work by sponsoring me on GitHub: 1. Recap: Muon as RMS-to-RMS norm-constrained steepest descent Consider a weight matrix $W \\in \\mathbb{R}^{m \\times n}$ and a “raw gradient” or differential $G \\in \\mathbb{R}^{m \\times n}$ we get via e.g., backpropagation. In standard gradient descent, we would update the weights as follows, $$W \\leftarrow W - \\eta G,$$ where $\\eta \\in (0, \\infty)$ is the learning rate. However, this is suboptimal because (1) the update sizes $\\| G \\|$ could vary wildly across steps thereby causing training instability, and (2) as we discussed in a previous blog post on (non-)Riemannian steepest descent, it does not take into account the matrix structure of the weights. In particular, it ignores how activations or “features” evolve through the network and how the model behaves as we scale it up. Tl;dr:\nIf we want the Euclidean norm $\\| \\cdot \\|_2$ of our features and feature updates to ‘grow’ with the model size, then the Spectral norm $\\| \\cdot \\|_{2 \\to 2}$ of our weights and weight updates must also ‘grow’ with the model size.\nEquivalently, following Yang et al. (2024), we can use the “natural” feature norm, the RMS norm $\\|\\cdot\\|_{RMS} = \\frac{1}{\\sqrt{n}}\\|\\cdot\\|_{2}$, and the “natural” weight norm, the RMS-to-RMS norm $\\|\\cdot\\|_{RMS \\to RMS} = \\frac{\\sqrt{n}}{\\sqrt{m}}\\| \\cdot \\|_{2 \\to 2}$, and rephrase the above as,\nIf we want the “natural” norm of our features and feature updates to be stable regardless of the model size, then the “natural” norm of our weights and weight updates must also be stable regardless of the model size.\nWe will discuss weight norm controls in the next section, but for now, instead of using the raw gradient $G$, we can instead try to find a descent direction $A \\in \\mathbb{R}^{m \\times n}$ that is maximally aligned to $G$ while satisfying our weight update condition above, $$\\| A \\|_{RMS \\to RMS} = \\frac{\\sqrt{n}}{\\sqrt{m}}\\| A \\|_{2 \\to 2} = \\text{constant}.$$ Thus our update rule becomes, $$W \\leftarrow W - \\eta \\frac{\\sqrt{m}}{\\sqrt{n}} A^*,$$ where $$\\begin{equation} A^* = \\arg\\max_{A\\in \\mathbb{R}^{m \\times n}:\\| A \\|_{2 \\to 2} = 1} \\langle G, A \\rangle, \\end{equation}$$ and $\\langle \\cdot, \\cdot \\rangle$ is the Frobenius inner product which measures the “alignment” between two matrices. From Bernstein \u0026 Newhouse (2024), this has a closed-form solution, $$A^* = \\texttt{msign}(G),$$ where $\\texttt{msign}(\\cdot)$ is the matrix sign function. And finally, adding a momentum term then yields the Muon optimizer (Jordan et al., 2024), $$\\begin{align*} M_t \u0026= \\beta M_{t-1} + (1 - \\beta) G \\\\ W_t \u0026= W_{t-1} - \\eta\\frac{\\sqrt{m}}{\\sqrt{n}} \\texttt{msign}(M_t), \\\\ \\end{align*}$$ for some momentum hyperparameter $\\beta \\in [0, 1)$.\nIf you want to learn more about Muon and the ideas behind it, check out Newhouse’s 3-part blog series. I highly recommend it!\n1.1. Recap: (non-)Riemannian steepest descent An update step in first-order optimization on a manifold $\\mathcal{M}$ goes as follows,\nCompute an ‘optimal’ descent direction $A^*$ in the tangent space at the current point $W_t \\in \\mathcal{M}$, $A^* \\in T_{W_t} \\mathcal{M}$. Use this to ‘move’ our weight $\\widetilde{W}_{t+1} \\leftarrow W_t - \\eta A^*$, where $\\eta$ is the learning rate. Note that $\\widetilde{W}_{t+1}$ may not be on the manifold $\\mathcal{M}$. And so, Retract the result back to the manifold via a retraction map $W_{t+1} \\leftarrow \\texttt{retract}(\\widetilde{W}_{t+1})$. We then repeat this process until convergence or until we find a satisfactory solution.\nAn important detail discussed by Large et al. (2024) and in the previous blog post on (non-)Riemannian optimization is that the so-called “raw gradient” $G$ we get via backpropagation is not actually in the tangent space, but rather in the cotangent space at $W$, $G \\in T_W^* \\mathcal{M}$, or the space of linear functionals acting on the tangent vectors. $G$ then is useless by itself. To make it useful, we need to map it to the tangent space first via a dualizer map, $\\texttt{dualizer}: T_W^* \\mathcal{M} \\mapsto T_W \\mathcal{M},$ $$A^* = \\texttt{dualizer}(G) = \\arg\\max_{A \\in T_W \\mathcal{M}} \\langle G, A \\rangle,$$ where the $\\langle \\cdot, \\cdot \\rangle$ operation is the canonical pairing between tangent and cotangent spaces. It is technically not an inner product, but behaves like the Frobenius inner product.\nIn Euclidean space, we got lucky: $T_W \\mathbb{R}^{m \\times n} = T_W^* \\mathbb{R}^{m \\times n} = \\mathbb{R}^{m \\times n}$, and thus, $A^* = G$, yielding the update rule for (stochastic) gradient descent (SGD). In Riemannian manifolds, which we get by e.g. equipping the tangent spaces with a Riemannian metric (or a norm that induces such a metric), the two spaces are no longer equivalent, but they are congruent. This means that for every $G \\in T_W^* \\mathcal{M}$, there exists a unique steepest descent direction $A^* \\in T_W \\mathcal{M}$ we can follow to minimize the loss and vice versa. In non-Riemannian manifolds, however, the optimal $A^*$ may no longer be unique or may not even exist.\nMuon then is what we get when we equip the tangent spaces of $\\mathcal{M} = \\mathbb{R}^{m \\times n}$ with the RMS-to-RMS norm, $\\| \\cdot \\|_{RMS \\to RMS}$. So while the underlying space is still Euclidean, the change in how we measure ‘distances’ makes the new manifold non-Euclidean and even non-Riemannian. In the next sections, we discuss how to build Muon-like optimizers for more exotic manifolds and demonstrate how a smart choice of manifolds to ‘place’ our weights in can accelerate generalization.\n2. Spectral norm-constrained steepest descent on the Stiefel manifold As discussed in the previous section, we not only need to control the weight update norms but also the weight norms themselves. There are multiple ways to do this and we presented some novel methods in our recent work on training transformers with enforced Lipschitz bounds (Newhouse*, Hess*, Cesista* et al., 2025). However, here we will focus on constraining the weights to be semi-orthogonal, i.e., $$\\begin{equation} W^T W = I_n. \\end{equation}$$\nSemi-orthogonal matrices lie on the Stiefel manifold, $\\text{St}(m, n) = \\{W \\in \\mathbb{R}^{m \\times n} | W^T W = I_n \\}$. Differentiating Equation (2) on both sides then yields the constraint that determines membership in the tangent space at $W \\in \\text{St}(m, n)$, $$T_W \\text{St}(m, n) = \\{A \\in \\mathbb{R}^{m \\times n} | W^T A + A^T W = 0\\}.$$ But a crucial difference from prior work on optimization on the Stiefel manifold (Ablin \u0026 Peyré, 2021; Gao et al., 2022) is that we equip the tangent spaces with the spectral norm, $\\| \\cdot \\|_{2 \\to 2}$, augmenting the Stiefel manifold with a Finsler structure. With this, our dualizer becomes, $$A^* = \\arg\\max_{A\\in \\mathbb{R}^{m \\times n}: \\| A \\|_{2 \\to 2} = 1} \\langle G, A \\rangle \\quad \\text{s.t. } A \\in T_W\\text{St}(m, n),$$ and using $\\text{msign}(\\cdot)$ as the retraction map, our update rule becomes, $$\\begin{equation} W \\leftarrow \\text{msign}\\left(W - \\eta A^* \\right) \\end{equation}$$\nEquivalently, $$A^* = \\arg\\max_{A\\in \\mathbb{R}^{m \\times n}} \\langle G, A \\rangle \\quad \\text{s.t. } A \\in \\text{St}(m, n) \\cap T_W \\text{St}(m, n),$$ Or in words, we want to find a descent direction $A$ that is both on the Stiefel manifold and in the tangent space at the current point $W \\in \\text{St}(m, n)$ that maximizes the “alignment” with the raw gradient $G$.\n2.1. RMS-to-RMS norm-constrained steepest descent on the (scaled) Stiefel manifold Following the natural norm conditions we discussed in the previous section, we may want to constrain our weights to be semi-orthogonal with respect to the RMS-to-RMS norm, i.e., $$W^T W = \\frac{m}{n}I_n.$$ This places our weights on the scaled Stiefel manifold, $\\widetilde{\\text{St}}(m, n) = \\{W \\in \\mathbb{R}^{m \\times n} | W^T W = s^2 I_n \\}$ with scale $s = \\sqrt{m}/\\sqrt{n}$. We can use the same dualizer map as for the unscaled Stiefel manifold, but our update rule becomes, $$\\begin{equation} W \\leftarrow \\frac{\\sqrt{m}}{\\sqrt{n}} \\text{msign}\\left(W - \\eta \\frac{\\sqrt{m}}{\\sqrt{n}} A^* \\right) \\end{equation}$$\n2.2. Retraction via rescaling Recall that $\\texttt{msign}(X) = X (X^T X)^{-1/2}$ and note that,\n$$\\begin{align*} (W - \\eta A^*)^T (W - \\eta A^*) \u0026= \\underbrace{W^T W}_{=I_n} - \\eta \\underbrace{((A^*)^T W + W^T A^*)}_{=0} + \\eta^2 \\underbrace{(A^*)^T A^*}_{=I_n} \\\\ \u0026= (1 + \\eta^2) I_n \\end{align*}$$\nThus we can rewrite the update rule for steepest descent on the unscaled Stiefel manifold as, $$W \\leftarrow \\frac{W - \\eta A^*}{\\sqrt{1 + \\eta^2}}$$ and for the scaled Stiefel manifold as, $$W \\leftarrow \\frac{W - \\eta \\frac{\\sqrt{m}}{\\sqrt{n}} A^*}{\\sqrt{1 + \\eta^2}}$$\n3. Equivalence between Bernstein’s and Su’s solutions Bernstein (2025a) and Su (2025) found the following solutions to the square and full-rank case,\n$$\\begin{align*} A^*_{\\text{bernstein}} \u0026= W \\texttt{msign}(\\texttt{skew}(W^TG))\\\\ A^*_{\\text{su}} \u0026= \\texttt{msign}(G - W\\texttt{sym}(W^T G)) \\end{align*}$$ where $\\texttt{sym}(X) = \\frac{1}{2}(X + X^T)$ and $\\texttt{skew}(X) = \\frac{1}{2}(X - X^T)$.\nWe will show that these are equivalent, i.e., $A^*_{\\text{bernstein}} = A^*_{\\text{su}}$. For this, we will reuse the following proposition we discussed in a previous post on spectral clipping.\nProposition 1 (Transpose Equivariance and Unitary Multiplication Equivariance of Odd Matrix Functions). Let $W \\in \\mathbb{R}^{m \\times n}$ and $W = U \\Sigma V^T$ be its reduced SVD. And let $f: \\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{m \\times n}$ be an odd analytic matrix function that acts on the singular values of $W$ as follows, $$f(W) = U f(\\Sigma) V^T.$$ Then $f$ is equivariant under transposition and unitary multiplication, i.e., $$\\begin{align*} f(W^T) \u0026= f(W)^T \\\\ f(WQ^T) \u0026= f(W)Q^T \\quad\\forall Q \\in \\mathbb{R}^{m \\times n} \\text{ such that } Q^TQ = I_n \\\\ f(Q^TW) \u0026= Q^Tf(W) \\quad\\forall Q \\in \\mathbb{R}^{m \\times n} \\text{ such that } QQ^T = I_m \\end{align*}$$\nThus,\n$$\\begin{align*} A^*_{\\text{bernstein}} \u0026= W \\texttt{msign}(\\texttt{skew}(W^TG)) \\\\ \u0026= \\texttt{msign}(W \\texttt{skew}(W^TG)) \u0026\\text{(from Proposition 1)} \\\\ \u0026= \\texttt{msign}\\left(\\frac{1}{2}W W^T G - \\frac{1}{2}W G^T W \\right) \\\\ \u0026= \\texttt{msign}\\left(W W^T G - \\frac{1}{2}W W^T G - \\frac{1}{2}W G^T W \\right) \\\\ \u0026= \\texttt{msign}\\left(G - W\\texttt{sym}(W^T G) \\right) \\\\ A^*_{\\text{bernstein}} \u0026= A^*_{\\text{su}} \\end{align*}$$ where the second-to-last equality relies on $W$ being square and full-rank, which then implies that $W W^T = I_m$.\n4. Projection-projection perspective One can interpret Bernstein’s and Su’s solutions as a two-step projection process:\n(Orthogonal) projection to the tangent space at $W$; and Projection to the closest semi-orthogonal matrix (i.e., closest point on the Stiefel manifold). This is because the map $G \\to G - W \\texttt{sym}(W^T G)$ is actually the orthogonal projection onto the tangent space at $W$ and that $\\texttt{msign}$ projects the resulting matrix to the stiefel manifold. We present these more rigorously as follows.\nTheorem 2 (Orthogonal projection to the tangent space at $W$). Let $W \\in \\text{St}(m, n)$ be a point on the Stiefel manifold. The projection of a vector $V \\in \\mathbb{R}^{m \\times n}$ onto the tangent space at $W$, denoted as $T_W\\text{St}(m, n)$ , is given by, $$\\begin{equation} \\texttt{proj}_{T_W\\text{St}(m, n)}(V) = V - W \\text{sym}(W^T V) \\end{equation}$$\nShow proof of Theorem 2 Proof. First, we need to show that the normal space at $W$, $N_W\\text{St}(m, n)$ is given by, $$N_W\\text{St}(m, n) = \\{WS | S = S^T\\}$$ for symmetric $S$. To show this, let $A \\in T_W\\text{St}(m, n)$ be an arbitrary tangent vector at $W$. Then we have, $$\\begin{align*} \\langle A, WS \\rangle \u0026= \\text{tr}(A^T WS) \\\\ \u0026= \\text{tr}(S W^T A) \u0026\\text{(transpose invariance of trace)} \\\\ \u0026= -\\text{tr}(S A^T W) \u0026\\text{(since $A \\in T_W\\text{St}(m, n)$)} \\\\ \u0026= -\\text{tr}(A^T W S) \u0026\\text{(cyclic property of trace)} \\\\ \\langle A, WS \\rangle \u0026= -\\langle A, WS \\rangle \\end{align*}$$ Thus $\\langle A, WS \\rangle = 0$ which implies that $A$ and $WS$ are orthogonal. Hence $WS \\in N_W\\text{St}(m, n)$.\nNow, for any $V \\in \\mathbb{R}^{m \\times n}$, we can write it as, $V = \\underbrace{V - WS}_{\\text{candidate tangent}} + \\underbrace{WS}_{\\text{candidate normal}}$ for some symmetric $S$. To find $S$, $$\\begin{align*} W^T (V - WS) + (V - WS)^T W \u0026= 0 \\\\ W^T V - S + V^TW - S \u0026= 0 \\\\ S \u0026= \\text{sym}(W^T V) \\end{align*}$$ Thus, $V - W \\text{sym}(W^T V) \\in T_W\\text{St}(m, n)$. And because of that, $W^T (V - W \\text{sym}(W^T V))$ must be skew-symmetric. Thus, $$\\begin{equation*} \\langle V - WS, WS \\rangle = \\langle \\underbrace{W^T (V - WS)}_{\\text{skew-symmetric}}, \\underbrace{S}_{\\text{symmetric}} \\rangle = 0 \\end{equation*}$$ Hence, $V - W \\text{sym}(W^T V)$ is the orthogonal projection of $V$ onto the tangent space at $W$.\nAnd from Proposition 4 of Bernstein \u0026 Newhouse (2024),\nProposition 3 (Projection to the closest semi-orthogonal matrix). Consider the orthogonal matrices $\\mathcal{O}_{m \\times n} := \\{ A \\in \\mathbb{R}^{m \\times n} : A A^T = I_m or A^T A = I_n \\}$ and let $\\| \\cdot \\|_F$ denote the Frobenius norm. For any matrix $G \\in R^{m \\times n}$ with reduced SVD $G = U \\Sigma V^T$: $$\\arg\\min_{A \\in \\mathcal{O}_{m \\times n}} \\| A - G \\|_F = \\texttt{msign}(G) = UV^T,$$ where the minimizer $UV^T$ is unique if and only if the matrix $G$ has full rank.\nThus we can write, $$A^*_{\\text{bernstein}} = A^*_{\\text{su}} = (\\texttt{proj}_{\\text{St}(m, n)} \\circ \\texttt{proj}_{T_W\\text{St}(m, n)})(G)$$ for the square and full-rank case at least.\n4.1. Why Bernstein’s \u0026 Su’s solutions only work for the square and full-rank case Note that the projections above aim to guarantee both of our criteria for the solution, but one step at a time. And that the $\\texttt{msign}$ after the projection may send the resulting matrix outside the tangent space at $W$. We show that this is not a problem in the square and full-rank case, but it is in the general case.\noperation on the tangent space at $W$? have unit spectral norm? (input $G$) not in general not in general 1st projection ($\\texttt{proj}_{T_W St(m, n)}(\\cdot)$) ${\\color{green}\\text{yes}}$ not necessarily 2nd projection ($\\texttt{msign}(\\cdot$)) only for the square\nand full-rank case ${\\color{green}\\text{yes}}$ To demonstrate this, we will need the following proposition.\nProposition 4 ($\\texttt{msign}$ preserves skew-symmetry). Let $X \\in \\R^{m \\times n}$ be a skew-symmetric matrix. Then $\\texttt{msign}(X)$ is also skew-symmetric.\nThe proof follows directly from the transpose equivariance and oddness of $\\texttt{msign}$, i.e., $\\texttt{msign}(X) = \\texttt{msign}(-X^T) = -\\texttt{msign}(X)^T$.\nAlso note that for the general case, $$\\begin{equation} WW^T + QQ^T = I_m \\quad\\text{and}\\quad W^T Q = 0 \\end{equation}$$ where $Q$ is the orthonormal complement of $W$.\nThus, $$\\begin{align*} (\\texttt{proj}_{\\text{St}(m, n)} \\circ \\texttt{proj}_{T_W St(m, n)})(G) \u0026= \\texttt{msign}(G - \\frac{1}{2}W W^T G - \\frac{1}{2} W G^T W) \\\\ \u0026= \\texttt{msign}((WW^T + QQ^T)G - \\frac{1}{2}W W^T G - \\frac{1}{2} W G^T W) \\\\ \u0026= \\texttt{msign}(W \\texttt{skew}(W^T G) + QQ^T G) \\end{align*}$$ For the square and full-rank case, we have $Q = 0$ and $W W^T = I$. And the above simplifies to Berstein’s solution, $$(\\texttt{proj}_{\\text{St}(m, n)} \\circ \\texttt{proj}_{T_W St(m, n)})(G) = W \\texttt{msign}(\\texttt{skew}(W^T G))$$ and since $\\texttt{skew}(W^T G)$ is skew-symmetric, then $\\texttt{msign}(\\texttt{skew}(W^T G))$ must be too. And thus, $$\\begin{align*} \u0026W^T (\\texttt{proj}_{\\text{St}(m, n)} \\circ \\texttt{proj}_{T_W St(m, n)})(G) + (\\texttt{proj}_{\\text{St}(m, n)} \\circ \\texttt{proj}_{T_W St(m, n)})(G)^T W \\\\ \u0026\\qquad= W^T W \\texttt{msign}(\\texttt{skew}(W^T G)) + (W\\texttt{msign}(\\texttt{skew}(W^T G)))^T W \\\\ \u0026\\qquad= \\texttt{msign}(\\texttt{skew}(W^T G)) + \\texttt{msign}(\\texttt{skew}(W^T G))^T \\\\ \u0026\\qquad= 0 \\end{align*}$$ Hence for the square and full-rank case, this two-step projection process guarantees that the resulting matrix has unit spectral norm and is in the tangent space at $W$.\nFor the more general case, we may no longer have $Q = 0$ or $W W^T = I$. Thus, we cannot guarantee that $W \\texttt{skew}(W^T G) + QQ^T G$ is skew-symmetric. And so the $\\texttt{msign}$ after the first projection may send the resulting matrix outside the tangent space at $W$.\n5. Heuristic solutions for the general case 5.1. Alignment upper bound As established previously, for any $G \\in \\mathbb{R}^{m \\times n}$, the solution to $\\arg\\max_{A \\in \\text{St}(m, n)} \\langle G, A \\rangle$ is $A^* = \\texttt{proj}_{\\text{St}(m, n)}(G) = \\texttt{msign}(G)$ and thus $\\max_{A \\in \\text{St}(m, n)} \\langle G, A \\rangle = \\| G \\|_{\\text{nuc}}$ where $\\| \\cdot \\|_{\\text{nuc}}$ is the nuclear norm. With an extra linear constraint $A \\in T_W \\text{St}(m, n)$, we have,\n$$\\begin{align*} \\max_{A \\in \\text{St}(m, n) \\cap T_W \\text{St}(m, n)} \\langle G, A \\rangle \u0026= \\max_{A \\in \\text{St}(m, n) \\cap T_W \\text{St}(m, n)} \\langle \\underbrace{G - \\texttt{proj}_{T_W \\text{St}(m, n)}(G)}_{\\in N_W\\text{St}(m, n)} + \\texttt{proj}_{T_W \\text{St}(m, n)}(G), A \\rangle \\\\ \u0026= \\max_{A \\in \\text{St}(m, n) \\cap T_W \\text{St}(m, n)} \\langle \\texttt{proj}_{T_W \\text{St}(m, n)}(G), A \\rangle \\\\ \u0026\\leq \\max_{A \\in \\text{St}(m, n)} \\langle \\texttt{proj}_{T_W \\text{St}(m, n)}(G), A \\rangle \\\\ \\max_{A \\in \\text{St}(m, n) \\cap T_W \\text{St}(m, n)} \\langle G, A \\rangle \u0026\\leq \\| \\texttt{proj}_{T_W \\text{St}(m, n)}(G) \\|_{\\text{nuc}} \\end{align*}$$\nThe alignment between an element in the normal space and an element in the tangent space is always zero, i.e., $\\langle \\texttt{proj}_{T_W \\text{St}(m, n)}(G), \\texttt{proj}_{N_W \\text{St}(m, n)}(G) \\rangle = 0$. Thus, we can cancel out the first term in the first equality. And we have first inequality because the max over a smaller set is less than or equal to the max over a larger set.\nWe achieve equality in the square and full-rank case because the maximizer for the first inequality is guaranteed to be in the tangent space at $W$, as discussed in the previous section.\n5.2. Fixed-point iteration of alternating projections Notice that $QQ^T G$ is the projection of $G$ onto the column space of $Q$, $\\texttt{proj}_{\\text{col}(Q)}(G) = QQ^T G$. One can think of this as the component of $G$ that is, in a sense, not “aligned to” $W$. In practice, this is typically small relative to the component of $G$ that is “aligned to” $W$. If so, then, $$(\\texttt{proj}_{\\text{St}(m, n)} \\circ \\texttt{proj}_{T_W St(m, n)})(G) \\approx \\texttt{msign}(W \\texttt{skew}(W^T G) + \\cancel{\\texttt{proj}_{\\text{col}(Q)}(G)}),$$ which means that while the resulting matrix after the two projections may not be in the tangent space at $W$, it would likely be nearby. And repeating this process a few times should close the gap.\nSample implementation:\ndef project_to_stiefel_tangent_space(X, delta_X): return delta_X - X @ sym(X.T @ delta_X) def orthogonalize(X): # copy Newton-Schulz iteration from Muon (Jordan et al., 2024) def steepest_descent_stiefel_manifold_heuristic(W, G, num_steps=1): assert num_steps \u003e 0, \"Number of steps must be positive\" A_star = G for _ in range(num_steps): A_star = project_to_stiefel_tangent_space(W, A_star) A_star = orthogonalize(A_star) return A_star 5.2.1. Local (linear) convergence guarantee For now, we cannot yet guarantee global convergence as it would potentially require deriving the Lyapunov function for this iteration which is extremely difficult. What we can guarantee, however, is local convergence due to $\\text{St}(m, n)$ and $T_W\\text{St}(m, n)$ being closed semi-algebraic sets. That is, assuming that the initial point $W$ is “close enough” to the intersection, we can guarantee that a subsequence of the iterates converges to a point in the intersection. Furthermore, assuming transversality, we can guarantee that the convergence is linear.\nDefinition 5 (Semi-algebraic set) Let $\\mathbb{F}$ be a real closed field. A subset $S$ of $\\mathbb{F}^n$ is a semi-algebraic set if it is a finite union of sets defined by polynomial equations and inequalities.\nWe can construct $\\text{St}(m, n) = \\{ W \\in \\mathbb{R}^{m \\times n} | W^T W = I_n \\}$ via $n(n+1)/2$ polynomial equations, and $T_W\\text{St}(m, n) = \\{ A \\in \\mathbb{R}^{m \\times n} | W^T A + A^T W = 0 \\}$ via $mn$ polynomial equations. Hence both are semi-algebraic sets. And the intersection of two semi-algebraic sets is also a semi-algebraic set. From Theorem 7.3 of Drusvyatskiy et al. (2016), we then have the following convergence guarantee.\nTheorem 6 (Convergence of alternating projections on semi-algebraic sets) Consider two nonempty closed semi-algebraic sets $X, Y \\subset E$ with $X$ bounded. If the method of alternating projections starts in $Y$ and near $X$, then the distance of the iterates to the intersection $X \\cap Y$ converges to zero, and hence every limit point lies in $X \\cap Y$.\nSetting $Y = T_W\\text{St}(m, n)$, $X = \\text{St}(m, n)$, and noting that the Stiefel manifold is bounded, we have that the iterates of our method of alternating projections converge to a point in the intersection $\\text{St}(m, n) \\cap T_W\\text{St}(m, n)$.\nBut does this algorithm converge in sufficient time? Somewhat yes, we can guarantee local linear convergence assuming transversality.\nDefinition 7 (Transversality) Let $\\mathcal{M}$ and $\\mathcal{N}$ be two (smooth) manifolds in a Euclidean space. We say that $\\mathcal{M}$ and $\\mathcal{N}$ intersect transversally at a point $x \\in \\mathcal{M} \\cap \\mathcal{N}$ when, $$N_x\\mathcal{M} \\cap N_x\\mathcal{N} = \\{0\\}.$$\nIntuitively speaking, this means that the tangent spaces at the intersection of the two manifolds have an “angle” between them, i.e., they are not parallel. The larger this angle is, the faster the convergence; and the smaller the angle, the slower the convergence. Theorem 2.1 of Drusvyatskiy et al. (2016) then gives us the following local linear convergence guarantee.\nTheorem 8 (Linear convergence of alternating projections, assuming transversality) If two closed sets in a Euclidean space intersect transversally at a point $\\tilde{x}$, then the method of alternating projections, started nearby, converges linearly to a point in the intersection.\n$\\text{St}(m, n)$ and $T_W\\text{St}(m, n)$ are both closed sets so the theorem above applies.\n5.3. Ternary search over nearby feasible solutions Here we present an alternative solution that is more efficient, but often yields slightly more suboptimal results than the fixed-point iteration method above.\n5.3.1. Problem decomposition The crux is to split $\\arg\\max_{A\\in \\mathbb{R}^{m \\times n}} \\langle G, A \\rangle$ into two optimization problems, one for the component of $G$ that is “aligned to” $W$ and one for the component of $G$ that is “not aligned to” $W$. To see this, let us first decompose $G$ and $A$ into, $$G = W G_W + Q G_Q \\qquad A = WB + QC$$ where $G_W = W^T G$ and $G_Q = Q^T G$. Thus, $$\\begin{align} \\langle G, A \\rangle \u0026= \\langle W G_W + Q G_Q, WB + QC \\rangle \\nonumber \\\\ \u0026= \\text{tr}((W G_W + Q G_Q)^T (WB + QC)) \\nonumber \\\\ \u0026= \\text{tr}(G_W^T B + G_Q^T C) \\nonumber \\\\ \\langle G, A \\rangle \u0026= \\langle G_W, B \\rangle + \\langle G_Q, C \\rangle \\\\ \\end{align}$$ The cross terms vanish in the third equality because $W^T Q = 0$. Finding the maximizer $A^*$ for the LHS is then equivalent to finding the maximizers $B^*$ and $C^*$ for the RHS and then combining them, $$A^* = WB^* + QC^*.$$\n5.3.2. Solving the two subproblems Now, to satisfy the constraint $A \\in T_W\\text{St}(m, n) \\cap \\text{St}(m, n)$, $$\\begin{align*} W^T A + A^T W \u0026= 0 \\qquad\\qquad\u0026 A^T A \u0026= I_n \\\\ W^T (WB + QC) + (WB + QC)^T W \u0026= 0 \\qquad\\qquad\u0026 (WB + QC)^T(WB + QC) \u0026= I_n \\\\ B + B^T \u0026= 0 \\qquad\\qquad\u0026 B^T B + C^T C \u0026= I_n \\\\ \\end{align*}$$ That is, we require that $B$ is skew-symmetric and $C$ satisfies $C^T C = I_n - B^T B$.\nWe cannot simply treat each subproblem separately because the second constraint couples $B$ and $C$. However, for this approximation, we make the assumption that doing so would yield a “good enough” solution.\nFor the first term, we can decompose $G_W$ into its skew-symmetric and symmetric components, $G_W = \\texttt{skew}(G_W) + \\texttt{sym}(G_W)$, $$\\begin{align*} \\arg\\max_{B \\text{ is skew}} \\langle G_W, B \\rangle \u0026= \\arg\\max_{B \\text{ is skew}} \\langle \\texttt{skew}(G_W) + \\texttt{sym}(G_W), B \\rangle \\\\ \u0026= \\arg\\max_{B \\text{ is skew}} \\langle \\texttt{skew}(G_W), B \\rangle + \\cancel{\\langle \\texttt{sym}(G_W), B \\rangle} \\end{align*}$$ And the maximizer is simply $\\tilde{B} = \\texttt{skew}(G_W) = \\texttt{skew}(W^T G)$.\nHowever, because of the constraint $B^T B + C^T C = I$, we have to “cap” the spectral norm of $B$ to be less than or equal to 1 otherwise we would fail to construct a real-valued $C$. We can do this via a variety of methods discussed in a previous post on spectral clipping and our latest paper on training transformers with enforced Lipschitz bounds (Newhouse*, Hess*, Cesista* et al., 2025). Let $\\tau \\leq 1$ be the spectral norm bound, then we can choose,\n$$B^*(\\tau) := \\texttt{hard\\_cap}_{\\tau}(\\tilde{B}) \\quad\\text{or}\\quad B^*(\\tau) := \\tau\\cdot\\texttt{msign}(\\tilde{B}) \\quad\\text{or}\\quad B^*(\\tau) := \\frac{\\tau}{\\| \\tilde{B} \\|_{2 \\to 2}}\\tilde{B}$$\nThese mappings preserve skew-symmetry and thus $B^*(\\tau)$ satisfies the first constraint.\nNow, parametrize $C$ as $C = UR$ where $U^T U = I$ and $R(\\tau) = (I_n - (B^*(\\tau))^T B^*(\\tau))^{1/2}$. It is trivial to check that $C$ satisfies our constraints and that $R(\\tau)$ is SPD. Thus, assuming we already have a fixed $B^*(\\tau)$ (and consequently a fixed $R(\\tau)$), solving the second subproblem, $$\\arg\\max_{C} \\langle G_Q, C \\rangle$$ is equivalent to solving, $$\\begin{align*} \\arg\\max_{U: U^T U = I} \\langle G_Q, U R(\\tau) \\rangle \u0026= \\arg\\max_{U: U^T U = I} \\text{tr}(G_Q^T U R(\\tau)) \\\\ \u0026= \\arg\\max_{U: U^T U = I} \\text{tr}(R(\\tau) G_Q^T U) \\\\ \u0026= \\arg\\max_{U: U^T U = I} \\langle G_Q R(\\tau), U \\rangle \\end{align*}$$ which has maximizer $U^* = \\texttt{msign}(G_Q R(\\tau))$. Thus, $C^*(\\tau) = \\texttt{msign}(G_Q R(\\tau)) R(\\tau)$.\nNote that for the square and full-rank case, we have $C = 0$ and so we require $B^T B = I$. For this, we can choose to orthogonalize $B = \\texttt{skew}(G_W)$ which then yields Bernstein’s solution, $$A^*_{\\text{bernstein}} = W \\texttt{msign}(\\texttt{skew}(W^T G))$$ This also motivates the choice of $\\texttt{msign}$ as the normalization method for $B$ more generally.\nWe can implement this in JAX as follows,\nfrom spectral_clipping import spectral_hardcap, spectral_normalize, orthogonalize def matsqrt(W: jax.Array): # We can also compute this via Newton-Schulz iteration or Cholesky decomposition U, s, Vh = jnp.linalg.svd(W, full_matrices=False) s_sqrt = jnp.sqrt(s) return U @ jnp.diag(s_sqrt) @ Vh def construct_nearby_feasible_solution(W, Q, G, tau=0.5, normalizer_method=0): m, n = W.shape if m == n: # assumes full-rank A = W @ orthogonalize(skew(W.T @ G)) # Bernstein's solution else: B = skew(W.T @ G) if normalizer_method == 0: B_tilde = tau * orthogonalize(B) elif normalizer_method == 1: B_tilde = spectral_hardcap(B, tau) elif normalizer_method == 2: B_tilde = spectral_normalize(B, tau) R = jnp.linalg.cholesky(jnp.eye(n) - B_tilde.T @ B_tilde) # R = matsqrt(jnp.eye(n) - B_tilde.T @ B_tilde) C = orthogonalize(Q.T @ G @ R) @ R A = W @ B_tilde + Q @ C return A 5.3.1. Where ternary search comes in From Equation (7), we have,\n$$\\begin{align*} f(\\tau) := \\langle G, A^*(\\tau) \\rangle \u0026= \\langle G_W, B^*(\\tau) \\rangle + \\langle G_Q, C^*(\\tau) \\rangle \\\\ \u0026= \\langle \\texttt{skew}(G_W), B^*(\\tau) \\rangle + \\langle G_Q, C^*(\\tau) \\rangle \\\\ \u0026= \\langle \\texttt{skew}(G_W), \\texttt{normalized}_{\\tau}(\\texttt{skew}(G_W)) \\rangle + \\langle G_Q, \\texttt{msign}(G_Q R(\\tau)) R(\\tau) \\rangle \\\\ \u0026= \\langle \\texttt{skew}(G_W), \\texttt{normalized}_{\\tau}(\\texttt{skew}(G_W)) \\rangle + \\| G_QR(\\tau) \\|_{\\text{nuc}} \\\\ \\end{align*}$$\nThe first term is linear and positively-sloped as we vary $\\tau$. And since the map $x \\mapsto \\sqrt{1 - x^2}$ is concave and non-increasing in $x \\in [0, 1]$, the second term must be concave and non-increasing as we increase $\\tau$. Taken together, $f$ must be unimodal. And thus we can use ternary search to find the optimal $\\tau$ that maximizes $f(\\tau)$.\nWe can implement this in JAX as follows,\ndef ternary_search_over_taus(W, Q, G, lo=0., hi=1., normalizer_method=0, max_iter=10): def evaluate(tau): A = construct_nearby_feasible_solution(W, Q, G, tau, normalizer_method) return jnp.trace(G.T @ A) def body_fun(i, val): lo, hi = val mid1 = (2*lo + hi) / 3 mid2 = (lo + 2*hi) / 3 f_mid1 = evaluate(mid1) f_mid2 = evaluate(mid2) new_lo = jnp.where(f_mid1 \u003e f_mid2, lo, mid1) new_hi = jnp.where(f_mid1 \u003e f_mid2, mid2, hi) return new_lo, new_hi final_lo, final_hi = jax.lax.fori_loop(0, max_iter, body_fun, (lo, hi)) # Compute final midpoint and its value final_tau = (final_lo + final_hi) / 2 final_value = evaluate(final_tau) return final_tau, final_value 6. Bonus: a Muon-like optimizer for the Embedding and Unembedding layers Embedding layers have a hidden geometry: the (scaled-)Oblique manifold, $\\widetilde{\\text{Ob}}(m, n) = \\{ W \\in \\mathbb{R}^{m \\times n} | \\text{diag}(W^T W) = s^2\\mathbf{1}_n \\}$ with scale $s = \\sqrt{m}$, or the manifold of matrices with unit-RMS-norm columns. More precisely, it is the embedding layer and the normalization layer right after it that results in unit-RMS-norm feature-vectors. But optimizers like Adam typically ignore this geometry and even its matrix-structure, treating the embedding layer the same as ‘flat’ vectors. We believe this leads to suboptimal performance and demonstrate this via grokking experiments we discuss in the next section.\nWhat if we build an optimizer that respects this geometry?\nFor this, we need two things:\nA ‘dualizer’ map that maps a “raw gradient” matrix $G \\in \\mathbb{R}^{m \\times n}$ to an update direction of steepest descent on the tangent space at $W \\in \\widetilde{\\text{Ob}}(m, n)$, i.e., $A^* \\in T_W\\widetilde{\\text{Ob}}(m, n)$ with $\\| A^* \\| = 1$ for some norm $\\| \\cdot \\|$ chosen a priori. And, A ‘projection’ or retraction map that maps an (updated) weight matrix $W \\in \\mathbb{R}^{m \\times n}$ back to the (scaled-)Oblique manifold. The retraction map is simply the column-wise normalization, $$\\texttt{col\\_normalize}(W) := \\text{col}_j(W) \\mapsto \\frac{\\text{col}_j(W)}{\\| \\text{col}_j(W) \\|_{RMS}} = \\sqrt{m}\\frac{\\text{col}_j(W)}{\\| \\text{col}_j(W) \\|_{2}} \\quad \\forall 0 \\leq j \u003c n$$ where $\\text{col}_j(W)$ is the $j$-th column of the weight matrix $W$.\nAs for the dualizer, which norm should we use? We can, for example, use the RMS-to-RMS norm for consistency and still be able to use the same alternating projection method as before. However, as argued by Bernstein \u0026 Newhouse (2024) and Pethick et al. (2024), it may be more natural to use the L1-to-RMS norm, $\\| \\cdot \\|_{1\\to RMS}$ because the maximizer for the following problem, $$\\arg\\max_{A: \\| A \\|_{1 \\to RMS} = 1} \\langle G, A \\rangle$$ is simply $\\texttt{col\\_normalize}(G) \\in \\widetilde{\\text{Ob}}(m, n)$. That is, all of the token embedding updates would have the same size, improving training stability. Thus our update rule becomes, $$ W \\leftarrow \\texttt{col\\_normalize}(W - \\eta A^*)$$ where $\\eta$ is the learning rate and, $$ A^* = \\arg\\max_{A: \\| A \\|_{1 \\to RMS} = 1} \\langle G, A \\rangle \\quad \\text{s.t. } A \\in T_W\\widetilde{\\text{Ob}}(m, n),$$\nEquivalently, $$A^* = \\arg\\max_{A} \\langle G, A \\rangle \\quad \\text{s.t. } A \\in \\widetilde{\\text{Ob}}(m, n) \\cap T_W \\widetilde{\\text{Ob}}(m, n),$$ or in words, we want to find a descent direction $A^*$ that is both on the (scaled-)Oblique manifold and in the tangent space at $W$ that maximizes the alignment with the “raw gradient” $G$.\n6.1. Optimal solution for steepest descent on the (scaled-)Oblique manifold The Oblique manifold is a product of hyperspheres, $\\text{Ob}(m, n) = \\underbrace{S^m \\times \\ldots \\times S^m}_{n}$. So, in a sense, the columns are acting independently of each other and steepest descent on the Oblique manifold is equivalent to steepest descent on the hypersphere, applied column-wise. Generalizing Bernstein’s (2025b) dualizer for steepest descent on the hypersphere to the Oblique manifold then yields,\nThe optimal solution for finding the direction of steepest descent on the Oblique manifold $A^*$ given “raw Euclidean gradient” or differential $G$ is to simply project $G$ onto the tangent space at point $W \\in \\widetilde{\\text{Ob}}(m, n)$ and then normalize column-wise.\nThe tangent space at $W$ is simply, $$T_W\\widetilde{\\text{Ob}}(m, n) = \\{A \\in \\mathbb{R}^{m \\times n} | \\text{diag}(W^T A) = 0\\}$$ or in words, the column-wise dot-product or “alignment” between $W$ and a candidate tangent vector $A$ must be zero for $A$ to be in the tangent space at $W$. The projector onto the tangent space at $W$ is then given by, $$\\texttt{proj}_{T_W\\widetilde{\\text{Ob}}(m, n)}(G) = G - W \\text{diag}(W^T G / m)$$ or in words, we subtract the component of $G$ that is “aligned to” $W$.\nNotice then that one of the constraints is concerned with the size of the columns while the other is concerned with the direction. These can be optimized independently of each other. Thus, the solution for $A^*$ is then simply, $$A^* = \\texttt{col\\_normalize}(\\texttt{proj}_{T_W\\widetilde{\\text{Ob}}(m, n)}(G))$$\n6.2. Steepest descent on the (scaled-)Row-Oblique manifold We argue that the Unembedding layer or the ’language model head’ should naturally be on the (scaled-)Row-Oblique manifold, $\\widetilde{\\text{RowOb}}(m, n) = \\{ W \\in \\mathbb{R}^{m \\times n} | \\text{diag}(WW^T) = s^2\\mathbf{1}_m \\}$ with scale $s = \\sqrt{n}$, or the manifold of matrices with unit-RMS-norm rows. The crux is that the logit for the $i$-th vocabulary token is given by the dot-product or ‘alignment’ between the $i$-th row of the weight matrix and the feature vector. So if the logits measure ‘alignment’, not ‘size’, then it is natural to constrain the rows to have unit-RMS-norm.\nAnd since we can construct $\\widetilde{\\text{RowOb}}(m, n)$ by transposing $\\widetilde{\\text{Ob}}(m, n)$, we can use the same reasoning as above to derive the optimal solution for steepest descent on the (scaled-)Row-Oblique manifold.\nOur retraction map is simply the row-wise normalization, $$\\texttt{row\\_normalize}(W) := \\text{row}_i(W) \\mapsto \\frac{\\text{row}_i(W)}{\\| \\text{row}_i(W) \\|_{RMS}} = \\sqrt{n}\\frac{\\text{row}_i(W)}{\\| \\text{row}_i(W) \\|_{2}} \\quad \\forall 0 \\leq i \u003c m$$ where $\\text{row}_i(W)$ is the $i$-th row of the weight matrix $W$. We then choose the $\\| \\cdot \\|_{RMS \\to \\infty}$ norm because the maximizer for the following problem, $$\\arg\\max_{A: \\| A \\|_{RMS \\to \\infty} = 1} \\langle G, A \\rangle$$ is simply $\\texttt{row\\_normalize}(G) \\in \\widetilde{\\text{RowOb}}(m, n)$. That is, the per-row updates would have even size.\nOur update rule then becomes, $$ W \\leftarrow \\texttt{row\\_normalize}(W - \\eta A^*)$$ where $\\eta$ is the learning rate and, $$A^* = \\arg\\max_{A} \\langle G, A \\rangle \\quad \\text{s.t. } A \\in \\widetilde{\\text{RowOb}}(m, n) \\cap T_W \\widetilde{\\text{RowOb}}(m, n),$$ which has the closed form solution, $$\\begin{align*} A^* \u0026= \\texttt{row\\_normalize}(\\texttt{proj}_{T_W\\widetilde{\\text{RowOb}}(m, n)}(G)) \\\\ \u0026= \\texttt{row\\_normalize}(G - \\text{diag}(G W^T / n) W) \\\\ \\end{align*}$$\n7. Experimental results 7.1. Alternating projections method beats ternary search on nearby feasible solutions on larger matrices Here we compare our two heuristic methods for the problem of spectral-norm constrained steepest descent on the Stiefel manifold. Observe from the figures above that the ternary search over nearby feasible solutions method results in almost optimal solutions, regardless of scale. However, the alternating projections method results in more aligned solutions, albeit at the cost of more compute and being more off-tangent.\n7.2. Grokking on the Addition-Modulo-113 task in 44 full-batch training steps We will release the source code soon, but if you want early access, please email me.\nWe use the same training setup for grokking experiments on the Addition-Modulo-113 problem as in a previous post on spectral clipping, with new dualizers and projection maps added. Following Prieto et al. (2025), we use a 2-layer MLP (plus Embedding and Unembedding layers) with 200 hidden units per layer. All matrix multiplications are done in bfloat16 precision.\nWe place our Embedding and Unembedding weights on the (scaled-)Oblique manifold and (scaled-)Row-Oblique manifold, respectively. We then vary the dualizer and retraction maps used in the linear layers and report the best median-steps-to-grokking across 64 random seeds. See figure above for the results.\nInterestingly, without weight constraints, models fail to grok within 1000 full-batch training steps. This is true for both the Muon optimizer and AdamW. However, with weight constraints, we were able to achieve grokking in 44 full-batch training steps, which we believe is SOTA.\nThe best recipe seems to be:\nlayer manifold $\\texttt{dualizer}$ $\\texttt{retract}$ Embedding (Scaled-)Oblique manifold $\\texttt{col\\_normalize} \\circ \\texttt{proj}_{T_W\\widetilde{\\text{Ob}}(m, n)}$ $\\texttt{col\\_normalize}$ 1st Linear RMS-to-RMS norm ball\naround the origin of $\\mathbb{R}^{m \\times n}$ $\\texttt{msign}$ $\\texttt{spectral\\_normalize}$ 2nd Linear RMS-to-RMS norm ball\naround the origin of $\\mathbb{R}^{m \\times n}$ $\\texttt{msign}$ $\\texttt{spectral\\_normalize}$ Unembedding (Scaled-)Row-Oblique manifold $\\texttt{row\\_normalize} \\circ \\texttt{proj}_{T_W\\widetilde{\\text{RowOb}}(m, n)}$ $\\texttt{row\\_normalize}$ Acknowledgements Big thanks to Jianlin Su, Jeremy Bernstein, Vinay Rao, Antonio Silveti-Falls, Mikail Khona, Omead Pooladzandi, Simo Ryu, and Kevin Yin for productive discussions on the topic.\nHow to cite @misc{cesista2025spectralclipping, author = {Franz Louis Cesista}, title = {\"Heuristic Solutions for Steepest Descent on the Stiefel Manifold\"}, year = {2025}, month = {July}, day = {18}, url = {http://leloykun.github.io/ponder/steepest-descent-stiefel/}, } If you find this post useful, please consider supporting my work by sponsoring me on GitHub: References Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein (2024). Muon: An optimizer for hidden layers in neural networks. Available at: https://kellerjordan.github.io/posts/muon/ Greg Yang, James B. Simon, Jeremy Bernstein (2024). A Spectral Condition for Feature Learning. URL https://arxiv.org/abs/2310.17813 Tim Large, Yang Liu, Minyoung Huh, Hyojin Bahng, Phillip Isola, Jeremy Bernstein (2024). Scalable Optimization in the Modular Norm. URL https://arxiv.org/abs/2405.14813 Jeremy Bernstein, Laker Newhouse (2024). Old Optimizer, New Norm: An Anthology. URL https://arxiv.org/abs/2409.20325 Laker Newhouse (2025). Understanding Muon. URL https://www.lakernewhouse.com/writing/muon-1 Laker Newhouse, R. Preston Hess, Franz Cesista, Andrii Zahorodnii, Jeremy Bernstein, Phillip Isola (2025). Training Transformers with Enforced Lipschitz Constants. URL https://arxiv.org/abs/2507.13338 Jeremy Bernstein (2025a). Orthogonal manifold. URL https://docs.modula.systems/algorithms/manifold/orthogonal/ Jeremy Bernstein (2025b). Hypersphere. URL https://docs.modula.systems/algorithms/manifold/hypersphere/ Jianlin Su (2025). Steepest descent on Stiefel manifold. URL https://x.com/YouJiacheng/status/1945522729161224532 D. Drusvyatskiy, A.D. Ioffe, A.S. Lewis (2016). Transversality and alternating projections for nonconvex sets. URL https://arxiv.org/abs/1401.7569 Thomas Pethick, Wanyun Xie, Kimon Antonakopoulos, Zhenyu Zhu, Antonio Silveti-Falls, Volkan Cevher (2025). Training Deep Learning Models with Norm-Constrained LMOs. URL https://arxiv.org/abs/2502.07529 Bin Gao, Simon Vary, Pierre Ablin, P.-A. Absil (2022). Optimization flows landing on the Stiefel manifold. URL https://arxiv.org/abs/2202.09058 Pierre Ablin, Gabriel Peyré (2021). Fast and accurate optimization on the orthogonal manifold without retraction. URL https://arxiv.org/abs/2102.07432 Lucas Prieto, Melih Barsbey, Pedro A.M. Mediano, Tolga Birdal (2025). Grokking at the Edge of Numerical Stability. URL https://arxiv.org/abs/2501.04697 ",
  "wordCount" : "5953",
  "inLanguage": "en",
  "image":"https://leloykun.github.io/ponder/steepest-descent-stiefel/steepest-descent-stiefel-manifold.jpg","datePublished": "2025-07-18T00:00:00Z",
  "dateModified": "2025-07-18T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Franz Louis Cesista"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://leloykun.github.io/ponder/steepest-descent-stiefel/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Franz Louis Cesista",
    "logo": {
      "@type": "ImageObject",
      "url": "https://leloykun.github.io/favicon.ico"
    }
  }
}
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-BWCGBRX8G1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BWCGBRX8G1');
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"
  integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"
  integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js"
  integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
        { left: "\\begin{equation}", right: "\\end{equation}", display: true },
        { left: "\\begin{equation*}", right: "\\end{equation*}", display: true },
        { left: "\\begin{align}", right: "\\end{align}", display: true },
        { left: "\\begin{align*}", right: "\\end{align*}", display: true },
        { left: "\\begin{alignat}", right: "\\end{alignat}", display: true },
        { left: "\\begin{gather}", right: "\\end{gather}", display: true },
        { left: "\\begin{CD}", right: "\\end{CD}", display: true },
      ],
      throwOnError: false,
      trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
      macros: {
        "\\eqref": "\\href{###1}{(\\text{#1})}",
        "\\ref": "\\href{###1}{\\text{#1}}",
        "\\label": "\\htmlId{#1}{}"
      }
    });
  });
</script>
 


</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://leloykun.github.io/" accesskey="h" title="Franz Louis Cesista">
                <img src="https://leloykun.github.io/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Franz Louis Cesista</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://leloykun.github.io/cv.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/personal-projects/" title="Personal Projects">
                    <span>Personal Projects</span>
                </a>
            </li>
            <li>
                <a href="https://leloykun.github.io/ponder/" title="Ponder (Posts)">
                    <span>Ponder (Posts)</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Heuristic Solutions for Steepest Descent on the Stiefel Manifold
    </h1>
    <div class="post-meta"><span title='2025-07-18 00:00:00 +0000 UTC'>July 18, 2025</span>&nbsp;&middot;&nbsp;28 min&nbsp;&middot;&nbsp;Franz Louis Cesista&nbsp;&middot;&nbsp;<a href="https://x.com/leloykun/status/1953821883482456446" rel="noopener noreferrer" target="_blank">Crossposted on X (formerly Twitter)</a>

</div>
  </header> 
<figure class="entry-cover">
            <img loading="eager"
                srcset='https://leloykun.github.io/ponder/steepest-descent-stiefel/steepest-descent-stiefel-manifold_hu96f70d7508379ec217e7a2e241ae6f20_56654_360x0_resize_q75_box.jpg 360w,https://leloykun.github.io/ponder/steepest-descent-stiefel/steepest-descent-stiefel-manifold_hu96f70d7508379ec217e7a2e241ae6f20_56654_480x0_resize_q75_box.jpg 480w,https://leloykun.github.io/ponder/steepest-descent-stiefel/steepest-descent-stiefel-manifold_hu96f70d7508379ec217e7a2e241ae6f20_56654_720x0_resize_q75_box.jpg 720w,https://leloykun.github.io/ponder/steepest-descent-stiefel/steepest-descent-stiefel-manifold_hu96f70d7508379ec217e7a2e241ae6f20_56654_1080x0_resize_q75_box.jpg 1080w,https://leloykun.github.io/ponder/steepest-descent-stiefel/steepest-descent-stiefel-manifold_hu96f70d7508379ec217e7a2e241ae6f20_56654_1500x0_resize_q75_box.jpg 1500w,https://leloykun.github.io/ponder/steepest-descent-stiefel/steepest-descent-stiefel-manifold.jpg 1512w'
                src="https://leloykun.github.io/ponder/steepest-descent-stiefel/steepest-descent-stiefel-manifold.jpg"
                sizes="(min-width: 768px) 720px, 100vw"
                width="1512" height="816"
                alt="Cover">
        
</figure><div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#1-recap-muon-as-rms-to-rms-norm-constrained-steepest-descent">1. Recap: Muon as RMS-to-RMS norm-constrained steepest descent</a>
      <ul>
        <li><a href="#11-recap-non-riemannian-steepest-descent">1.1. Recap: (non-)Riemannian steepest descent</a></li>
      </ul>
    </li>
    <li><a href="#2-spectral-norm-constrained-steepest-descent-on-the-stiefel-manifold">2. Spectral norm-constrained steepest descent on the Stiefel manifold</a>
      <ul>
        <li><a href="#21-rms-to-rms-norm-constrained-steepest-descent-on-the-scaled-stiefel-manifold">2.1. RMS-to-RMS norm-constrained steepest descent on the (scaled) Stiefel manifold</a></li>
        <li><a href="#22-retraction-via-rescaling">2.2. Retraction via rescaling</a></li>
      </ul>
    </li>
    <li><a href="#3-equivalence-between-bernsteins-and-sus-solutions">3. Equivalence between Bernstein&rsquo;s and Su&rsquo;s solutions</a></li>
    <li><a href="#4-projection-projection-perspective">4. Projection-projection perspective</a>
      <ul>
        <li><a href="#41-why-bernsteins--sus-solutions-only-work-for-the-square-and-full-rank-case">4.1. Why Bernstein&rsquo;s &amp; Su&rsquo;s solutions only work for the square and full-rank case</a></li>
      </ul>
    </li>
    <li><a href="#5-heuristic-solutions-for-the-general-case">5. Heuristic solutions for the general case</a>
      <ul>
        <li><a href="#51-alignment-upper-bound">5.1. Alignment upper bound</a></li>
        <li><a href="#52-fixed-point-iteration-of-alternating-projections">5.2. Fixed-point iteration of alternating projections</a></li>
        <li><a href="#53-ternary-search-over-nearby-feasible-solutions">5.3. Ternary search over nearby feasible solutions</a></li>
      </ul>
    </li>
    <li><a href="#6-bonus-a-muon-like-optimizer-for-the-embedding-and-unembedding-layers">6. Bonus: a Muon-like optimizer for the Embedding and Unembedding layers</a>
      <ul>
        <li><a href="#61-optimal-solution-for-steepest-descent-on-the-scaled-oblique-manifold">6.1. Optimal solution for steepest descent on the (scaled-)Oblique manifold</a></li>
        <li><a href="#62-steepest-descent-on-the-scaled-row-oblique-manifold">6.2. Steepest descent on the (scaled-)Row-Oblique manifold</a></li>
      </ul>
    </li>
    <li><a href="#7-experimental-results">7. Experimental results</a>
      <ul>
        <li><a href="#71-alternating-projections-method-beats-ternary-search-on-nearby-feasible-solutions-on-larger-matrices">7.1. Alternating projections method beats ternary search on nearby feasible solutions on larger matrices</a></li>
        <li><a href="#72-grokking-on-the-addition-modulo-113-task-in-44-full-batch-training-steps">7.2. Grokking on the Addition-Modulo-113 task in 44 full-batch training steps</a></li>
      </ul>
    </li>
    <li><a href="#acknowledgements">Acknowledgements</a></li>
    <li><a href="#how-to-cite">How to cite</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>If you find this post useful, please consider supporting my work by sponsoring me on GitHub: <a href="https://github.com/sponsors/leloykun" target="_blank"><img alt="Sponsor on GitHub" loading="lazy" src="https://img.shields.io/badge/%F0%9F%A4%9D-Sponsor%20me-1da1f2?logo=github&style=flat-square"></a></p>
</blockquote>
<h2 id="1-recap-muon-as-rms-to-rms-norm-constrained-steepest-descent">1. Recap: Muon as RMS-to-RMS norm-constrained steepest descent<a hidden class="anchor" aria-hidden="true" href="#1-recap-muon-as-rms-to-rms-norm-constrained-steepest-descent">#</a></h2>
<p>Consider a weight matrix $W \in \mathbb{R}^{m \times n}$ and a &ldquo;raw gradient&rdquo; or differential $G \in \mathbb{R}^{m \times n}$ we get via e.g., backpropagation. In standard gradient descent, we would update the weights as follows,
$$W \leftarrow W - \eta G,$$
where $\eta \in (0, \infty)$ is the learning rate. However, this is suboptimal because (1) the update sizes $\| G \|$ could vary wildly across steps thereby causing training instability, and (2) as we discussed in a <a href="../steepest-descent-non-riemannian/">previous blog post on (non-)Riemannian steepest descent</a>, it does not take into account the matrix structure of the weights. In particular, it ignores how activations or &ldquo;features&rdquo; evolve through the network and how the model behaves as we scale it up. Tl;dr:</p>
<blockquote>
<p>If we want the Euclidean norm $\| \cdot \|_2$ of our features and feature updates to &lsquo;grow&rsquo; with the model size,
then the <em>Spectral norm</em> $\| \cdot \|_{2 \to 2}$ of our weights and weight updates must also &lsquo;grow&rsquo; with the model size.</p>
</blockquote>
<p>Equivalently, following Yang et al. (2024), we can use the &ldquo;natural&rdquo; feature norm, the RMS norm $\|\cdot\|_{RMS} = \frac{1}{\sqrt{n}}\|\cdot\|_{2}$, and the &ldquo;natural&rdquo; weight norm, the RMS-to-RMS norm $\|\cdot\|_{RMS \to RMS} = \frac{\sqrt{n}}{\sqrt{m}}\| \cdot \|_{2 \to 2}$, and rephrase the above as,</p>
<blockquote>
<p>If we want the &ldquo;natural&rdquo; norm of our features and feature updates to be stable regardless of the model size,
then the &ldquo;natural&rdquo; norm of our weights and weight updates must also be stable regardless of the model size.</p>
</blockquote>
<p>We will discuss weight norm controls in the next section, but for now, instead of using the raw gradient $G$, we can instead try to find a descent direction $A \in \mathbb{R}^{m \times n}$ that is maximally aligned to $G$ while satisfying our weight update condition above,
$$\| A \|_{RMS \to RMS} = \frac{\sqrt{n}}{\sqrt{m}}\| A \|_{2 \to 2} = \text{constant}.$$
Thus our update rule becomes,
$$W \leftarrow W - \eta \frac{\sqrt{m}}{\sqrt{n}} A^*,$$
where
$$\begin{equation}
A^* = \arg\max_{A\in \mathbb{R}^{m \times n}:\| A \|_{2 \to 2} = 1} \langle G, A \rangle,
\end{equation}$$
and $\langle \cdot, \cdot \rangle$ is the Frobenius inner product which measures the &ldquo;alignment&rdquo; between two matrices. From Bernstein &amp; Newhouse (2024), this has a closed-form solution,
$$A^* = \texttt{msign}(G),$$
where $\texttt{msign}(\cdot)$ is the matrix sign function. And finally, adding a momentum term then yields the Muon optimizer (Jordan et al., 2024),
$$\begin{align*}
M_t &amp;= \beta M_{t-1} + (1 - \beta) G \\
W_t &amp;= W_{t-1} - \eta\frac{\sqrt{m}}{\sqrt{n}} \texttt{msign}(M_t), \\
\end{align*}$$
for some momentum hyperparameter $\beta \in [0, 1)$.</p>
<blockquote>
<p>If you want to learn more about Muon and the ideas behind it, check out <a href="https://www.lakernewhouse.com/writing/muon-1" target="_blank">Newhouse&rsquo;s 3-part blog series</a>. I highly recommend it!</p>
</blockquote>
<h3 id="11-recap-non-riemannian-steepest-descent">1.1. Recap: (non-)Riemannian steepest descent<a hidden class="anchor" aria-hidden="true" href="#11-recap-non-riemannian-steepest-descent">#</a></h3>
<p>An update step in first-order optimization on a manifold $\mathcal{M}$ goes as follows,</p>
<ol>
<li>Compute an &lsquo;optimal&rsquo; descent direction $A^*$ in the tangent space at the current point $W_t \in \mathcal{M}$, $A^* \in T_{W_t} \mathcal{M}$.</li>
<li>Use this to &lsquo;move&rsquo; our weight $\widetilde{W}_{t+1} \leftarrow W_t - \eta A^*$, where $\eta$ is the learning rate. Note that $\widetilde{W}_{t+1}$ may not be on the manifold $\mathcal{M}$. And so,</li>
<li>Retract the result back to the manifold via a retraction map $W_{t+1} \leftarrow \texttt{retract}(\widetilde{W}_{t+1})$.</li>
</ol>
<p>We then repeat this process until convergence or until we find a satisfactory solution.</p>
<p>An important detail discussed by Large et al. (2024) and in the <a href="../steepest-descent-non-riemannian/">previous blog post on (non-)Riemannian optimization</a> is that the so-called &ldquo;raw gradient&rdquo; $G$ we get via backpropagation is <em>not</em> actually in the tangent space, but rather in the <em>co</em>tangent space at $W$, $G \in T_W^* \mathcal{M}$, or the space of <em>linear functionals</em> acting on the tangent vectors. $G$ then is useless by itself. To make it useful, we need to map it to the tangent space first via a dualizer map, $\texttt{dualizer}: T_W^* \mathcal{M} \mapsto T_W \mathcal{M},$
$$A^* = \texttt{dualizer}(G) = \arg\max_{A \in T_W \mathcal{M}} \langle G, A \rangle,$$
where the $\langle \cdot, \cdot \rangle$ operation is the canonical pairing between tangent and cotangent spaces. It is technically not an inner product, but <em>behaves like</em> the Frobenius inner product.</p>
<p>In Euclidean space, we got lucky: $T_W \mathbb{R}^{m \times n} = T_W^* \mathbb{R}^{m \times n} = \mathbb{R}^{m \times n}$, and thus, $A^* = G$, yielding the update rule for (stochastic) gradient descent (SGD). In Riemannian manifolds, which we get by e.g. equipping the tangent spaces with a Riemannian metric (or a norm that induces such a metric), the two spaces are no longer equivalent, but they are congruent. This means that for every $G \in T_W^* \mathcal{M}$, there exists a <em>unique</em> steepest descent direction $A^* \in T_W \mathcal{M}$ we can follow to minimize the loss and vice versa. In non-Riemannian manifolds, however, the optimal $A^*$ may no longer be unique or may not even exist.</p>
<p>Muon then is what we get when we equip the tangent spaces of $\mathcal{M} = \mathbb{R}^{m \times n}$ with the RMS-to-RMS norm, $\| \cdot \|_{RMS \to RMS}$. So while the underlying space is still Euclidean, the change in how we measure &lsquo;distances&rsquo; makes the new manifold non-Euclidean and even non-Riemannian. In the next sections, we discuss how to build Muon-like optimizers for more exotic manifolds and demonstrate how a smart choice of manifolds to &lsquo;place&rsquo; our weights in can accelerate generalization.</p>
<h2 id="2-spectral-norm-constrained-steepest-descent-on-the-stiefel-manifold">2. Spectral norm-constrained steepest descent on the Stiefel manifold<a hidden class="anchor" aria-hidden="true" href="#2-spectral-norm-constrained-steepest-descent-on-the-stiefel-manifold">#</a></h2>
<p>As discussed in the previous section, we not only need to control the weight update norms but also the weight norms themselves. There are multiple ways to do this and we presented some novel methods in our recent work on <a href="https://arxiv.org/abs/2507.13338" target="_blank">training transformers with enforced Lipschitz bounds</a> (Newhouse*, Hess*, Cesista* et al., 2025). However, here we will focus on constraining the weights to be semi-orthogonal, i.e.,
$$\begin{equation} W^T W = I_n. \end{equation}$$</p>
<p>Semi-orthogonal matrices lie on the Stiefel manifold, $\text{St}(m, n) = \{W \in \mathbb{R}^{m \times n} | W^T W = I_n \}$. Differentiating Equation (2) on both sides then yields the constraint that determines membership in the tangent space at $W \in \text{St}(m, n)$,
$$T_W \text{St}(m, n) = \{A \in \mathbb{R}^{m \times n} | W^T A + A^T W = 0\}.$$
But a crucial difference from prior work on optimization on the Stiefel manifold (Ablin &amp; Peyré, 2021; Gao et al., 2022) is that we equip the tangent spaces with the spectral norm, $\| \cdot \|_{2 \to 2}$, augmenting the Stiefel manifold with a <a href="https://en.wikipedia.org/wiki/Finsler_manifold" target="_blank">Finsler structure</a>. With this, our dualizer becomes,
$$A^* = \arg\max_{A\in \mathbb{R}^{m \times n}: \| A \|_{2 \to 2} = 1} \langle G, A \rangle \quad \text{s.t. } A \in T_W\text{St}(m, n),$$
and using $\text{msign}(\cdot)$ as the retraction map, our update rule becomes,
$$\begin{equation}
W \leftarrow \text{msign}\left(W - \eta A^* \right)
\end{equation}$$</p>
<p>Equivalently,
$$A^* = \arg\max_{A\in \mathbb{R}^{m \times n}} \langle G, A \rangle  \quad \text{s.t. } A \in \text{St}(m, n) \cap T_W \text{St}(m, n),$$
Or in words, we want to find a descent direction $A$ that is both on the Stiefel manifold and in the tangent space at the current point $W \in \text{St}(m, n)$ that maximizes the &ldquo;alignment&rdquo; with the raw gradient $G$.</p>
<h3 id="21-rms-to-rms-norm-constrained-steepest-descent-on-the-scaled-stiefel-manifold">2.1. RMS-to-RMS norm-constrained steepest descent on the (scaled) Stiefel manifold<a hidden class="anchor" aria-hidden="true" href="#21-rms-to-rms-norm-constrained-steepest-descent-on-the-scaled-stiefel-manifold">#</a></h3>
<p>Following the natural norm conditions we discussed in the previous section, we may want to constrain our weights to be semi-orthogonal <em>with respect to</em> the RMS-to-RMS norm, i.e.,
$$W^T W = \frac{m}{n}I_n.$$
This places our weights on the scaled Stiefel manifold, $\widetilde{\text{St}}(m, n) = \{W \in \mathbb{R}^{m \times n} | W^T W = s^2 I_n \}$ with scale $s = \sqrt{m}/\sqrt{n}$. We can use the same dualizer map as for the unscaled Stiefel manifold, but our update rule becomes,
$$\begin{equation}
W \leftarrow \frac{\sqrt{m}}{\sqrt{n}} \text{msign}\left(W - \eta \frac{\sqrt{m}}{\sqrt{n}} A^* \right)
\end{equation}$$</p>
<h3 id="22-retraction-via-rescaling">2.2. Retraction via rescaling<a hidden class="anchor" aria-hidden="true" href="#22-retraction-via-rescaling">#</a></h3>
<p>Recall that $\texttt{msign}(X) = X (X^T X)^{-1/2}$ and note that,</p>
<p>$$\begin{align*}
(W - \eta A^*)^T (W - \eta A^*)
&amp;= \underbrace{W^T W}_{=I_n} - \eta \underbrace{((A^*)^T W + W^T A^*)}_{=0} + \eta^2 \underbrace{(A^*)^T A^*}_{=I_n} \\
&amp;= (1 + \eta^2) I_n
\end{align*}$$</p>
<p>Thus we can rewrite the update rule for steepest descent on the unscaled Stiefel manifold as,
$$W \leftarrow \frac{W - \eta A^*}{\sqrt{1 + \eta^2}}$$
and for the scaled Stiefel manifold as,
$$W \leftarrow \frac{W - \eta \frac{\sqrt{m}}{\sqrt{n}} A^*}{\sqrt{1 + \eta^2}}$$</p>
<h2 id="3-equivalence-between-bernsteins-and-sus-solutions">3. Equivalence between Bernstein&rsquo;s and Su&rsquo;s solutions<a hidden class="anchor" aria-hidden="true" href="#3-equivalence-between-bernsteins-and-sus-solutions">#</a></h2>
<p>Bernstein (2025a) and Su (2025) found the following solutions to the square and full-rank case,</p>
<p>$$\begin{align*}
A^*_{\text{bernstein}} &amp;= W \texttt{msign}(\texttt{skew}(W^TG))\\
A^*_{\text{su}} &amp;= \texttt{msign}(G - W\texttt{sym}(W^T G))
\end{align*}$$
where $\texttt{sym}(X) = \frac{1}{2}(X + X^T)$ and $\texttt{skew}(X) = \frac{1}{2}(X - X^T)$.</p>
<p>We will show that these are equivalent, i.e., $A^*_{\text{bernstein}} = A^*_{\text{su}}$. For this, we will reuse the following proposition we discussed in a <a href="../spectral-clipping">previous post on spectral clipping</a>.</p>
<blockquote>
<p><strong>Proposition 1 (Transpose Equivariance and Unitary Multiplication Equivariance of Odd Matrix Functions)</strong>. Let $W \in \mathbb{R}^{m \times n}$ and $W = U \Sigma V^T$ be its reduced SVD. And let $f: \mathbb{R}^{m \times n} \to \mathbb{R}^{m \times n}$ be an odd analytic matrix function that acts on the singular values of $W$ as follows,
$$f(W) = U f(\Sigma) V^T.$$
Then $f$ is equivariant under transposition and unitary multiplication, i.e.,
$$\begin{align*}
f(W^T) &amp;= f(W)^T \\
f(WQ^T) &amp;= f(W)Q^T \quad\forall Q \in \mathbb{R}^{m \times n} \text{ such that } Q^TQ = I_n \\
f(Q^TW) &amp;= Q^Tf(W) \quad\forall Q \in \mathbb{R}^{m \times n} \text{ such that } QQ^T = I_m
\end{align*}$$</p>
</blockquote>
<p>Thus,</p>
<p>$$\begin{align*}
A^*_{\text{bernstein}}
&amp;= W \texttt{msign}(\texttt{skew}(W^TG)) \\
&amp;= \texttt{msign}(W \texttt{skew}(W^TG)) &amp;\text{(from Proposition 1)} \\
&amp;= \texttt{msign}\left(\frac{1}{2}W W^T G - \frac{1}{2}W G^T W \right) \\
&amp;= \texttt{msign}\left(W W^T G - \frac{1}{2}W W^T G - \frac{1}{2}W G^T W \right) \\
&amp;= \texttt{msign}\left(G - W\texttt{sym}(W^T G) \right) \\
A^*_{\text{bernstein}} &amp;= A^*_{\text{su}}
\end{align*}$$
where the second-to-last equality relies on $W$ being square and full-rank, which then implies that $W W^T = I_m$.</p>
<h2 id="4-projection-projection-perspective">4. Projection-projection perspective<a hidden class="anchor" aria-hidden="true" href="#4-projection-projection-perspective">#</a></h2>
<p>One can interpret Bernstein&rsquo;s and Su&rsquo;s solutions as a two-step projection process:</p>
<ol>
<li>(Orthogonal) projection to the tangent space at $W$; and</li>
<li>Projection to the closest semi-orthogonal matrix (i.e., closest point on the Stiefel manifold).</li>
</ol>
<p>This is because the map $G \to G - W \texttt{sym}(W^T G)$ is actually the orthogonal projection onto the tangent space at $W$ and that $\texttt{msign}$ projects the resulting matrix to the stiefel manifold. We present these more rigorously as follows.</p>
<blockquote>
<p><strong>Theorem 2 (Orthogonal projection to the tangent space at $W$).</strong> Let $W \in \text{St}(m, n)$ be a point on the Stiefel manifold. The projection of a vector $V \in \mathbb{R}^{m \times n}$ onto the tangent space at $W$, denoted as $T_W\text{St}(m, n)$ , is given by,
$$\begin{equation}
\texttt{proj}_{T_W\text{St}(m, n)}(V) = V - W \text{sym}(W^T V)
\end{equation}$$</p>
</blockquote>


<p><details >
  <summary markdown="span">Show <strong>proof of Theorem 2</strong></summary>
  <blockquote>
<p><strong>Proof.</strong> First, we need to show that the normal space at $W$, $N_W\text{St}(m, n)$ is given by,
$$N_W\text{St}(m, n) = \{WS | S = S^T\}$$
for symmetric $S$. To show this, let $A \in T_W\text{St}(m, n)$ be an arbitrary tangent vector at $W$. Then we have,
$$\begin{align*}
\langle A, WS \rangle &amp;= \text{tr}(A^T WS) \\
&amp;= \text{tr}(S W^T A) &amp;\text{(transpose invariance of trace)} \\
&amp;= -\text{tr}(S A^T W) &amp;\text{(since $A \in T_W\text{St}(m, n)$)} \\
&amp;= -\text{tr}(A^T W S) &amp;\text{(cyclic property of trace)} \\
\langle A, WS \rangle &amp;= -\langle A, WS \rangle
\end{align*}$$
Thus $\langle A, WS \rangle = 0$ which implies that $A$ and $WS$ are orthogonal. Hence $WS \in N_W\text{St}(m, n)$.</p>
<p>Now, for any $V \in \mathbb{R}^{m \times n}$, we can write it as,
$V = \underbrace{V - WS}_{\text{candidate tangent}} + \underbrace{WS}_{\text{candidate normal}}$ for some symmetric $S$. To find $S$,
$$\begin{align*}
W^T (V - WS) + (V - WS)^T W &amp;= 0 \\
W^T V - S + V^TW - S &amp;= 0 \\
S &amp;= \text{sym}(W^T V)
\end{align*}$$
Thus, $V - W \text{sym}(W^T V) \in T_W\text{St}(m, n)$. And because of that, $W^T (V - W \text{sym}(W^T V))$ must be skew-symmetric. Thus,
$$\begin{equation*}
\langle V - WS, WS \rangle
= \langle \underbrace{W^T (V - WS)}_{\text{skew-symmetric}}, \underbrace{S}_{\text{symmetric}} \rangle
= 0
\end{equation*}$$
Hence, $V - W \text{sym}(W^T V)$ is the orthogonal projection of $V$ onto the tangent space at $W$.</p>
</blockquote>

</details></p>

<p>And from Proposition 4 of Bernstein &amp; Newhouse (2024),</p>
<blockquote>
<p><strong>Proposition 3 (Projection to the closest semi-orthogonal matrix).</strong> Consider the orthogonal matrices $\mathcal{O}_{m \times n} := \{ A \in \mathbb{R}^{m \times n} : A A^T = I_m or A^T A = I_n \}$ and let $\| \cdot \|_F$ denote the Frobenius norm. For any matrix $G \in R^{m \times n}$ with reduced SVD $G = U \Sigma V^T$:
$$\arg\min_{A \in \mathcal{O}_{m \times n}} \| A - G \|_F = \texttt{msign}(G) = UV^T,$$
where the minimizer $UV^T$ is unique if and only if the matrix $G$ has full rank.</p>
</blockquote>
<p>Thus we can write,
$$A^*_{\text{bernstein}} = A^*_{\text{su}} = (\texttt{proj}_{\text{St}(m, n)} \circ \texttt{proj}_{T_W\text{St}(m, n)})(G)$$
for the square and full-rank case at least.</p>
<h3 id="41-why-bernsteins--sus-solutions-only-work-for-the-square-and-full-rank-case">4.1. Why Bernstein&rsquo;s &amp; Su&rsquo;s solutions only work for the square and full-rank case<a hidden class="anchor" aria-hidden="true" href="#41-why-bernsteins--sus-solutions-only-work-for-the-square-and-full-rank-case">#</a></h3>
<p>Note that the projections above aim to guarantee both of our criteria for the solution, but one step at a time. And that the $\texttt{msign}$ after the projection may send the resulting matrix outside the tangent space at $W$. We show that this is not a problem in the square and full-rank case, but it is in the general case.</p>
<table>
<thead>
<tr>
<th>operation</th>
<th style="text-align:right">on the tangent space at $W$?</th>
<th style="text-align:right">have unit spectral norm?</th>
</tr>
</thead>
<tbody>
<tr>
<td>(input $G$)</td>
<td style="text-align:right">not in general</td>
<td style="text-align:right">not in general</td>
</tr>
<tr>
<td>1st projection ($\texttt{proj}_{T_W St(m, n)}(\cdot)$)</td>
<td style="text-align:right">${\color{green}\text{yes}}$</td>
<td style="text-align:right">not necessarily</td>
</tr>
<tr>
<td>2nd projection ($\texttt{msign}(\cdot$))</td>
<td style="text-align:right">only for the square<br>and full-rank case</td>
<td style="text-align:right">${\color{green}\text{yes}}$</td>
</tr>
</tbody>
</table>
<p>To demonstrate this, we will need the following proposition.</p>
<blockquote>
<p><strong>Proposition 4 ($\texttt{msign}$ preserves skew-symmetry).</strong> Let $X \in \R^{m \times n}$ be a skew-symmetric matrix. Then $\texttt{msign}(X)$ is also skew-symmetric.</p>
</blockquote>
<p>The proof follows directly from the transpose equivariance and oddness of $\texttt{msign}$, i.e., $\texttt{msign}(X) = \texttt{msign}(-X^T) = -\texttt{msign}(X)^T$.</p>
<p>Also note that for the general case,
$$\begin{equation}
WW^T + QQ^T = I_m \quad\text{and}\quad W^T Q = 0
\end{equation}$$
where $Q$ is the orthonormal complement of $W$.</p>
<p>Thus,
$$\begin{align*}
(\texttt{proj}_{\text{St}(m, n)} \circ \texttt{proj}_{T_W St(m, n)})(G)
&amp;= \texttt{msign}(G - \frac{1}{2}W W^T G - \frac{1}{2} W G^T W) \\
&amp;= \texttt{msign}((WW^T + QQ^T)G - \frac{1}{2}W W^T G - \frac{1}{2} W G^T W) \\
&amp;= \texttt{msign}(W \texttt{skew}(W^T G) + QQ^T G)
\end{align*}$$
For the square and full-rank case, we have $Q = 0$ and $W W^T = I$. And the above simplifies to Berstein&rsquo;s solution,
$$(\texttt{proj}_{\text{St}(m, n)} \circ \texttt{proj}_{T_W St(m, n)})(G) = W \texttt{msign}(\texttt{skew}(W^T G))$$
and since $\texttt{skew}(W^T G)$ is skew-symmetric, then $\texttt{msign}(\texttt{skew}(W^T G))$ must be too. And thus,
$$\begin{align*}
&amp;W^T (\texttt{proj}_{\text{St}(m, n)} \circ \texttt{proj}_{T_W St(m, n)})(G) + (\texttt{proj}_{\text{St}(m, n)} \circ \texttt{proj}_{T_W St(m, n)})(G)^T W \\
&amp;\qquad= W^T W \texttt{msign}(\texttt{skew}(W^T G)) + (W\texttt{msign}(\texttt{skew}(W^T G)))^T W \\
&amp;\qquad= \texttt{msign}(\texttt{skew}(W^T G)) + \texttt{msign}(\texttt{skew}(W^T G))^T \\
&amp;\qquad= 0
\end{align*}$$
Hence for the square and full-rank case, this two-step projection process guarantees that the resulting matrix has unit spectral norm <em>and</em> is in the tangent space at $W$.</p>
<p>For the more general case, we may no longer have $Q = 0$ or $W W^T = I$. Thus, we cannot guarantee that $W \texttt{skew}(W^T G) + QQ^T G$ is skew-symmetric. And so the $\texttt{msign}$ after the first projection may send the resulting matrix outside the tangent space at $W$.</p>
<h2 id="5-heuristic-solutions-for-the-general-case">5. Heuristic solutions for the general case<a hidden class="anchor" aria-hidden="true" href="#5-heuristic-solutions-for-the-general-case">#</a></h2>
<h3 id="51-alignment-upper-bound">5.1. Alignment upper bound<a hidden class="anchor" aria-hidden="true" href="#51-alignment-upper-bound">#</a></h3>
<p>As established previously, for any $G \in \mathbb{R}^{m \times n}$, the solution to $\arg\max_{A \in \text{St}(m, n)} \langle G, A \rangle$ is $A^* = \texttt{proj}_{\text{St}(m, n)}(G) = \texttt{msign}(G)$ and thus $\max_{A \in \text{St}(m, n)} \langle G, A \rangle = \| G \|_{\text{nuc}}$ where $\| \cdot \|_{\text{nuc}}$ is the nuclear norm. With an extra linear constraint $A \in T_W \text{St}(m, n)$, we have,</p>
<p>$$\begin{align*}
\max_{A \in \text{St}(m, n) \cap T_W \text{St}(m, n)} \langle G, A \rangle
&amp;= \max_{A \in \text{St}(m, n) \cap T_W \text{St}(m, n)} \langle \underbrace{G - \texttt{proj}_{T_W \text{St}(m, n)}(G)}_{\in N_W\text{St}(m, n)} + \texttt{proj}_{T_W \text{St}(m, n)}(G), A \rangle \\
&amp;= \max_{A \in \text{St}(m, n) \cap T_W \text{St}(m, n)} \langle \texttt{proj}_{T_W \text{St}(m, n)}(G), A \rangle \\
&amp;\leq \max_{A \in \text{St}(m, n)} \langle \texttt{proj}_{T_W \text{St}(m, n)}(G), A \rangle \\
\max_{A \in \text{St}(m, n) \cap T_W \text{St}(m, n)} \langle G, A \rangle
&amp;\leq \| \texttt{proj}_{T_W \text{St}(m, n)}(G) \|_{\text{nuc}}
\end{align*}$$</p>
<p>The alignment between an element in the normal space and an element in the tangent space is always zero, i.e., $\langle \texttt{proj}_{T_W \text{St}(m, n)}(G), \texttt{proj}_{N_W \text{St}(m, n)}(G) \rangle = 0$. Thus, we can cancel out the first term in the first equality. And we have first inequality because the max over a smaller set is less than or equal to the max over a larger set.</p>
<p>We achieve equality in the square and full-rank case because the maximizer for the first inequality is guaranteed to be in the tangent space at $W$, as discussed in the previous section.</p>
<h3 id="52-fixed-point-iteration-of-alternating-projections">5.2. Fixed-point iteration of alternating projections<a hidden class="anchor" aria-hidden="true" href="#52-fixed-point-iteration-of-alternating-projections">#</a></h3>
<p>Notice that $QQ^T G$ is the projection of $G$ onto the column space of $Q$, $\texttt{proj}_{\text{col}(Q)}(G) = QQ^T G$. One can think of this as the component of $G$ that is, in a sense, <em>not</em> &ldquo;aligned to&rdquo; $W$. In practice, this is typically small relative to the component of $G$ that <em>is</em> &ldquo;aligned to&rdquo; $W$. If so, then,
$$(\texttt{proj}_{\text{St}(m, n)} \circ \texttt{proj}_{T_W St(m, n)})(G) \approx \texttt{msign}(W \texttt{skew}(W^T G) + \cancel{\texttt{proj}_{\text{col}(Q)}(G)}),$$
which means that while the resulting matrix after the two projections may not be in the tangent space at $W$, it would likely be <em>nearby</em>. And repeating this process a few times should close the gap.</p>
<p>Sample implementation:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">project_to_stiefel_tangent_space</span>(X, delta_X):
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> delta_X - X @ sym(X.T @ delta_X)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">orthogonalize</span>(X):
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># copy Newton-Schulz iteration from Muon (Jordan et al., 2024)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">steepest_descent_stiefel_manifold_heuristic</span>(W, G, num_steps=<span style="color:#099">1</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#00a">assert</span> num_steps &gt; <span style="color:#099">0</span>, <span style="color:#a50">&#34;Number of steps must be positive&#34;</span>
</span></span><span style="display:flex;"><span>    A_star = G
</span></span><span style="display:flex;"><span>    <span style="color:#00a">for</span> _ <span style="color:#00a">in</span> <span style="color:#0aa">range</span>(num_steps):
</span></span><span style="display:flex;"><span>        A_star = project_to_stiefel_tangent_space(W, A_star)
</span></span><span style="display:flex;"><span>        A_star = orthogonalize(A_star)
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> A_star
</span></span></code></pre></div><h4 id="521-local-linear-convergence-guarantee">5.2.1. Local (linear) convergence guarantee<a hidden class="anchor" aria-hidden="true" href="#521-local-linear-convergence-guarantee">#</a></h4>
<p>For now, we cannot yet guarantee global convergence as it would potentially require deriving the Lyapunov function for this iteration which is extremely difficult. What we can guarantee, however, is <em>local</em> convergence due to $\text{St}(m, n)$ and $T_W\text{St}(m, n)$ being closed semi-algebraic sets. That is, assuming that the initial point $W$ is &ldquo;close enough&rdquo; to the intersection, we can guarantee that a subsequence of the iterates converges to a point in the intersection. Furthermore, assuming transversality, we can guarantee that the convergence is linear.</p>
<blockquote>
<p><strong>Definition 5 (Semi-algebraic set)</strong> Let $\mathbb{F}$ be a real closed field. A subset $S$ of $\mathbb{F}^n$ is a semi-algebraic set if it is a finite union of sets defined by polynomial equations and inequalities.</p>
</blockquote>
<p>We can construct $\text{St}(m, n) = \{ W \in \mathbb{R}^{m \times n} | W^T W = I_n \}$ via $n(n+1)/2$ polynomial equations, and $T_W\text{St}(m, n) = \{ A \in \mathbb{R}^{m \times n} | W^T A + A^T W = 0 \}$ via $mn$ polynomial equations. Hence both are semi-algebraic sets. And the intersection of two semi-algebraic sets is also a semi-algebraic set. From Theorem 7.3 of Drusvyatskiy et al. (2016), we then have the following convergence guarantee.</p>
<blockquote>
<p><strong>Theorem 6 (Convergence of alternating projections on semi-algebraic sets)</strong> Consider two nonempty closed semi-algebraic sets $X, Y \subset E$ with $X$ bounded. If the method of alternating projections starts in $Y$ and near $X$, then the distance of the iterates to the intersection $X \cap Y$ converges to zero, and hence every limit point lies in $X \cap Y$.</p>
</blockquote>
<p>Setting $Y = T_W\text{St}(m, n)$, $X = \text{St}(m, n)$, and noting that the Stiefel manifold is bounded, we have that the iterates of our method of alternating projections converge to a point in the intersection $\text{St}(m, n) \cap T_W\text{St}(m, n)$.</p>
<p>But does this algorithm converge in sufficient time? Somewhat yes, we can guarantee local <em>linear</em> convergence assuming transversality.</p>
<blockquote>
<p><strong>Definition 7 (Transversality)</strong> Let $\mathcal{M}$ and $\mathcal{N}$ be two (smooth) manifolds in a Euclidean space. We say that $\mathcal{M}$ and $\mathcal{N}$ intersect transversally at a point $x \in \mathcal{M} \cap \mathcal{N}$ when,
$$N_x\mathcal{M} \cap N_x\mathcal{N} = \{0\}.$$</p>
</blockquote>
<p>Intuitively speaking, this means that the tangent spaces at the intersection of the two manifolds have an &ldquo;angle&rdquo; between them, i.e., they are not parallel. The larger this angle is, the faster the convergence; and the smaller the angle, the slower the convergence. Theorem 2.1 of Drusvyatskiy et al. (2016) then gives us the following local linear convergence guarantee.</p>
<blockquote>
<p><strong>Theorem 8 (Linear convergence of alternating projections, assuming transversality)</strong> If two closed sets in a Euclidean space intersect transversally at a point $\tilde{x}$, then the method of alternating projections, started nearby, converges linearly to a point in the intersection.</p>
</blockquote>
<p>$\text{St}(m, n)$ and $T_W\text{St}(m, n)$ are both closed sets so the theorem above applies.</p>
<h3 id="53-ternary-search-over-nearby-feasible-solutions">5.3. Ternary search over nearby feasible solutions<a hidden class="anchor" aria-hidden="true" href="#53-ternary-search-over-nearby-feasible-solutions">#</a></h3>
<p>Here we present an alternative solution that is more efficient, but often yields slightly more suboptimal results than the fixed-point iteration method above.</p>
<h4 id="531-problem-decomposition">5.3.1. Problem decomposition<a hidden class="anchor" aria-hidden="true" href="#531-problem-decomposition">#</a></h4>
<p>The crux is to split $\arg\max_{A\in \mathbb{R}^{m \times n}} \langle G, A \rangle$ into two optimization problems, one for the component of $G$ that is &ldquo;aligned to&rdquo; $W$ and one for the component of $G$ that is &ldquo;not aligned to&rdquo; $W$. To see this, let us first decompose $G$ and $A$ into,
$$G = W G_W + Q G_Q \qquad A = WB + QC$$
where $G_W = W^T G$ and $G_Q = Q^T G$. Thus,
$$\begin{align}
\langle G, A \rangle
&amp;= \langle W G_W + Q G_Q, WB + QC \rangle \nonumber \\
&amp;= \text{tr}((W G_W + Q G_Q)^T (WB + QC)) \nonumber \\
&amp;= \text{tr}(G_W^T B + G_Q^T C) \nonumber \\
\langle G, A \rangle
&amp;= \langle G_W, B \rangle + \langle G_Q, C \rangle \\
\end{align}$$
The cross terms vanish in the third equality because $W^T Q = 0$. Finding the maximizer $A^*$ for the LHS is then equivalent to finding the maximizers $B^*$ and $C^*$ for the RHS and then combining them,
$$A^* = WB^* + QC^*.$$</p>
<h4 id="532-solving-the-two-subproblems">5.3.2. Solving the two subproblems<a hidden class="anchor" aria-hidden="true" href="#532-solving-the-two-subproblems">#</a></h4>
<p>Now, to satisfy the constraint $A \in T_W\text{St}(m, n) \cap \text{St}(m, n)$,
$$\begin{align*}
W^T A + A^T W &amp;= 0 \qquad\qquad&amp; A^T A &amp;= I_n \\
W^T (WB + QC) + (WB + QC)^T W &amp;= 0 \qquad\qquad&amp; (WB + QC)^T(WB + QC) &amp;= I_n \\
B + B^T &amp;= 0 \qquad\qquad&amp; B^T B + C^T C &amp;= I_n \\
\end{align*}$$
That is, we require that $B$ is skew-symmetric and $C$ satisfies $C^T C = I_n - B^T B$.</p>
<p>We cannot simply treat each subproblem separately because the second constraint couples $B$ and $C$. However, for this approximation, we make the assumption that doing so would yield a &ldquo;good enough&rdquo; solution.</p>
<hr>
<p>For the first term, we can decompose $G_W$ into its skew-symmetric and symmetric components, $G_W = \texttt{skew}(G_W) + \texttt{sym}(G_W)$,
$$\begin{align*}
\arg\max_{B \text{ is skew}} \langle G_W, B \rangle
&amp;= \arg\max_{B \text{ is skew}} \langle \texttt{skew}(G_W) + \texttt{sym}(G_W), B \rangle \\
&amp;= \arg\max_{B \text{ is skew}} \langle \texttt{skew}(G_W), B \rangle + \cancel{\langle \texttt{sym}(G_W), B \rangle}
\end{align*}$$
And the maximizer is simply $\tilde{B} = \texttt{skew}(G_W) = \texttt{skew}(W^T G)$.</p>
<p>However, because of the constraint $B^T B + C^T C = I$, we have to &ldquo;cap&rdquo; the spectral norm of $B$ to be less than or equal to 1 otherwise we would fail to construct a real-valued $C$. We can do this via a variety of methods discussed in a <a href="../spectral-clipping/">previous post on spectral clipping</a> and our latest paper on <a href="https://arxiv.org/abs/2507.13338" target="_blank">training transformers with enforced Lipschitz bounds</a> (Newhouse*, Hess*, Cesista* et al., 2025). Let $\tau \leq 1$ be the spectral norm bound, then we can choose,</p>
<p>$$B^*(\tau) := \texttt{hard\_cap}_{\tau}(\tilde{B})
\quad\text{or}\quad B^*(\tau) := \tau\cdot\texttt{msign}(\tilde{B})
\quad\text{or}\quad B^*(\tau) := \frac{\tau}{\| \tilde{B} \|_{2 \to 2}}\tilde{B}$$</p>
<p>These mappings preserve skew-symmetry and thus $B^*(\tau)$ satisfies the first constraint.</p>
<hr>
<p>Now, parametrize $C$ as $C = UR$ where $U^T U = I$ and $R(\tau) = (I_n - (B^*(\tau))^T B^*(\tau))^{1/2}$. It is trivial to check that $C$ satisfies our constraints and that $R(\tau)$ is SPD. Thus, assuming we already have a fixed $B^*(\tau)$ (and consequently a fixed $R(\tau)$), solving the second subproblem,
$$\arg\max_{C} \langle G_Q, C \rangle$$
is equivalent to solving,
$$\begin{align*}
\arg\max_{U: U^T U = I} \langle G_Q, U R(\tau) \rangle
&amp;= \arg\max_{U: U^T U = I} \text{tr}(G_Q^T U R(\tau)) \\
&amp;= \arg\max_{U: U^T U = I} \text{tr}(R(\tau) G_Q^T U) \\
&amp;= \arg\max_{U: U^T U = I} \langle G_Q R(\tau), U \rangle
\end{align*}$$
which has maximizer $U^* = \texttt{msign}(G_Q R(\tau))$. Thus, $C^*(\tau) = \texttt{msign}(G_Q R(\tau)) R(\tau)$.</p>
<hr>
<p>Note that for the square and full-rank case, we have $C = 0$ and so we require $B^T B = I$. For this, we can choose to orthogonalize $B = \texttt{skew}(G_W)$ which then yields Bernstein&rsquo;s solution,
$$A^*_{\text{bernstein}} = W \texttt{msign}(\texttt{skew}(W^T G))$$
This also motivates the choice of $\texttt{msign}$ as the normalization method for $B$ more generally.</p>
<p>We can implement this in JAX as follows,</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">from</span> <span style="color:#0aa;text-decoration:underline">spectral_clipping</span> <span style="color:#00a">import</span> spectral_hardcap, spectral_normalize, orthogonalize
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">matsqrt</span>(W: jax.Array):
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># We can also compute this via Newton-Schulz iteration or Cholesky decomposition</span>
</span></span><span style="display:flex;"><span>    U, s, Vh = jnp.linalg.svd(W, full_matrices=<span style="color:#00a">False</span>)
</span></span><span style="display:flex;"><span>    s_sqrt = jnp.sqrt(s)
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> U @ jnp.diag(s_sqrt) @ Vh
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">construct_nearby_feasible_solution</span>(W, Q, G, tau=<span style="color:#099">0.5</span>, normalizer_method=<span style="color:#099">0</span>):
</span></span><span style="display:flex;"><span>    m, n = W.shape
</span></span><span style="display:flex;"><span>    <span style="color:#00a">if</span> m == n:
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># assumes full-rank</span>
</span></span><span style="display:flex;"><span>        A = W @ orthogonalize(skew(W.T @ G))  <span style="color:#aaa;font-style:italic"># Bernstein&#39;s solution</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a">else</span>:
</span></span><span style="display:flex;"><span>        B = skew(W.T @ G)
</span></span><span style="display:flex;"><span>        <span style="color:#00a">if</span> normalizer_method == <span style="color:#099">0</span>:
</span></span><span style="display:flex;"><span>            B_tilde = tau * orthogonalize(B)
</span></span><span style="display:flex;"><span>        <span style="color:#00a">elif</span> normalizer_method == <span style="color:#099">1</span>:
</span></span><span style="display:flex;"><span>            B_tilde = spectral_hardcap(B, tau)
</span></span><span style="display:flex;"><span>        <span style="color:#00a">elif</span> normalizer_method == <span style="color:#099">2</span>:
</span></span><span style="display:flex;"><span>            B_tilde = spectral_normalize(B, tau)
</span></span><span style="display:flex;"><span>        R = jnp.linalg.cholesky(jnp.eye(n) - B_tilde.T @ B_tilde)
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># R = matsqrt(jnp.eye(n) - B_tilde.T @ B_tilde)</span>
</span></span><span style="display:flex;"><span>        C = orthogonalize(Q.T @ G @ R) @ R
</span></span><span style="display:flex;"><span>        A = W @ B_tilde + Q @ C
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> A
</span></span></code></pre></div><h4 id="531-where-ternary-search-comes-in">5.3.1. Where ternary search comes in<a hidden class="anchor" aria-hidden="true" href="#531-where-ternary-search-comes-in">#</a></h4>
<p><img loading="lazy" src="/ponder/steepest-descent-stiefel/alignment-unimodal.png#center"></p>
<p>From Equation (7), we have,</p>
<p>$$\begin{align*}
f(\tau) := \langle G, A^*(\tau) \rangle
&amp;= \langle G_W, B^*(\tau) \rangle + \langle G_Q, C^*(\tau) \rangle \\
&amp;= \langle \texttt{skew}(G_W), B^*(\tau) \rangle + \langle G_Q, C^*(\tau) \rangle \\
&amp;= \langle \texttt{skew}(G_W), \texttt{normalized}_{\tau}(\texttt{skew}(G_W)) \rangle + \langle G_Q, \texttt{msign}(G_Q R(\tau)) R(\tau) \rangle \\
&amp;= \langle \texttt{skew}(G_W), \texttt{normalized}_{\tau}(\texttt{skew}(G_W)) \rangle + \| G_QR(\tau) \|_{\text{nuc}} \\
\end{align*}$$</p>
<p>The first term is linear and positively-sloped as we vary $\tau$. And since the map $x \mapsto \sqrt{1 - x^2}$ is concave and non-increasing in $x \in [0, 1]$, the second term must be concave and non-increasing as we increase $\tau$. Taken together, $f$ must be unimodal. And thus we can use ternary search to find the optimal $\tau$ that maximizes $f(\tau)$.</p>
<p>We can implement this in JAX as follows,</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">ternary_search_over_taus</span>(W, Q, G, lo=<span style="color:#099">0.</span>, hi=<span style="color:#099">1.</span>, normalizer_method=<span style="color:#099">0</span>, max_iter=<span style="color:#099">10</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#00a">def</span> <span style="color:#0a0">evaluate</span>(tau):
</span></span><span style="display:flex;"><span>        A = construct_nearby_feasible_solution(W, Q, G, tau, normalizer_method)
</span></span><span style="display:flex;"><span>        <span style="color:#00a">return</span> jnp.trace(G.T @ A)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a">def</span> <span style="color:#0a0">body_fun</span>(i, val):
</span></span><span style="display:flex;"><span>        lo, hi = val
</span></span><span style="display:flex;"><span>        mid1 = (<span style="color:#099">2</span>*lo + hi) / <span style="color:#099">3</span>
</span></span><span style="display:flex;"><span>        mid2 = (lo + <span style="color:#099">2</span>*hi) / <span style="color:#099">3</span>
</span></span><span style="display:flex;"><span>        f_mid1 = evaluate(mid1)
</span></span><span style="display:flex;"><span>        f_mid2 = evaluate(mid2)
</span></span><span style="display:flex;"><span>        new_lo = jnp.where(f_mid1 &gt; f_mid2, lo, mid1)
</span></span><span style="display:flex;"><span>        new_hi = jnp.where(f_mid1 &gt; f_mid2, mid2, hi)        
</span></span><span style="display:flex;"><span>        <span style="color:#00a">return</span> new_lo, new_hi
</span></span><span style="display:flex;"><span>    final_lo, final_hi = jax.lax.fori_loop(<span style="color:#099">0</span>, max_iter, body_fun, (lo, hi))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># Compute final midpoint and its value</span>
</span></span><span style="display:flex;"><span>    final_tau = (final_lo + final_hi) / <span style="color:#099">2</span>
</span></span><span style="display:flex;"><span>    final_value = evaluate(final_tau)
</span></span><span style="display:flex;"><span>    <span style="color:#00a">return</span> final_tau, final_value
</span></span></code></pre></div><h2 id="6-bonus-a-muon-like-optimizer-for-the-embedding-and-unembedding-layers">6. Bonus: a Muon-like optimizer for the Embedding and Unembedding layers<a hidden class="anchor" aria-hidden="true" href="#6-bonus-a-muon-like-optimizer-for-the-embedding-and-unembedding-layers">#</a></h2>
<p>Embedding layers have a hidden geometry: the (scaled-)Oblique manifold, $\widetilde{\text{Ob}}(m, n) = \{ W \in \mathbb{R}^{m \times n} | \text{diag}(W^T W) = s^2\mathbf{1}_n \}$ with scale $s = \sqrt{m}$, or the manifold of matrices with unit-RMS-norm columns. More precisely, it is the embedding layer <em>and</em> the normalization layer right after it that results in unit-RMS-norm feature-vectors. But optimizers like Adam typically ignore this geometry and even its matrix-structure, treating the embedding layer the same as &lsquo;flat&rsquo; vectors. We believe this leads to suboptimal performance and demonstrate this via grokking experiments we discuss in the next section.</p>
<p>What if we build an optimizer that respects this geometry?</p>
<p>For this, we need two things:</p>
<ol>
<li>A &lsquo;dualizer&rsquo; map that maps a &ldquo;raw gradient&rdquo; matrix $G \in \mathbb{R}^{m \times n}$ to an update direction of steepest descent on the tangent space at $W \in \widetilde{\text{Ob}}(m, n)$, i.e., $A^* \in T_W\widetilde{\text{Ob}}(m, n)$ with $\| A^* \| = 1$ for some norm $\| \cdot \|$ chosen a priori. And,</li>
<li>A &lsquo;projection&rsquo; or retraction map that maps an (updated) weight matrix $W \in \mathbb{R}^{m \times n}$ back to the (scaled-)Oblique manifold.</li>
</ol>
<p>The retraction map is simply the column-wise normalization,
$$\texttt{col\_normalize}(W) := \text{col}_j(W) \mapsto \frac{\text{col}_j(W)}{\| \text{col}_j(W) \|_{RMS}} = \sqrt{m}\frac{\text{col}_j(W)}{\| \text{col}_j(W) \|_{2}} \quad \forall 0 \leq j &lt; n$$
where $\text{col}_j(W)$ is the $j$-th column of the weight matrix $W$.</p>
<p>As for the dualizer, which norm should we use? We can, for example, use the RMS-to-RMS norm for consistency and still be able to use the same alternating projection method as before. However, as argued by Bernstein &amp; Newhouse (2024) and Pethick et al. (2024), it may be more natural to use the L1-to-RMS norm, $\| \cdot \|_{1\to RMS}$ because the maximizer for the following problem,
$$\arg\max_{A: \| A \|_{1 \to RMS} = 1} \langle G, A \rangle$$
is simply $\texttt{col\_normalize}(G) \in \widetilde{\text{Ob}}(m, n)$. That is, all of the token embedding updates would have the same size, improving training stability. Thus our update rule becomes,
$$ W \leftarrow \texttt{col\_normalize}(W - \eta A^*)$$
where $\eta$ is the learning rate and,
$$ A^* = \arg\max_{A: \| A \|_{1 \to RMS} = 1} \langle G, A \rangle \quad \text{s.t. } A \in T_W\widetilde{\text{Ob}}(m, n),$$</p>
<p>Equivalently,
$$A^* = \arg\max_{A} \langle G, A \rangle  \quad \text{s.t. } A \in \widetilde{\text{Ob}}(m, n) \cap T_W \widetilde{\text{Ob}}(m, n),$$
or in words, we want to find a descent direction $A^*$ that is both on the (scaled-)Oblique manifold and in the tangent space at $W$ that maximizes the alignment with the &ldquo;raw gradient&rdquo; $G$.</p>
<h3 id="61-optimal-solution-for-steepest-descent-on-the-scaled-oblique-manifold">6.1. Optimal solution for steepest descent on the (scaled-)Oblique manifold<a hidden class="anchor" aria-hidden="true" href="#61-optimal-solution-for-steepest-descent-on-the-scaled-oblique-manifold">#</a></h3>
<p>The Oblique manifold is a product of hyperspheres, $\text{Ob}(m, n) = \underbrace{S^m \times \ldots \times S^m}_{n}$. So, in a sense, the columns are acting independently of each other and steepest descent on the Oblique manifold is equivalent to steepest descent on the hypersphere, applied column-wise. Generalizing Bernstein&rsquo;s (2025b) dualizer for steepest descent on the hypersphere to the Oblique manifold then yields,</p>
<blockquote>
<p>The optimal solution for finding the direction of steepest descent on the Oblique manifold $A^*$ given &ldquo;raw Euclidean gradient&rdquo; or differential $G$ is to simply project $G$ onto the tangent space at point $W \in \widetilde{\text{Ob}}(m, n)$ and then normalize column-wise.</p>
</blockquote>
<p>The tangent space at $W$ is simply,
$$T_W\widetilde{\text{Ob}}(m, n) = \{A \in \mathbb{R}^{m \times n} | \text{diag}(W^T A) = 0\}$$
or in words, the column-wise dot-product or &ldquo;alignment&rdquo; between $W$ and a candidate tangent vector $A$ must be zero for $A$ to be in the tangent space at $W$. The projector onto the tangent space at $W$ is then given by,
$$\texttt{proj}_{T_W\widetilde{\text{Ob}}(m, n)}(G) = G - W \text{diag}(W^T G / m)$$
or in words, we subtract the component of $G$ that is &ldquo;aligned to&rdquo; $W$.</p>
<p>Notice then that one of the constraints is concerned with the <em>size</em> of the columns while the other is concerned with the <em>direction</em>. These can be optimized independently of each other. Thus, the solution for $A^*$ is then simply,
$$A^* = \texttt{col\_normalize}(\texttt{proj}_{T_W\widetilde{\text{Ob}}(m, n)}(G))$$</p>
<h3 id="62-steepest-descent-on-the-scaled-row-oblique-manifold">6.2. Steepest descent on the (scaled-)Row-Oblique manifold<a hidden class="anchor" aria-hidden="true" href="#62-steepest-descent-on-the-scaled-row-oblique-manifold">#</a></h3>
<p>We argue that the Unembedding layer or the &rsquo;language model head&rsquo; should naturally be on the (scaled-)Row-Oblique manifold, $\widetilde{\text{RowOb}}(m, n) = \{ W \in \mathbb{R}^{m \times n} | \text{diag}(WW^T) = s^2\mathbf{1}_m \}$ with scale $s = \sqrt{n}$, or the manifold of matrices with unit-RMS-norm rows. The crux is that the logit for the $i$-th vocabulary token is given by the dot-product or &lsquo;alignment&rsquo; between the $i$-th row of the weight matrix and the feature vector. So if the logits measure &lsquo;alignment&rsquo;, not &lsquo;size&rsquo;, then it is natural to constrain the rows to have unit-RMS-norm.</p>
<p>And since we can construct $\widetilde{\text{RowOb}}(m, n)$ by transposing $\widetilde{\text{Ob}}(m, n)$, we can use the same reasoning as above to derive the optimal solution for steepest descent on the (scaled-)Row-Oblique manifold.</p>
<p>Our retraction map is simply the row-wise normalization,
$$\texttt{row\_normalize}(W) := \text{row}_i(W) \mapsto \frac{\text{row}_i(W)}{\| \text{row}_i(W) \|_{RMS}} = \sqrt{n}\frac{\text{row}_i(W)}{\| \text{row}_i(W) \|_{2}} \quad \forall 0 \leq i &lt; m$$
where $\text{row}_i(W)$ is the $i$-th row of the weight matrix $W$. We then choose the $\| \cdot \|_{RMS \to \infty}$ norm because the maximizer for the following problem,
$$\arg\max_{A: \| A \|_{RMS \to \infty} = 1} \langle G, A \rangle$$
is simply $\texttt{row\_normalize}(G) \in \widetilde{\text{RowOb}}(m, n)$. That is, the per-row updates would have even size.</p>
<p>Our update rule then becomes,
$$ W \leftarrow \texttt{row\_normalize}(W - \eta A^*)$$
where $\eta$ is the learning rate and,
$$A^* = \arg\max_{A} \langle G, A \rangle  \quad \text{s.t. } A \in \widetilde{\text{RowOb}}(m, n) \cap T_W \widetilde{\text{RowOb}}(m, n),$$
which has the closed form solution,
$$\begin{align*}
A^* &amp;= \texttt{row\_normalize}(\texttt{proj}_{T_W\widetilde{\text{RowOb}}(m, n)}(G)) \\
&amp;= \texttt{row\_normalize}(G - \text{diag}(G W^T / n) W) \\
\end{align*}$$</p>
<h2 id="7-experimental-results">7. Experimental results<a hidden class="anchor" aria-hidden="true" href="#7-experimental-results">#</a></h2>
<h3 id="71-alternating-projections-method-beats-ternary-search-on-nearby-feasible-solutions-on-larger-matrices">7.1. Alternating projections method beats ternary search on nearby feasible solutions on larger matrices<a hidden class="anchor" aria-hidden="true" href="#71-alternating-projections-method-beats-ternary-search-on-nearby-feasible-solutions-on-larger-matrices">#</a></h3>
<p><img loading="lazy" src="/ponder/steepest-descent-stiefel/steepest-descent-stiefel.png#center"></p>
<p><img loading="lazy" src="/ponder/steepest-descent-stiefel/steepest-descent-stiefel-edge.png#center"></p>
<p>Here we compare our two heuristic methods for the problem of spectral-norm constrained steepest descent on the Stiefel manifold. Observe from the figures above that the ternary search over nearby feasible solutions method results in almost optimal solutions, regardless of scale. However, the alternating projections method results in more aligned solutions, albeit at the cost of more compute and being more off-tangent.</p>
<h3 id="72-grokking-on-the-addition-modulo-113-task-in-44-full-batch-training-steps">7.2. Grokking on the Addition-Modulo-113 task in 44 full-batch training steps<a hidden class="anchor" aria-hidden="true" href="#72-grokking-on-the-addition-modulo-113-task-in-44-full-batch-training-steps">#</a></h3>
<blockquote>
<p>We will release the source code soon, but if you want early access, please email me.</p>
</blockquote>
<p><img loading="lazy" src="/ponder/steepest-descent-stiefel/grokking_results.png#center"></p>
<p>We use the same training setup for grokking experiments on the Addition-Modulo-113 problem as in a previous <a href="../spectral-clipping/">post on spectral clipping</a>, with new dualizers and projection maps added. Following Prieto et al. (2025), we use a 2-layer MLP (plus Embedding and Unembedding layers) with 200 hidden units per layer. All matrix multiplications are done in <code>bfloat16</code> precision.</p>
<p>We place our Embedding and Unembedding weights on the (scaled-)Oblique manifold and (scaled-)Row-Oblique manifold, respectively. We then vary the dualizer and retraction maps used in the linear layers and report the best median-steps-to-grokking across 64 random seeds. See figure above for the results.</p>
<p>Interestingly, without weight constraints, models fail to grok within 1000 full-batch training steps. This is true for both the Muon optimizer and AdamW. However, with weight constraints, we were able to achieve grokking in 44 full-batch training steps, which we believe is SOTA.</p>
<p>The best recipe seems to be:</p>
<table>
<thead>
<tr>
<th>layer</th>
<th style="text-align:right">manifold</th>
<th style="text-align:center">$\texttt{dualizer}$</th>
<th style="text-align:center">$\texttt{retract}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Embedding</td>
<td style="text-align:right">(Scaled-)Oblique manifold</td>
<td style="text-align:center">$\texttt{col\_normalize} \circ \texttt{proj}_{T_W\widetilde{\text{Ob}}(m, n)}$</td>
<td style="text-align:center">$\texttt{col\_normalize}$</td>
</tr>
<tr>
<td>1st Linear</td>
<td style="text-align:right">RMS-to-RMS norm ball<br>around the origin of $\mathbb{R}^{m \times n}$</td>
<td style="text-align:center">$\texttt{msign}$</td>
<td style="text-align:center">$\texttt{spectral\_normalize}$</td>
</tr>
<tr>
<td>2nd Linear</td>
<td style="text-align:right">RMS-to-RMS norm ball<br>around the origin of $\mathbb{R}^{m \times n}$</td>
<td style="text-align:center">$\texttt{msign}$</td>
<td style="text-align:center">$\texttt{spectral\_normalize}$</td>
</tr>
<tr>
<td>Unembedding</td>
<td style="text-align:right">(Scaled-)Row-Oblique manifold</td>
<td style="text-align:center">$\texttt{row\_normalize} \circ \texttt{proj}_{T_W\widetilde{\text{RowOb}}(m, n)}$</td>
<td style="text-align:center">$\texttt{row\_normalize}$</td>
</tr>
</tbody>
</table>
<h2 id="acknowledgements">Acknowledgements<a hidden class="anchor" aria-hidden="true" href="#acknowledgements">#</a></h2>
<p>Big thanks to Jianlin Su, Jeremy Bernstein, Vinay Rao, Antonio Silveti-Falls, Mikail Khona, Omead Pooladzandi, Simo Ryu, and Kevin Yin for productive discussions on the topic.</p>
<h2 id="how-to-cite">How to cite<a hidden class="anchor" aria-hidden="true" href="#how-to-cite">#</a></h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bibtex" data-lang="bibtex"><span style="display:flex;"><span><span style="color:#0a0;text-decoration:underline">@misc</span>{cesista2025spectralclipping,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">author</span> = <span style="color:#a50">{Franz Louis Cesista}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">title</span> = <span style="color:#a50">{&#34;Heuristic Solutions for Steepest Descent on the Stiefel Manifold&#34;}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">year</span> = <span style="color:#a50">{2025}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">month</span> = <span style="color:#a50">{July}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">day</span> = <span style="color:#a50">{18}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">url</span> = <span style="color:#a50">{http://leloykun.github.io/ponder/steepest-descent-stiefel/}</span>,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><blockquote>
<p>If you find this post useful, please consider supporting my work by sponsoring me on GitHub: <a href="https://github.com/sponsors/leloykun" target="_blank"><img alt="Sponsor on GitHub" loading="lazy" src="https://img.shields.io/badge/%F0%9F%A4%9D-Sponsor%20me-1da1f2?logo=github&style=flat-square"></a></p>
</blockquote>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ol>
<li>Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein (2024). Muon: An optimizer for hidden layers in neural networks. Available at: <a href="https://kellerjordan.github.io/posts/muon/" target="_blank">https://kellerjordan.github.io/posts/muon/</a></li>
<li>Greg Yang, James B. Simon, Jeremy Bernstein (2024). A Spectral Condition for Feature Learning. URL <a href="https://arxiv.org/abs/2310.17813" target="_blank">https://arxiv.org/abs/2310.17813</a></li>
<li>Tim Large, Yang Liu, Minyoung Huh, Hyojin Bahng, Phillip Isola, Jeremy Bernstein (2024). Scalable Optimization in the Modular Norm. URL <a href="https://arxiv.org/abs/2405.14813" target="_blank">https://arxiv.org/abs/2405.14813</a></li>
<li>Jeremy Bernstein, Laker Newhouse (2024). Old Optimizer, New Norm: An Anthology. URL <a href="https://arxiv.org/abs/2409.20325" target="_blank">https://arxiv.org/abs/2409.20325</a></li>
<li>Laker Newhouse (2025). Understanding Muon. URL <a href="https://www.lakernewhouse.com/writing/muon-1" target="_blank">https://www.lakernewhouse.com/writing/muon-1</a></li>
<li>Laker Newhouse, R. Preston Hess, Franz Cesista, Andrii Zahorodnii, Jeremy Bernstein, Phillip Isola (2025). Training Transformers with Enforced Lipschitz Constants. URL <a href="https://arxiv.org/abs/2507.13338" target="_blank">https://arxiv.org/abs/2507.13338</a></li>
<li>Jeremy Bernstein (2025a). Orthogonal manifold. URL <a href="https://docs.modula.systems/algorithms/manifold/orthogonal/" target="_blank">https://docs.modula.systems/algorithms/manifold/orthogonal/</a></li>
<li>Jeremy Bernstein (2025b). Hypersphere. URL <a href="https://docs.modula.systems/algorithms/manifold/hypersphere/" target="_blank">https://docs.modula.systems/algorithms/manifold/hypersphere/</a></li>
<li>Jianlin Su (2025). Steepest descent on Stiefel manifold. URL <a href="https://x.com/YouJiacheng/status/1945522729161224532" target="_blank">https://x.com/YouJiacheng/status/1945522729161224532</a></li>
<li>D. Drusvyatskiy, A.D. Ioffe, A.S. Lewis (2016). Transversality and alternating projections for nonconvex sets. URL <a href="https://arxiv.org/abs/1401.7569" target="_blank">https://arxiv.org/abs/1401.7569</a></li>
<li>Thomas Pethick, Wanyun Xie, Kimon Antonakopoulos, Zhenyu Zhu, Antonio Silveti-Falls, Volkan Cevher (2025). Training Deep Learning Models with Norm-Constrained LMOs. URL <a href="https://arxiv.org/abs/2502.07529" target="_blank">https://arxiv.org/abs/2502.07529</a></li>
<li>Bin Gao, Simon Vary, Pierre Ablin, P.-A. Absil (2022). Optimization flows landing on the Stiefel manifold. URL <a href="https://arxiv.org/abs/2202.09058" target="_blank">https://arxiv.org/abs/2202.09058</a></li>
<li>Pierre Ablin, Gabriel Peyré (2021). Fast and accurate optimization on the orthogonal manifold without retraction. URL <a href="https://arxiv.org/abs/2102.07432" target="_blank">https://arxiv.org/abs/2102.07432</a></li>
<li>Lucas Prieto, Melih Barsbey, Pedro A.M. Mediano, Tolga Birdal (2025). Grokking at the Edge of Numerical Stability. URL <a href="https://arxiv.org/abs/2501.04697" target="_blank">https://arxiv.org/abs/2501.04697</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://leloykun.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://leloykun.github.io/tags/optimizers/">Optimizers</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Heuristic Solutions for Steepest Descent on the Stiefel Manifold on x"
            href="https://x.com/intent/tweet/?text=Heuristic%20Solutions%20for%20Steepest%20Descent%20on%20the%20Stiefel%20Manifold&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-stiefel%2f&amp;hashtags=MachineLearning%2cOptimizers">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Heuristic Solutions for Steepest Descent on the Stiefel Manifold on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-stiefel%2f&amp;title=Heuristic%20Solutions%20for%20Steepest%20Descent%20on%20the%20Stiefel%20Manifold&amp;summary=Heuristic%20Solutions%20for%20Steepest%20Descent%20on%20the%20Stiefel%20Manifold&amp;source=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-stiefel%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Heuristic Solutions for Steepest Descent on the Stiefel Manifold on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-stiefel%2f&title=Heuristic%20Solutions%20for%20Steepest%20Descent%20on%20the%20Stiefel%20Manifold">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Heuristic Solutions for Steepest Descent on the Stiefel Manifold on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-stiefel%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Heuristic Solutions for Steepest Descent on the Stiefel Manifold on whatsapp"
            href="https://api.whatsapp.com/send?text=Heuristic%20Solutions%20for%20Steepest%20Descent%20on%20the%20Stiefel%20Manifold%20-%20https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-stiefel%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Heuristic Solutions for Steepest Descent on the Stiefel Manifold on telegram"
            href="https://telegram.me/share/url?text=Heuristic%20Solutions%20for%20Steepest%20Descent%20on%20the%20Stiefel%20Manifold&amp;url=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-stiefel%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Heuristic Solutions for Steepest Descent on the Stiefel Manifold on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Heuristic%20Solutions%20for%20Steepest%20Descent%20on%20the%20Stiefel%20Manifold&u=https%3a%2f%2fleloykun.github.io%2fponder%2fsteepest-descent-stiefel%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://leloykun.github.io/">Franz Louis Cesista</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
